nohup: ignoring input
Your base folder is: /workspace/GPT
Flash Attention is available!
Training with 4 GPUs
2025-03-21 19:13:58,744 - transformer_training - INFO - Tokenizing dataset...
2025-03-21 19:14:00,499 - transformer_training - INFO - Chunking dataset...
2025-03-21 19:14:01,225 - transformer_training - INFO - Converting to tensors...
2025-03-21 19:15:04,587 - transformer_training - INFO - Train Data: torch.Size([213867, 512]), torch.int64
2025-03-21 19:15:04,587 - transformer_training - INFO - Val Data: torch.Size([418, 512]), torch.int64
2025-03-21 19:15:04,587 - transformer_training - INFO - Test Data: torch.Size([485, 512]), torch.int64
2025-03-21 19:15:04,587 - transformer_training - INFO - Vocabulary size: 50257
Train Data: torch.Size([213867, 512]), torch.int64
Val   Data: torch.Size([418, 512]), torch.int64
Test  Data: torch.Size([485, 512]), torch.int64
Vocabulary size: 50257
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Model initialized with 60,179,537 parameters
Model initialized with 60,179,537 parameters
Model initialized with 60,179,537 parameters
Model initialized with 60,179,537 parameters
Total iterations: 1000
Batches per epoch: 836
2025-03-21 19:15:13,320 - transformer_training - INFO - Main loop iteration: 0
2025-03-21 19:15:13,321 - transformer_training - INFO - Main loop iteration: 0
2025-03-21 19:15:13,329 - transformer_training - INFO - Main loop iteration: 0
2025-03-21 19:15:13,334 - transformer_training - INFO - Main loop iteration: 0
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 0: loss 10.9274, lr 0.000400, 28936.95 tokens/sec
2025-03-21 19:15:17,953 - transformer_training - INFO - Main loop iteration: 1
2025-03-21 19:15:17,995 - transformer_training - INFO - Main loop iteration: 1
2025-03-21 19:15:18,201 - transformer_training - INFO - Main loop iteration: 1
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
[rank2]:[E321 19:25:18.309146266 ProcessGroupNCCL.cpp:629] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=BROADCAST, NumelIn=79, NumelOut=79, Timeout(ms)=600000) ran for 600039 milliseconds before timing out.
[rank2]:[E321 19:25:18.310012790 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 2]  failure detected by watchdog at work sequence id: 6 PG status: last enqueued work: 6, last completed work: 5
[rank2]:[E321 19:25:18.310041310 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank2]:[E321 19:25:18.310057930 ProcessGroupNCCL.cpp:681] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E321 19:25:18.310087486 ProcessGroupNCCL.cpp:695] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E321 19:25:18.313312992 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=BROADCAST, NumelIn=79, NumelOut=79, Timeout(ms)=600000) ran for 600039 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fd4bab6c1b6 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7fd469229c74 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7fd46922b7d0 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fd46922c6ed in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fd55b0bb5c0 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7fd55df78ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: clone + 0x44 (0x7fd55e009a04 in /usr/lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=BROADCAST, NumelIn=79, NumelOut=79, Timeout(ms)=600000) ran for 600039 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fd4bab6c1b6 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7fd469229c74 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7fd46922b7d0 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fd46922c6ed in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fd55b0bb5c0 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7fd55df78ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: clone + 0x44 (0x7fd55e009a04 in /usr/lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fd4bab6c1b6 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7fd468e876fc in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7fd55b0bb5c0 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7fd55df78ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #4: clone + 0x44 (0x7fd55e009a04 in /usr/lib/x86_64-linux-gnu/libc.so.6)

[rank3]:[E321 19:25:18.315745200 ProcessGroupNCCL.cpp:629] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=BROADCAST, NumelIn=79, NumelOut=79, Timeout(ms)=600000) ran for 600045 milliseconds before timing out.
[rank3]:[E321 19:25:18.316678504 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 3]  failure detected by watchdog at work sequence id: 6 PG status: last enqueued work: 6, last completed work: 5
[rank3]:[E321 19:25:18.316716037 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank3]:[E321 19:25:18.316734187 ProcessGroupNCCL.cpp:681] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E321 19:25:18.316744020 ProcessGroupNCCL.cpp:695] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank3]:[E321 19:25:18.319783373 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=BROADCAST, NumelIn=79, NumelOut=79, Timeout(ms)=600000) ran for 600045 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f09f196c1b6 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7f09a0029c74 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7f09a002b7d0 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f09a002c6ed in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f0a91e5d5c0 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7f0a94cb6ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: clone + 0x44 (0x7f0a94d47a04 in /usr/lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=BROADCAST, NumelIn=79, NumelOut=79, Timeout(ms)=600000) ran for 600045 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f09f196c1b6 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7f09a0029c74 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7f09a002b7d0 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f09a002c6ed in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f0a91e5d5c0 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7f0a94cb6ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: clone + 0x44 (0x7f0a94d47a04 in /usr/lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f09f196c1b6 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7f099fc876fc in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f0a91e5d5c0 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7f0a94cb6ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #4: clone + 0x44 (0x7f0a94d47a04 in /usr/lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[E321 19:25:18.322093798 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=BROADCAST, NumelIn=79, NumelOut=79, Timeout(ms)=600000) ran for 600052 milliseconds before timing out.
[rank1]:[E321 19:25:18.323548822 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 6 PG status: last enqueued work: 6, last completed work: 5
[rank1]:[E321 19:25:18.323583422 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E321 19:25:18.323599782 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E321 19:25:18.323610342 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E321 19:25:18.325906835 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=BROADCAST, NumelIn=79, NumelOut=79, Timeout(ms)=600000) ran for 600052 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f2ffd96c1b6 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7f2fabc29c74 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7f2fabc2b7d0 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f2fabc2c6ed in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f2ffddaf5c0 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7f30a097dac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: clone + 0x44 (0x7f30a0a0ea04 in /usr/lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=BROADCAST, NumelIn=79, NumelOut=79, Timeout(ms)=600000) ran for 600052 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f2ffd96c1b6 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7f2fabc29c74 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7f2fabc2b7d0 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f2fabc2c6ed in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f2ffddaf5c0 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7f30a097dac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: clone + 0x44 (0x7f30a0a0ea04 in /usr/lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f2ffd96c1b6 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7f2fab8876fc in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f2ffddaf5c0 in /workspace/GPT/.venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7f30a097dac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #4: clone + 0x44 (0x7f30a0a0ea04 in /usr/lib/x86_64-linux-gnu/libc.so.6)

W0321 19:25:18.957000 5871 torch/multiprocessing/spawn.py:169] Terminating process 6064 via signal SIGTERM
Traceback (most recent call last):
  File "/workspace/GPT/models/gpt_a100.py", line 786, in <module>
    main()
  File "/workspace/GPT/models/gpt_a100.py", line 777, in main
    mp.spawn(
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 340, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 296, in start_processes
    while not context.join():
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 196, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 3 terminated with signal SIGABRT
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 80 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
