nohup: ignoring input
Your base folder is: /workspace/GPT
Flash Attention is available!
Training with 1 GPUs
2025-03-22 06:57:15,828 - transformer_training - INFO - Tokenizing dataset...
2025-03-22 06:57:18,154 - transformer_training - INFO - Chunking dataset...
Chunking (num_proc=256):   0%|          | 0/2530 [00:00<?, ? examples/s]Chunking (num_proc=256):   0%|          | 10/2530 [00:00<00:38, 64.80 examples/s]Chunking (num_proc=256):   1%|          | 30/2530 [00:00<00:41, 59.74 examples/s]Chunking (num_proc=256):  11%|█         | 270/2530 [00:00<00:03, 617.27 examples/s]Chunking (num_proc=256):  18%|█▊        | 460/2530 [00:00<00:02, 929.52 examples/s]Chunking (num_proc=256):  29%|██▉       | 730/2530 [00:00<00:01, 1382.73 examples/s]Chunking (num_proc=256):  40%|████      | 1020/2530 [00:00<00:00, 1790.13 examples/s]Chunking (num_proc=256):  51%|█████     | 1280/2530 [00:01<00:00, 1995.74 examples/s]Chunking (num_proc=256):  62%|██████▏   | 1560/2530 [00:01<00:00, 2175.21 examples/s]Chunking (num_proc=256):  72%|███████▏  | 1819/2530 [00:01<00:00, 2248.91 examples/s]Chunking (num_proc=256):  91%|█████████ | 2303/2530 [00:01<00:00, 2977.10 examples/s]Chunking (num_proc=256): 100%|██████████| 2530/2530 [00:01<00:00, 1743.31 examples/s]
Chunking (num_proc=256):   0%|          | 0/1014942 [00:00<?, ? examples/s]Chunking (num_proc=256):   0%|          | 512/1014942 [00:00<04:37, 3661.12 examples/s]Chunking (num_proc=256):   1%|          | 8192/1014942 [00:00<00:42, 23476.05 examples/s]Chunking (num_proc=256):   1%|▏         | 12800/1014942 [00:00<00:33, 30090.57 examples/s]Chunking (num_proc=256):   9%|▊         | 87040/1014942 [00:00<00:03, 240423.36 examples/s]Chunking (num_proc=256):  15%|█▍        | 149492/1014942 [00:00<00:02, 348774.84 examples/s]Chunking (num_proc=256):  19%|█▉        | 196298/1014942 [00:00<00:02, 383712.18 examples/s]Chunking (num_proc=256):  24%|██▎       | 239782/1014942 [00:00<00:01, 390728.99 examples/s]Chunking (num_proc=256):  28%|██▊       | 282730/1014942 [00:01<00:01, 371789.65 examples/s]Chunking (num_proc=256):  32%|███▏      | 322749/1014942 [00:01<00:02, 284815.54 examples/s]Chunking (num_proc=256):  36%|███▌      | 364947/1014942 [00:01<00:02, 316265.09 examples/s]Chunking (num_proc=256):  40%|███▉      | 403192/1014942 [00:01<00:01, 332148.31 examples/s]Chunking (num_proc=256):  47%|████▋     | 479432/1014942 [00:01<00:01, 444184.12 examples/s]Chunking (num_proc=256):  57%|█████▋    | 577651/1014942 [00:01<00:00, 587314.04 examples/s]Chunking (num_proc=256):  73%|███████▎  | 744219/1014942 [00:01<00:00, 887289.53 examples/s]Chunking (num_proc=256):  83%|████████▎ | 838820/1014942 [00:01<00:00, 891624.54 examples/s]Chunking (num_proc=256):  92%|█████████▏| 931877/1014942 [00:01<00:00, 870028.39 examples/s]Chunking (num_proc=256): 100%|██████████| 1014942/1014942 [00:02<00:00, 463882.98 examples/s]
Chunking (num_proc=256):   0%|          | 0/2163 [00:00<?, ? examples/s]Chunking (num_proc=256):   0%|          | 9/2163 [00:00<01:57, 18.40 examples/s]Chunking (num_proc=256):  11%|█         | 243/2163 [00:00<00:03, 542.96 examples/s]Chunking (num_proc=256):  22%|██▏       | 477/2163 [00:00<00:01, 970.67 examples/s]Chunking (num_proc=256):  35%|███▌      | 764/2163 [00:00<00:00, 1455.34 examples/s]Chunking (num_proc=256):  47%|████▋     | 1023/2163 [00:00<00:00, 1741.63 examples/s]Chunking (num_proc=256):  58%|█████▊    | 1259/2163 [00:01<00:00, 1770.92 examples/s]Chunking (num_proc=256):  68%|██████▊   | 1475/2163 [00:01<00:00, 1810.81 examples/s]Chunking (num_proc=256):  78%|███████▊  | 1683/2163 [00:01<00:00, 1821.86 examples/s]Chunking (num_proc=256):  91%|█████████▏| 1979/2163 [00:01<00:00, 2129.14 examples/s]Chunking (num_proc=256): 100%|██████████| 2163/2163 [00:01<00:00, 1450.31 examples/s]
Filter:   0%|          | 0/394 [00:00<?, ? examples/s]Filter: 100%|██████████| 394/394 [00:00<00:00, 4820.62 examples/s]
Filter:   0%|          | 0/213838 [00:00<?, ? examples/s]Filter:   0%|          | 1000/213838 [00:00<00:40, 5215.23 examples/s]Filter:   1%|          | 2000/213838 [00:00<00:37, 5654.76 examples/s]Filter:   1%|▏         | 3000/213838 [00:00<00:36, 5781.70 examples/s]Filter:   2%|▏         | 4000/213838 [00:00<00:35, 5854.62 examples/s]Filter:   2%|▏         | 5000/213838 [00:00<00:35, 5866.81 examples/s]Filter:   3%|▎         | 6000/213838 [00:01<00:35, 5895.35 examples/s]Filter:   3%|▎         | 7000/213838 [00:01<00:34, 5926.08 examples/s]Filter:   4%|▎         | 8000/213838 [00:01<00:35, 5856.98 examples/s]Filter:   4%|▍         | 9000/213838 [00:01<00:34, 5879.95 examples/s]Filter:   5%|▍         | 10000/213838 [00:01<00:34, 5908.67 examples/s]Filter:   5%|▌         | 11000/213838 [00:01<00:34, 5925.85 examples/s]Filter:   6%|▌         | 12000/213838 [00:02<00:33, 5941.47 examples/s]Filter:   6%|▌         | 13000/213838 [00:02<00:33, 5954.94 examples/s]Filter:   7%|▋         | 14000/213838 [00:02<00:33, 5959.23 examples/s]Filter:   7%|▋         | 15000/213838 [00:02<00:33, 5966.46 examples/s]Filter:   7%|▋         | 16000/213838 [00:02<00:33, 5953.56 examples/s]Filter:   8%|▊         | 17000/213838 [00:02<00:33, 5954.34 examples/s]Filter:   8%|▊         | 18000/213838 [00:03<00:32, 5937.26 examples/s]Filter:   9%|▉         | 19000/213838 [00:03<00:32, 5947.60 examples/s]Filter:   9%|▉         | 20000/213838 [00:03<00:32, 5952.50 examples/s]Filter:  10%|▉         | 21000/213838 [00:03<00:32, 5966.90 examples/s]Filter:  10%|█         | 22000/213838 [00:03<00:32, 5926.08 examples/s]Filter:  11%|█         | 23000/213838 [00:03<00:32, 5935.52 examples/s]Filter:  11%|█         | 24000/213838 [00:04<00:31, 5934.73 examples/s]Filter:  12%|█▏        | 25000/213838 [00:04<00:31, 5947.20 examples/s]Filter:  12%|█▏        | 26000/213838 [00:04<00:31, 5890.17 examples/s]Filter:  13%|█▎        | 27000/213838 [00:04<00:31, 5925.91 examples/s]Filter:  13%|█▎        | 28000/213838 [00:04<00:31, 5945.57 examples/s]Filter:  14%|█▎        | 29000/213838 [00:04<00:31, 5941.48 examples/s]Filter:  14%|█▍        | 30000/213838 [00:05<00:30, 5945.02 examples/s]Filter:  14%|█▍        | 31000/213838 [00:05<00:30, 5954.90 examples/s]Filter:  15%|█▍        | 32000/213838 [00:05<00:30, 5965.25 examples/s]Filter:  15%|█▌        | 33000/213838 [00:05<00:30, 5975.43 examples/s]Filter:  16%|█▌        | 34000/213838 [00:05<00:30, 5970.59 examples/s]Filter:  16%|█▋        | 35000/213838 [00:05<00:29, 5965.56 examples/s]Filter:  17%|█▋        | 36000/213838 [00:06<00:29, 5958.31 examples/s]Filter:  17%|█▋        | 37000/213838 [00:06<00:29, 5973.72 examples/s]Filter:  18%|█▊        | 38000/213838 [00:06<00:29, 5947.43 examples/s]Filter:  18%|█▊        | 39000/213838 [00:06<00:29, 5943.76 examples/s]Filter:  19%|█▊        | 40000/213838 [00:06<00:29, 5936.51 examples/s]Filter:  19%|█▉        | 41000/213838 [00:06<00:29, 5941.80 examples/s]Filter:  20%|█▉        | 42000/213838 [00:07<00:28, 5946.50 examples/s]Filter:  20%|██        | 43000/213838 [00:07<00:28, 5944.04 examples/s]Filter:  21%|██        | 44000/213838 [00:07<00:28, 5949.51 examples/s]Filter:  21%|██        | 45000/213838 [00:07<00:28, 5914.57 examples/s]Filter:  22%|██▏       | 46000/213838 [00:07<00:28, 5933.60 examples/s]Filter:  22%|██▏       | 47000/213838 [00:07<00:28, 5943.28 examples/s]Filter:  22%|██▏       | 48000/213838 [00:08<00:27, 5946.43 examples/s]Filter:  23%|██▎       | 49000/213838 [00:08<00:27, 5953.70 examples/s]Filter:  23%|██▎       | 50000/213838 [00:08<00:27, 5883.27 examples/s]Filter:  24%|██▍       | 51000/213838 [00:08<00:28, 5806.20 examples/s]Filter:  24%|██▍       | 52000/213838 [00:08<00:27, 5861.55 examples/s]Filter:  25%|██▍       | 53000/213838 [00:08<00:27, 5892.62 examples/s]Filter:  25%|██▌       | 54000/213838 [00:09<00:26, 5926.04 examples/s]Filter:  26%|██▌       | 55000/213838 [00:09<00:26, 5943.39 examples/s]Filter:  26%|██▌       | 56000/213838 [00:09<00:26, 5959.82 examples/s]Filter:  27%|██▋       | 57000/213838 [00:09<00:26, 5954.13 examples/s]Filter:  27%|██▋       | 58000/213838 [00:09<00:26, 5961.85 examples/s]Filter:  28%|██▊       | 59000/213838 [00:09<00:25, 5955.75 examples/s]Filter:  28%|██▊       | 60000/213838 [00:10<00:25, 5966.19 examples/s]Filter:  29%|██▊       | 61000/213838 [00:10<00:25, 5981.12 examples/s]Filter:  29%|██▉       | 62000/213838 [00:10<00:25, 5989.73 examples/s]Filter:  29%|██▉       | 63000/213838 [00:10<00:25, 5980.98 examples/s]Filter:  30%|██▉       | 64000/213838 [00:10<00:25, 5990.53 examples/s]Filter:  30%|███       | 65000/213838 [00:10<00:24, 5963.64 examples/s]Filter:  31%|███       | 66000/213838 [00:11<00:24, 5959.38 examples/s]Filter:  31%|███▏      | 67000/213838 [00:11<00:24, 5962.85 examples/s]Filter:  32%|███▏      | 68000/213838 [00:11<00:24, 5971.91 examples/s]Filter:  32%|███▏      | 69000/213838 [00:11<00:24, 5988.27 examples/s]Filter:  33%|███▎      | 70000/213838 [00:11<00:24, 5974.44 examples/s]Filter:  33%|███▎      | 71000/213838 [00:11<00:23, 5985.99 examples/s]Filter:  34%|███▎      | 72000/213838 [00:12<00:23, 5974.03 examples/s]Filter:  34%|███▍      | 73000/213838 [00:12<00:23, 5969.92 examples/s]Filter:  35%|███▍      | 74000/213838 [00:12<00:23, 5981.48 examples/s]Filter:  35%|███▌      | 75000/213838 [00:12<00:23, 5965.71 examples/s]Filter:  36%|███▌      | 76000/213838 [00:12<00:23, 5960.47 examples/s]Filter:  36%|███▌      | 77000/213838 [00:12<00:22, 5960.88 examples/s]Filter:  36%|███▋      | 78000/213838 [00:13<00:22, 5958.58 examples/s]Filter:  37%|███▋      | 79000/213838 [00:13<00:22, 5967.78 examples/s]Filter:  37%|███▋      | 80000/213838 [00:13<00:22, 5972.61 examples/s]Filter:  38%|███▊      | 81000/213838 [00:13<00:22, 5975.81 examples/s]Filter:  38%|███▊      | 82000/213838 [00:13<00:22, 5964.85 examples/s]Filter:  39%|███▉      | 83000/213838 [00:13<00:22, 5944.15 examples/s]Filter:  39%|███▉      | 84000/213838 [00:14<00:21, 5957.89 examples/s]Filter:  40%|███▉      | 85000/213838 [00:14<00:21, 5967.67 examples/s]Filter:  40%|████      | 86000/213838 [00:14<00:21, 5988.78 examples/s]Filter:  41%|████      | 87000/213838 [00:14<00:21, 5972.21 examples/s]Filter:  41%|████      | 88000/213838 [00:14<00:21, 5967.86 examples/s]Filter:  42%|████▏     | 89000/213838 [00:14<00:20, 5965.15 examples/s]Filter:  42%|████▏     | 90000/213838 [00:15<00:20, 5960.25 examples/s]Filter:  43%|████▎     | 91000/213838 [00:15<00:20, 5964.13 examples/s]Filter:  43%|████▎     | 92000/213838 [00:15<00:20, 5968.28 examples/s]Filter:  43%|████▎     | 93000/213838 [00:15<00:20, 5964.43 examples/s]Filter:  44%|████▍     | 94000/213838 [00:15<00:20, 5953.43 examples/s]Filter:  44%|████▍     | 95000/213838 [00:15<00:19, 5946.92 examples/s]Filter:  45%|████▍     | 96000/213838 [00:16<00:19, 5970.29 examples/s]Filter:  45%|████▌     | 97000/213838 [00:16<00:19, 5974.47 examples/s]Filter:  46%|████▌     | 98000/213838 [00:16<00:19, 5960.87 examples/s]Filter:  46%|████▋     | 99000/213838 [00:16<00:19, 5915.43 examples/s]Filter:  47%|████▋     | 100000/213838 [00:16<00:19, 5921.20 examples/s]Filter:  47%|████▋     | 101000/213838 [00:16<00:19, 5918.31 examples/s]Filter:  48%|████▊     | 102000/213838 [00:17<00:18, 5941.00 examples/s]Filter:  48%|████▊     | 103000/213838 [00:17<00:18, 5943.18 examples/s]Filter:  49%|████▊     | 104000/213838 [00:17<00:18, 5948.88 examples/s]Filter:  49%|████▉     | 105000/213838 [00:17<00:18, 5942.60 examples/s]Filter:  50%|████▉     | 106000/213838 [00:17<00:18, 5947.63 examples/s]Filter:  50%|█████     | 107000/213838 [00:18<00:17, 5952.17 examples/s]Filter:  51%|█████     | 108000/213838 [00:18<00:17, 5961.54 examples/s]Filter:  51%|█████     | 109000/213838 [00:18<00:17, 5961.80 examples/s]Filter:  51%|█████▏    | 110000/213838 [00:18<00:17, 5974.21 examples/s]Filter:  52%|█████▏    | 111000/213838 [00:18<00:17, 5978.50 examples/s]Filter:  52%|█████▏    | 112000/213838 [00:18<00:17, 5976.81 examples/s]Filter:  53%|█████▎    | 113000/213838 [00:19<00:16, 5979.64 examples/s]Filter:  53%|█████▎    | 114000/213838 [00:19<00:16, 5967.67 examples/s]Filter:  54%|█████▍    | 115000/213838 [00:19<00:16, 5976.55 examples/s]Filter:  54%|█████▍    | 116000/213838 [00:19<00:16, 5974.97 examples/s]Filter:  55%|█████▍    | 117000/213838 [00:19<00:16, 5982.58 examples/s]Filter:  55%|█████▌    | 118000/213838 [00:19<00:16, 5967.80 examples/s]Filter:  56%|█████▌    | 119000/213838 [00:20<00:15, 5960.69 examples/s]Filter:  56%|█████▌    | 120000/213838 [00:20<00:15, 5962.69 examples/s]Filter:  57%|█████▋    | 121000/213838 [00:20<00:15, 5960.93 examples/s]Filter:  57%|█████▋    | 122000/213838 [00:20<00:15, 5972.52 examples/s]Filter:  58%|█████▊    | 123000/213838 [00:20<00:15, 5960.50 examples/s]Filter:  58%|█████▊    | 124000/213838 [00:20<00:15, 5961.29 examples/s]Filter:  58%|█████▊    | 125000/213838 [00:21<00:14, 5973.99 examples/s]Filter:  59%|█████▉    | 126000/213838 [00:21<00:14, 5981.58 examples/s]Filter:  59%|█████▉    | 127000/213838 [00:21<00:14, 5971.11 examples/s]Filter:  60%|█████▉    | 128000/213838 [00:21<00:14, 5979.85 examples/s]Filter:  60%|██████    | 129000/213838 [00:21<00:14, 5970.46 examples/s]Filter:  61%|██████    | 130000/213838 [00:21<00:14, 5973.10 examples/s]Filter:  61%|██████▏   | 131000/213838 [00:22<00:13, 5959.47 examples/s]Filter:  62%|██████▏   | 132000/213838 [00:22<00:13, 5957.43 examples/s]Filter:  62%|██████▏   | 133000/213838 [00:22<00:13, 5948.94 examples/s]Filter:  63%|██████▎   | 134000/213838 [00:22<00:13, 5967.90 examples/s]Filter:  63%|██████▎   | 135000/213838 [00:22<00:13, 5965.75 examples/s]Filter:  64%|██████▎   | 136000/213838 [00:22<00:13, 5977.65 examples/s]Filter:  64%|██████▍   | 137000/213838 [00:23<00:12, 5973.48 examples/s]Filter:  65%|██████▍   | 138000/213838 [00:23<00:12, 5960.28 examples/s]Filter:  65%|██████▌   | 139000/213838 [00:23<00:12, 5962.70 examples/s]Filter:  65%|██████▌   | 140000/213838 [00:23<00:12, 5956.79 examples/s]Filter:  66%|██████▌   | 141000/213838 [00:23<00:12, 5946.67 examples/s]Filter:  66%|██████▋   | 142000/213838 [00:23<00:12, 5943.34 examples/s]Filter:  67%|██████▋   | 143000/213838 [00:24<00:11, 5945.97 examples/s]Filter:  67%|██████▋   | 144000/213838 [00:24<00:11, 5943.79 examples/s]Filter:  68%|██████▊   | 145000/213838 [00:24<00:11, 5945.69 examples/s]Filter:  68%|██████▊   | 146000/213838 [00:24<00:11, 5953.67 examples/s]Filter:  69%|██████▊   | 147000/213838 [00:24<00:11, 5950.28 examples/s]Filter:  69%|██████▉   | 148000/213838 [00:24<00:11, 5953.82 examples/s]Filter:  70%|██████▉   | 149000/213838 [00:25<00:10, 5959.01 examples/s]Filter:  70%|███████   | 150000/213838 [00:25<00:10, 5948.11 examples/s]Filter:  71%|███████   | 151000/213838 [00:25<00:10, 5945.05 examples/s]Filter:  71%|███████   | 152000/213838 [00:25<00:10, 5948.08 examples/s]Filter:  72%|███████▏  | 153000/213838 [00:25<00:10, 5950.50 examples/s]Filter:  72%|███████▏  | 154000/213838 [00:25<00:10, 5962.97 examples/s]Filter:  72%|███████▏  | 155000/213838 [00:26<00:09, 5975.48 examples/s]Filter:  73%|███████▎  | 156000/213838 [00:26<00:09, 5958.72 examples/s]Filter:  73%|███████▎  | 157000/213838 [00:26<00:09, 5951.40 examples/s]Filter:  74%|███████▍  | 158000/213838 [00:26<00:09, 5952.68 examples/s]Filter:  74%|███████▍  | 159000/213838 [00:26<00:09, 5941.88 examples/s]Filter:  75%|███████▍  | 160000/213838 [00:26<00:09, 5965.54 examples/s]Filter:  75%|███████▌  | 161000/213838 [00:27<00:08, 5968.39 examples/s]Filter:  76%|███████▌  | 162000/213838 [00:27<00:08, 5964.01 examples/s]Filter:  76%|███████▌  | 163000/213838 [00:27<00:08, 5950.00 examples/s]Filter:  77%|███████▋  | 164000/213838 [00:27<00:08, 5961.74 examples/s]Filter:  77%|███████▋  | 165000/213838 [00:27<00:08, 5931.40 examples/s]Filter:  78%|███████▊  | 166000/213838 [00:27<00:08, 5934.43 examples/s]Filter:  78%|███████▊  | 167000/213838 [00:28<00:07, 5940.45 examples/s]Filter:  79%|███████▊  | 168000/213838 [00:28<00:07, 5948.47 examples/s]Filter:  79%|███████▉  | 169000/213838 [00:28<00:07, 5937.91 examples/s]Filter:  79%|███████▉  | 170000/213838 [00:28<00:07, 5925.48 examples/s]Filter:  80%|███████▉  | 171000/213838 [00:28<00:07, 5929.62 examples/s]Filter:  80%|████████  | 172000/213838 [00:28<00:07, 5930.13 examples/s]Filter:  81%|████████  | 173000/213838 [00:29<00:06, 5948.77 examples/s]Filter:  81%|████████▏ | 174000/213838 [00:29<00:06, 5958.60 examples/s]Filter:  82%|████████▏ | 175000/213838 [00:29<00:06, 5932.37 examples/s]Filter:  82%|████████▏ | 176000/213838 [00:29<00:06, 5935.20 examples/s]Filter:  83%|████████▎ | 177000/213838 [00:29<00:06, 5929.96 examples/s]Filter:  83%|████████▎ | 178000/213838 [00:29<00:06, 5946.06 examples/s]Filter:  84%|████████▎ | 179000/213838 [00:30<00:05, 5948.45 examples/s]Filter:  84%|████████▍ | 180000/213838 [00:30<00:05, 5936.93 examples/s]Filter:  85%|████████▍ | 181000/213838 [00:30<00:05, 5939.41 examples/s]Filter:  85%|████████▌ | 182000/213838 [00:30<00:05, 5957.35 examples/s]Filter:  86%|████████▌ | 183000/213838 [00:30<00:05, 5942.11 examples/s]Filter:  86%|████████▌ | 184000/213838 [00:30<00:05, 5948.37 examples/s]Filter:  87%|████████▋ | 185000/213838 [00:31<00:04, 5948.93 examples/s]Filter:  87%|████████▋ | 186000/213838 [00:31<00:04, 5942.34 examples/s]Filter:  87%|████████▋ | 187000/213838 [00:31<00:04, 5954.74 examples/s]Filter:  88%|████████▊ | 188000/213838 [00:31<00:04, 5957.41 examples/s]Filter:  88%|████████▊ | 189000/213838 [00:31<00:04, 5936.94 examples/s]Filter:  89%|████████▉ | 190000/213838 [00:31<00:04, 5944.31 examples/s]Filter:  89%|████████▉ | 191000/213838 [00:32<00:03, 5945.51 examples/s]Filter:  90%|████████▉ | 192000/213838 [00:32<00:03, 5939.77 examples/s]Filter:  90%|█████████ | 193000/213838 [00:32<00:03, 5936.94 examples/s]Filter:  91%|█████████ | 194000/213838 [00:32<00:03, 5935.93 examples/s]Filter:  91%|█████████ | 195000/213838 [00:32<00:03, 5921.67 examples/s]Filter:  92%|█████████▏| 196000/213838 [00:32<00:03, 5934.68 examples/s]Filter:  92%|█████████▏| 197000/213838 [00:33<00:02, 5957.18 examples/s]Filter:  93%|█████████▎| 198000/213838 [00:33<00:02, 5958.45 examples/s]Filter:  93%|█████████▎| 199000/213838 [00:33<00:02, 5965.65 examples/s]Filter:  94%|█████████▎| 200000/213838 [00:33<00:02, 5962.06 examples/s]Filter:  94%|█████████▍| 201000/213838 [00:33<00:02, 5945.23 examples/s]Filter:  94%|█████████▍| 202000/213838 [00:33<00:01, 5939.22 examples/s]Filter:  95%|█████████▍| 203000/213838 [00:34<00:01, 5934.60 examples/s]Filter:  95%|█████████▌| 204000/213838 [00:34<00:01, 5928.36 examples/s]Filter:  96%|█████████▌| 205000/213838 [00:34<00:01, 5926.05 examples/s]Filter:  96%|█████████▋| 206000/213838 [00:34<00:01, 5937.59 examples/s]Filter:  97%|█████████▋| 207000/213838 [00:34<00:01, 5938.23 examples/s]Filter:  97%|█████████▋| 208000/213838 [00:34<00:00, 5940.67 examples/s]Filter:  98%|█████████▊| 209000/213838 [00:35<00:00, 5945.87 examples/s]Filter:  98%|█████████▊| 210000/213838 [00:35<00:00, 5947.17 examples/s]Filter:  99%|█████████▊| 211000/213838 [00:35<00:00, 5962.10 examples/s]Filter:  99%|█████████▉| 212000/213838 [00:35<00:00, 5973.89 examples/s]Filter: 100%|█████████▉| 213000/213838 [00:35<00:00, 5954.74 examples/s]Filter: 100%|██████████| 213838/213838 [00:35<00:00, 5950.23 examples/s]Filter: 100%|██████████| 213838/213838 [00:35<00:00, 5946.87 examples/s]
Filter:   0%|          | 0/324 [00:00<?, ? examples/s]Filter: 100%|██████████| 324/324 [00:00<00:00, 4967.81 examples/s]
2025-03-22 06:58:15,131 - transformer_training - INFO - Converting to tensors...
2025-03-22 06:58:58,562 - transformer_training - INFO - Train Data: torch.Size([213838, 512]), torch.int64
2025-03-22 06:58:58,562 - transformer_training - INFO - Val Data: torch.Size([324, 512]), torch.int64
2025-03-22 06:58:58,562 - transformer_training - INFO - Test Data: torch.Size([394, 512]), torch.int64
2025-03-22 06:58:58,562 - transformer_training - INFO - Vocabulary size: 50257
Train Data: torch.Size([213838, 512]), torch.int64
Val   Data: torch.Size([324, 512]), torch.int64
Test  Data: torch.Size([394, 512]), torch.int64
Vocabulary size: 50257
Your base folder is: /workspace/GPT
Flash Attention is available!
Model initialized with 101,997,649 parameters
Total iterations: 5000
Batches per epoch: 3342
2025-03-22 06:59:06,009 - transformer_training - INFO - Main loop iteration: 0
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 0: loss 10.9524, lr 0.000500, 8086.53 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 0: train loss 10.9571, val loss 10.9401
New best model saved with val loss: 10.9401
2025-03-22 06:59:30,390 - transformer_training - INFO - Main loop iteration: 1
2025-03-22 06:59:30,859 - transformer_training - INFO - Main loop iteration: 2
2025-03-22 06:59:31,142 - transformer_training - INFO - Main loop iteration: 3
2025-03-22 06:59:31,711 - transformer_training - INFO - Main loop iteration: 4
2025-03-22 06:59:31,967 - transformer_training - INFO - Main loop iteration: 5
2025-03-22 06:59:32,227 - transformer_training - INFO - Main loop iteration: 6
2025-03-22 06:59:32,488 - transformer_training - INFO - Main loop iteration: 7
2025-03-22 06:59:32,767 - transformer_training - INFO - Main loop iteration: 8
2025-03-22 06:59:33,017 - transformer_training - INFO - Main loop iteration: 9
2025-03-22 06:59:33,279 - transformer_training - INFO - Main loop iteration: 10
Iter 10: loss 10.5053, lr 0.000002, 126974.83 tokens/sec
2025-03-22 06:59:33,538 - transformer_training - INFO - Main loop iteration: 11
2025-03-22 06:59:33,815 - transformer_training - INFO - Main loop iteration: 12
2025-03-22 06:59:34,064 - transformer_training - INFO - Main loop iteration: 13
2025-03-22 06:59:34,325 - transformer_training - INFO - Main loop iteration: 14
2025-03-22 06:59:34,584 - transformer_training - INFO - Main loop iteration: 15
2025-03-22 06:59:34,862 - transformer_training - INFO - Main loop iteration: 16
2025-03-22 06:59:35,119 - transformer_training - INFO - Main loop iteration: 17
2025-03-22 06:59:35,380 - transformer_training - INFO - Main loop iteration: 18
2025-03-22 06:59:35,641 - transformer_training - INFO - Main loop iteration: 19
2025-03-22 06:59:35,918 - transformer_training - INFO - Main loop iteration: 20
Iter 20: loss 10.4255, lr 0.000005, 131831.88 tokens/sec
2025-03-22 06:59:36,168 - transformer_training - INFO - Main loop iteration: 21
2025-03-22 06:59:36,427 - transformer_training - INFO - Main loop iteration: 22
2025-03-22 06:59:36,685 - transformer_training - INFO - Main loop iteration: 23
2025-03-22 06:59:36,962 - transformer_training - INFO - Main loop iteration: 24
2025-03-22 06:59:37,212 - transformer_training - INFO - Main loop iteration: 25
2025-03-22 06:59:37,471 - transformer_training - INFO - Main loop iteration: 26
2025-03-22 06:59:37,730 - transformer_training - INFO - Main loop iteration: 27
2025-03-22 06:59:38,006 - transformer_training - INFO - Main loop iteration: 28
2025-03-22 06:59:38,256 - transformer_training - INFO - Main loop iteration: 29
2025-03-22 06:59:38,516 - transformer_training - INFO - Main loop iteration: 30
Iter 30: loss 10.3383, lr 0.000007, 127078.38 tokens/sec
2025-03-22 06:59:38,774 - transformer_training - INFO - Main loop iteration: 31
2025-03-22 06:59:39,057 - transformer_training - INFO - Main loop iteration: 32
2025-03-22 06:59:39,307 - transformer_training - INFO - Main loop iteration: 33
2025-03-22 06:59:39,566 - transformer_training - INFO - Main loop iteration: 34
2025-03-22 06:59:39,825 - transformer_training - INFO - Main loop iteration: 35
2025-03-22 06:59:40,101 - transformer_training - INFO - Main loop iteration: 36
2025-03-22 06:59:40,351 - transformer_training - INFO - Main loop iteration: 37
2025-03-22 06:59:40,611 - transformer_training - INFO - Main loop iteration: 38
2025-03-22 06:59:40,870 - transformer_training - INFO - Main loop iteration: 39
2025-03-22 06:59:41,147 - transformer_training - INFO - Main loop iteration: 40
Iter 40: loss 10.2143, lr 0.000010, 131767.93 tokens/sec
2025-03-22 06:59:41,396 - transformer_training - INFO - Main loop iteration: 41
2025-03-22 06:59:41,656 - transformer_training - INFO - Main loop iteration: 42
2025-03-22 06:59:41,914 - transformer_training - INFO - Main loop iteration: 43
2025-03-22 06:59:42,191 - transformer_training - INFO - Main loop iteration: 44
2025-03-22 06:59:42,441 - transformer_training - INFO - Main loop iteration: 45
2025-03-22 06:59:42,701 - transformer_training - INFO - Main loop iteration: 46
2025-03-22 06:59:42,965 - transformer_training - INFO - Main loop iteration: 47
2025-03-22 06:59:43,242 - transformer_training - INFO - Main loop iteration: 48
2025-03-22 06:59:43,491 - transformer_training - INFO - Main loop iteration: 49
2025-03-22 06:59:43,751 - transformer_training - INFO - Main loop iteration: 50
Iter 50: loss 10.1424, lr 0.000012, 127005.34 tokens/sec
2025-03-22 06:59:44,010 - transformer_training - INFO - Main loop iteration: 51
2025-03-22 06:59:44,287 - transformer_training - INFO - Main loop iteration: 52
2025-03-22 06:59:44,536 - transformer_training - INFO - Main loop iteration: 53
2025-03-22 06:59:44,795 - transformer_training - INFO - Main loop iteration: 54
2025-03-22 06:59:45,054 - transformer_training - INFO - Main loop iteration: 55
2025-03-22 06:59:45,331 - transformer_training - INFO - Main loop iteration: 56
2025-03-22 06:59:45,580 - transformer_training - INFO - Main loop iteration: 57
2025-03-22 06:59:45,839 - transformer_training - INFO - Main loop iteration: 58
2025-03-22 06:59:46,098 - transformer_training - INFO - Main loop iteration: 59
2025-03-22 06:59:46,375 - transformer_training - INFO - Main loop iteration: 60
Iter 60: loss 10.0673, lr 0.000015, 131848.32 tokens/sec
2025-03-22 06:59:46,625 - transformer_training - INFO - Main loop iteration: 61
2025-03-22 06:59:46,884 - transformer_training - INFO - Main loop iteration: 62
2025-03-22 06:59:47,143 - transformer_training - INFO - Main loop iteration: 63
2025-03-22 06:59:47,420 - transformer_training - INFO - Main loop iteration: 64
2025-03-22 06:59:47,669 - transformer_training - INFO - Main loop iteration: 65
2025-03-22 06:59:47,929 - transformer_training - INFO - Main loop iteration: 66
2025-03-22 06:59:48,191 - transformer_training - INFO - Main loop iteration: 67
2025-03-22 06:59:48,468 - transformer_training - INFO - Main loop iteration: 68
2025-03-22 06:59:48,717 - transformer_training - INFO - Main loop iteration: 69
2025-03-22 06:59:48,977 - transformer_training - INFO - Main loop iteration: 70
Iter 70: loss 10.0202, lr 0.000017, 125969.55 tokens/sec
2025-03-22 06:59:49,238 - transformer_training - INFO - Main loop iteration: 71
2025-03-22 06:59:49,515 - transformer_training - INFO - Main loop iteration: 72
2025-03-22 06:59:49,764 - transformer_training - INFO - Main loop iteration: 73
2025-03-22 06:59:50,024 - transformer_training - INFO - Main loop iteration: 74
2025-03-22 06:59:50,283 - transformer_training - INFO - Main loop iteration: 75
2025-03-22 06:59:50,560 - transformer_training - INFO - Main loop iteration: 76
2025-03-22 06:59:50,809 - transformer_training - INFO - Main loop iteration: 77
2025-03-22 06:59:51,069 - transformer_training - INFO - Main loop iteration: 78
2025-03-22 06:59:51,328 - transformer_training - INFO - Main loop iteration: 79
2025-03-22 06:59:51,604 - transformer_training - INFO - Main loop iteration: 80
Iter 80: loss 9.9742, lr 0.000020, 131563.84 tokens/sec
2025-03-22 06:59:51,854 - transformer_training - INFO - Main loop iteration: 81
2025-03-22 06:59:52,114 - transformer_training - INFO - Main loop iteration: 82
2025-03-22 06:59:52,373 - transformer_training - INFO - Main loop iteration: 83
2025-03-22 06:59:52,655 - transformer_training - INFO - Main loop iteration: 84
2025-03-22 06:59:52,908 - transformer_training - INFO - Main loop iteration: 85
2025-03-22 06:59:53,168 - transformer_training - INFO - Main loop iteration: 86
2025-03-22 06:59:53,426 - transformer_training - INFO - Main loop iteration: 87
2025-03-22 06:59:53,703 - transformer_training - INFO - Main loop iteration: 88
2025-03-22 06:59:53,957 - transformer_training - INFO - Main loop iteration: 89
2025-03-22 06:59:54,217 - transformer_training - INFO - Main loop iteration: 90
Iter 90: loss 9.9225, lr 0.000022, 127102.71 tokens/sec
2025-03-22 06:59:54,476 - transformer_training - INFO - Main loop iteration: 91
2025-03-22 06:59:54,758 - transformer_training - INFO - Main loop iteration: 92
2025-03-22 06:59:55,008 - transformer_training - INFO - Main loop iteration: 93
2025-03-22 06:59:55,268 - transformer_training - INFO - Main loop iteration: 94
2025-03-22 06:59:55,526 - transformer_training - INFO - Main loop iteration: 95
2025-03-22 06:59:55,803 - transformer_training - INFO - Main loop iteration: 96
2025-03-22 06:59:56,052 - transformer_training - INFO - Main loop iteration: 97
2025-03-22 06:59:56,312 - transformer_training - INFO - Main loop iteration: 98
2025-03-22 06:59:56,570 - transformer_training - INFO - Main loop iteration: 99
2025-03-22 06:59:56,847 - transformer_training - INFO - Main loop iteration: 100
Iter 100: loss 9.8899, lr 0.000025, 131727.89 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 100: train loss 9.8638, val loss 9.8433
New best model saved with val loss: 9.8433
2025-03-22 07:00:17,823 - transformer_training - INFO - Main loop iteration: 101
2025-03-22 07:00:18,099 - transformer_training - INFO - Main loop iteration: 102
2025-03-22 07:00:18,360 - transformer_training - INFO - Main loop iteration: 103
2025-03-22 07:00:18,637 - transformer_training - INFO - Main loop iteration: 104
2025-03-22 07:00:18,886 - transformer_training - INFO - Main loop iteration: 105
2025-03-22 07:00:19,146 - transformer_training - INFO - Main loop iteration: 106
2025-03-22 07:00:19,405 - transformer_training - INFO - Main loop iteration: 107
2025-03-22 07:00:19,683 - transformer_training - INFO - Main loop iteration: 108
2025-03-22 07:00:19,932 - transformer_training - INFO - Main loop iteration: 109
2025-03-22 07:00:20,192 - transformer_training - INFO - Main loop iteration: 110
Iter 110: loss 9.8552, lr 0.000027, 126878.01 tokens/sec
2025-03-22 07:00:20,451 - transformer_training - INFO - Main loop iteration: 111
2025-03-22 07:00:20,728 - transformer_training - INFO - Main loop iteration: 112
2025-03-22 07:00:20,977 - transformer_training - INFO - Main loop iteration: 113
2025-03-22 07:00:21,238 - transformer_training - INFO - Main loop iteration: 114
2025-03-22 07:00:21,496 - transformer_training - INFO - Main loop iteration: 115
2025-03-22 07:00:21,775 - transformer_training - INFO - Main loop iteration: 116
2025-03-22 07:00:22,024 - transformer_training - INFO - Main loop iteration: 117
2025-03-22 07:00:22,285 - transformer_training - INFO - Main loop iteration: 118
2025-03-22 07:00:22,544 - transformer_training - INFO - Main loop iteration: 119
2025-03-22 07:00:22,822 - transformer_training - INFO - Main loop iteration: 120
Iter 120: loss 9.8233, lr 0.000030, 131718.04 tokens/sec
2025-03-22 07:00:23,071 - transformer_training - INFO - Main loop iteration: 121
2025-03-22 07:00:23,332 - transformer_training - INFO - Main loop iteration: 122
2025-03-22 07:00:23,590 - transformer_training - INFO - Main loop iteration: 123
2025-03-22 07:00:23,868 - transformer_training - INFO - Main loop iteration: 124
2025-03-22 07:00:24,116 - transformer_training - INFO - Main loop iteration: 125
2025-03-22 07:00:24,376 - transformer_training - INFO - Main loop iteration: 126
2025-03-22 07:00:24,635 - transformer_training - INFO - Main loop iteration: 127
2025-03-22 07:00:24,912 - transformer_training - INFO - Main loop iteration: 128
2025-03-22 07:00:25,161 - transformer_training - INFO - Main loop iteration: 129
2025-03-22 07:00:25,422 - transformer_training - INFO - Main loop iteration: 130
Iter 130: loss 9.7664, lr 0.000032, 127177.39 tokens/sec
2025-03-22 07:00:25,680 - transformer_training - INFO - Main loop iteration: 131
2025-03-22 07:00:25,958 - transformer_training - INFO - Main loop iteration: 132
2025-03-22 07:00:26,207 - transformer_training - INFO - Main loop iteration: 133
2025-03-22 07:00:26,467 - transformer_training - INFO - Main loop iteration: 134
2025-03-22 07:00:26,725 - transformer_training - INFO - Main loop iteration: 135
2025-03-22 07:00:27,003 - transformer_training - INFO - Main loop iteration: 136
2025-03-22 07:00:27,253 - transformer_training - INFO - Main loop iteration: 137
2025-03-22 07:00:27,513 - transformer_training - INFO - Main loop iteration: 138
2025-03-22 07:00:27,772 - transformer_training - INFO - Main loop iteration: 139
2025-03-22 07:00:28,049 - transformer_training - INFO - Main loop iteration: 140
Iter 140: loss 9.6952, lr 0.000035, 130681.42 tokens/sec
2025-03-22 07:00:28,301 - transformer_training - INFO - Main loop iteration: 141
2025-03-22 07:00:28,561 - transformer_training - INFO - Main loop iteration: 142
2025-03-22 07:00:28,819 - transformer_training - INFO - Main loop iteration: 143
2025-03-22 07:00:29,102 - transformer_training - INFO - Main loop iteration: 144
2025-03-22 07:00:29,352 - transformer_training - INFO - Main loop iteration: 145
2025-03-22 07:00:29,612 - transformer_training - INFO - Main loop iteration: 146
2025-03-22 07:00:29,871 - transformer_training - INFO - Main loop iteration: 147
2025-03-22 07:00:30,155 - transformer_training - INFO - Main loop iteration: 148
2025-03-22 07:00:30,404 - transformer_training - INFO - Main loop iteration: 149
2025-03-22 07:00:30,665 - transformer_training - INFO - Main loop iteration: 150
Iter 150: loss 9.6522, lr 0.000037, 127235.32 tokens/sec
2025-03-22 07:00:30,923 - transformer_training - INFO - Main loop iteration: 151
2025-03-22 07:00:31,207 - transformer_training - INFO - Main loop iteration: 152
2025-03-22 07:00:31,456 - transformer_training - INFO - Main loop iteration: 153
2025-03-22 07:00:31,717 - transformer_training - INFO - Main loop iteration: 154
2025-03-22 07:00:31,975 - transformer_training - INFO - Main loop iteration: 155
2025-03-22 07:00:32,259 - transformer_training - INFO - Main loop iteration: 156
2025-03-22 07:00:32,508 - transformer_training - INFO - Main loop iteration: 157
2025-03-22 07:00:32,769 - transformer_training - INFO - Main loop iteration: 158
2025-03-22 07:00:33,027 - transformer_training - INFO - Main loop iteration: 159
2025-03-22 07:00:33,311 - transformer_training - INFO - Main loop iteration: 160
Iter 160: loss 9.6106, lr 0.000040, 131682.33 tokens/sec
2025-03-22 07:00:33,561 - transformer_training - INFO - Main loop iteration: 161
2025-03-22 07:00:33,821 - transformer_training - INFO - Main loop iteration: 162
2025-03-22 07:00:34,079 - transformer_training - INFO - Main loop iteration: 163
2025-03-22 07:00:34,362 - transformer_training - INFO - Main loop iteration: 164
2025-03-22 07:00:34,612 - transformer_training - INFO - Main loop iteration: 165
2025-03-22 07:00:34,872 - transformer_training - INFO - Main loop iteration: 166
2025-03-22 07:00:35,131 - transformer_training - INFO - Main loop iteration: 167
2025-03-22 07:00:35,414 - transformer_training - INFO - Main loop iteration: 168
2025-03-22 07:00:35,664 - transformer_training - INFO - Main loop iteration: 169
2025-03-22 07:00:35,924 - transformer_training - INFO - Main loop iteration: 170
Iter 170: loss 9.5736, lr 0.000042, 126934.14 tokens/sec
2025-03-22 07:00:36,183 - transformer_training - INFO - Main loop iteration: 171
2025-03-22 07:00:36,461 - transformer_training - INFO - Main loop iteration: 172
2025-03-22 07:00:36,710 - transformer_training - INFO - Main loop iteration: 173
2025-03-22 07:00:36,970 - transformer_training - INFO - Main loop iteration: 174
2025-03-22 07:00:37,229 - transformer_training - INFO - Main loop iteration: 175
2025-03-22 07:00:37,506 - transformer_training - INFO - Main loop iteration: 176
2025-03-22 07:00:37,755 - transformer_training - INFO - Main loop iteration: 177
2025-03-22 07:00:38,016 - transformer_training - INFO - Main loop iteration: 178
2025-03-22 07:00:38,275 - transformer_training - INFO - Main loop iteration: 179
2025-03-22 07:00:38,553 - transformer_training - INFO - Main loop iteration: 180
Iter 180: loss 9.4759, lr 0.000045, 129583.60 tokens/sec
2025-03-22 07:00:38,806 - transformer_training - INFO - Main loop iteration: 181
2025-03-22 07:00:39,067 - transformer_training - INFO - Main loop iteration: 182
2025-03-22 07:00:39,325 - transformer_training - INFO - Main loop iteration: 183
2025-03-22 07:00:39,609 - transformer_training - INFO - Main loop iteration: 184
2025-03-22 07:00:39,858 - transformer_training - INFO - Main loop iteration: 185
2025-03-22 07:00:40,247 - transformer_training - INFO - Main loop iteration: 186
2025-03-22 07:00:40,506 - transformer_training - INFO - Main loop iteration: 187
2025-03-22 07:00:40,789 - transformer_training - INFO - Main loop iteration: 188
2025-03-22 07:00:41,039 - transformer_training - INFO - Main loop iteration: 189
2025-03-22 07:00:41,299 - transformer_training - INFO - Main loop iteration: 190
Iter 190: loss 9.4218, lr 0.000047, 124487.63 tokens/sec
2025-03-22 07:00:41,563 - transformer_training - INFO - Main loop iteration: 191
2025-03-22 07:00:41,845 - transformer_training - INFO - Main loop iteration: 192
2025-03-22 07:00:42,095 - transformer_training - INFO - Main loop iteration: 193
2025-03-22 07:00:42,355 - transformer_training - INFO - Main loop iteration: 194
2025-03-22 07:00:42,613 - transformer_training - INFO - Main loop iteration: 195
2025-03-22 07:00:42,891 - transformer_training - INFO - Main loop iteration: 196
2025-03-22 07:00:43,140 - transformer_training - INFO - Main loop iteration: 197
2025-03-22 07:00:43,400 - transformer_training - INFO - Main loop iteration: 198
2025-03-22 07:00:43,659 - transformer_training - INFO - Main loop iteration: 199
2025-03-22 07:00:43,943 - transformer_training - INFO - Main loop iteration: 200
Iter 200: loss 9.3616, lr 0.000050, 131731.55 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 200: train loss 9.3369, val loss 9.3112
New best model saved with val loss: 9.3112
2025-03-22 07:01:05,151 - transformer_training - INFO - Main loop iteration: 201
2025-03-22 07:01:05,404 - transformer_training - INFO - Main loop iteration: 202
2025-03-22 07:01:05,665 - transformer_training - INFO - Main loop iteration: 203
2025-03-22 07:01:05,941 - transformer_training - INFO - Main loop iteration: 204
2025-03-22 07:01:06,193 - transformer_training - INFO - Main loop iteration: 205
2025-03-22 07:01:06,460 - transformer_training - INFO - Main loop iteration: 206
2025-03-22 07:01:06,718 - transformer_training - INFO - Main loop iteration: 207
2025-03-22 07:01:06,998 - transformer_training - INFO - Main loop iteration: 208
2025-03-22 07:01:07,248 - transformer_training - INFO - Main loop iteration: 209
2025-03-22 07:01:07,512 - transformer_training - INFO - Main loop iteration: 210
Iter 210: loss 9.3209, lr 0.000052, 124581.40 tokens/sec
2025-03-22 07:01:07,776 - transformer_training - INFO - Main loop iteration: 211
2025-03-22 07:01:08,054 - transformer_training - INFO - Main loop iteration: 212
2025-03-22 07:01:08,306 - transformer_training - INFO - Main loop iteration: 213
2025-03-22 07:01:08,567 - transformer_training - INFO - Main loop iteration: 214
2025-03-22 07:01:08,827 - transformer_training - INFO - Main loop iteration: 215
2025-03-22 07:01:09,105 - transformer_training - INFO - Main loop iteration: 216
2025-03-22 07:01:09,355 - transformer_training - INFO - Main loop iteration: 217
2025-03-22 07:01:09,616 - transformer_training - INFO - Main loop iteration: 218
2025-03-22 07:01:09,876 - transformer_training - INFO - Main loop iteration: 219
2025-03-22 07:01:10,154 - transformer_training - INFO - Main loop iteration: 220
Iter 220: loss 9.1954, lr 0.000055, 129773.13 tokens/sec
2025-03-22 07:01:10,407 - transformer_training - INFO - Main loop iteration: 221
2025-03-22 07:01:10,703 - transformer_training - INFO - Main loop iteration: 222
2025-03-22 07:01:10,971 - transformer_training - INFO - Main loop iteration: 223
2025-03-22 07:01:11,256 - transformer_training - INFO - Main loop iteration: 224
2025-03-22 07:01:11,512 - transformer_training - INFO - Main loop iteration: 225
2025-03-22 07:01:11,775 - transformer_training - INFO - Main loop iteration: 226
2025-03-22 07:01:12,034 - transformer_training - INFO - Main loop iteration: 227
2025-03-22 07:01:12,313 - transformer_training - INFO - Main loop iteration: 228
2025-03-22 07:01:12,565 - transformer_training - INFO - Main loop iteration: 229
2025-03-22 07:01:12,827 - transformer_training - INFO - Main loop iteration: 230
Iter 230: loss 9.1442, lr 0.000057, 126366.12 tokens/sec
2025-03-22 07:01:13,088 - transformer_training - INFO - Main loop iteration: 231
2025-03-22 07:01:13,373 - transformer_training - INFO - Main loop iteration: 232
2025-03-22 07:01:13,624 - transformer_training - INFO - Main loop iteration: 233
2025-03-22 07:01:13,886 - transformer_training - INFO - Main loop iteration: 234
2025-03-22 07:01:14,153 - transformer_training - INFO - Main loop iteration: 235
2025-03-22 07:01:14,434 - transformer_training - INFO - Main loop iteration: 236
2025-03-22 07:01:14,685 - transformer_training - INFO - Main loop iteration: 237
2025-03-22 07:01:14,947 - transformer_training - INFO - Main loop iteration: 238
2025-03-22 07:01:15,207 - transformer_training - INFO - Main loop iteration: 239
2025-03-22 07:01:15,486 - transformer_training - INFO - Main loop iteration: 240
Iter 240: loss 9.0922, lr 0.000060, 130876.42 tokens/sec
2025-03-22 07:01:15,737 - transformer_training - INFO - Main loop iteration: 241
2025-03-22 07:01:16,009 - transformer_training - INFO - Main loop iteration: 242
2025-03-22 07:01:16,270 - transformer_training - INFO - Main loop iteration: 243
2025-03-22 07:01:16,548 - transformer_training - INFO - Main loop iteration: 244
2025-03-22 07:01:16,802 - transformer_training - INFO - Main loop iteration: 245
2025-03-22 07:01:17,064 - transformer_training - INFO - Main loop iteration: 246
2025-03-22 07:01:17,325 - transformer_training - INFO - Main loop iteration: 247
2025-03-22 07:01:17,604 - transformer_training - INFO - Main loop iteration: 248
2025-03-22 07:01:17,855 - transformer_training - INFO - Main loop iteration: 249
2025-03-22 07:01:18,116 - transformer_training - INFO - Main loop iteration: 250
Iter 250: loss 8.9632, lr 0.000062, 126397.03 tokens/sec
2025-03-22 07:01:18,377 - transformer_training - INFO - Main loop iteration: 251
2025-03-22 07:01:18,659 - transformer_training - INFO - Main loop iteration: 252
2025-03-22 07:01:18,911 - transformer_training - INFO - Main loop iteration: 253
2025-03-22 07:01:19,174 - transformer_training - INFO - Main loop iteration: 254
2025-03-22 07:01:19,434 - transformer_training - INFO - Main loop iteration: 255
2025-03-22 07:01:19,714 - transformer_training - INFO - Main loop iteration: 256
2025-03-22 07:01:19,965 - transformer_training - INFO - Main loop iteration: 257
2025-03-22 07:01:20,228 - transformer_training - INFO - Main loop iteration: 258
2025-03-22 07:01:20,492 - transformer_training - INFO - Main loop iteration: 259
2025-03-22 07:01:20,771 - transformer_training - INFO - Main loop iteration: 260
Iter 260: loss 8.8752, lr 0.000065, 130001.56 tokens/sec
2025-03-22 07:01:21,024 - transformer_training - INFO - Main loop iteration: 261
2025-03-22 07:01:21,287 - transformer_training - INFO - Main loop iteration: 262
2025-03-22 07:01:21,548 - transformer_training - INFO - Main loop iteration: 263
2025-03-22 07:01:21,829 - transformer_training - INFO - Main loop iteration: 264
2025-03-22 07:01:22,086 - transformer_training - INFO - Main loop iteration: 265
2025-03-22 07:01:22,353 - transformer_training - INFO - Main loop iteration: 266
2025-03-22 07:01:22,618 - transformer_training - INFO - Main loop iteration: 267
2025-03-22 07:01:22,903 - transformer_training - INFO - Main loop iteration: 268
2025-03-22 07:01:23,156 - transformer_training - INFO - Main loop iteration: 269
2025-03-22 07:01:23,419 - transformer_training - INFO - Main loop iteration: 270
Iter 270: loss 8.8273, lr 0.000067, 126358.92 tokens/sec
2025-03-22 07:01:23,680 - transformer_training - INFO - Main loop iteration: 271
2025-03-22 07:01:23,963 - transformer_training - INFO - Main loop iteration: 272
2025-03-22 07:01:24,213 - transformer_training - INFO - Main loop iteration: 273
2025-03-22 07:01:24,476 - transformer_training - INFO - Main loop iteration: 274
2025-03-22 07:01:24,736 - transformer_training - INFO - Main loop iteration: 275
2025-03-22 07:01:25,015 - transformer_training - INFO - Main loop iteration: 276
2025-03-22 07:01:25,270 - transformer_training - INFO - Main loop iteration: 277
2025-03-22 07:01:25,535 - transformer_training - INFO - Main loop iteration: 278
2025-03-22 07:01:25,801 - transformer_training - INFO - Main loop iteration: 279
2025-03-22 07:01:26,087 - transformer_training - INFO - Main loop iteration: 280
Iter 280: loss 8.6846, lr 0.000070, 129824.36 tokens/sec
2025-03-22 07:01:26,340 - transformer_training - INFO - Main loop iteration: 281
2025-03-22 07:01:26,603 - transformer_training - INFO - Main loop iteration: 282
2025-03-22 07:01:26,868 - transformer_training - INFO - Main loop iteration: 283
2025-03-22 07:01:27,147 - transformer_training - INFO - Main loop iteration: 284
2025-03-22 07:01:27,399 - transformer_training - INFO - Main loop iteration: 285
2025-03-22 07:01:27,662 - transformer_training - INFO - Main loop iteration: 286
2025-03-22 07:01:27,921 - transformer_training - INFO - Main loop iteration: 287
2025-03-22 07:01:28,200 - transformer_training - INFO - Main loop iteration: 288
2025-03-22 07:01:28,452 - transformer_training - INFO - Main loop iteration: 289
2025-03-22 07:01:28,715 - transformer_training - INFO - Main loop iteration: 290
Iter 290: loss 8.6280, lr 0.000072, 126456.00 tokens/sec
2025-03-22 07:01:28,975 - transformer_training - INFO - Main loop iteration: 291
2025-03-22 07:01:29,254 - transformer_training - INFO - Main loop iteration: 292
2025-03-22 07:01:29,505 - transformer_training - INFO - Main loop iteration: 293
2025-03-22 07:01:29,767 - transformer_training - INFO - Main loop iteration: 294
2025-03-22 07:01:30,027 - transformer_training - INFO - Main loop iteration: 295
2025-03-22 07:01:30,306 - transformer_training - INFO - Main loop iteration: 296
2025-03-22 07:01:30,559 - transformer_training - INFO - Main loop iteration: 297
2025-03-22 07:01:30,821 - transformer_training - INFO - Main loop iteration: 298
2025-03-22 07:01:31,081 - transformer_training - INFO - Main loop iteration: 299
2025-03-22 07:01:31,360 - transformer_training - INFO - Main loop iteration: 300
Iter 300: loss 8.5063, lr 0.000075, 129710.30 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 300: train loss 8.4948, val loss 8.4519
New best model saved with val loss: 8.4519
2025-03-22 07:01:52,185 - transformer_training - INFO - Main loop iteration: 301
2025-03-22 07:01:52,465 - transformer_training - INFO - Main loop iteration: 302
2025-03-22 07:01:52,725 - transformer_training - INFO - Main loop iteration: 303
2025-03-22 07:01:53,009 - transformer_training - INFO - Main loop iteration: 304
2025-03-22 07:01:53,260 - transformer_training - INFO - Main loop iteration: 305
2025-03-22 07:01:53,522 - transformer_training - INFO - Main loop iteration: 306
2025-03-22 07:01:53,788 - transformer_training - INFO - Main loop iteration: 307
2025-03-22 07:01:54,068 - transformer_training - INFO - Main loop iteration: 308
2025-03-22 07:01:54,319 - transformer_training - INFO - Main loop iteration: 309
2025-03-22 07:01:54,580 - transformer_training - INFO - Main loop iteration: 310
Iter 310: loss 8.4392, lr 0.000077, 126203.21 tokens/sec
2025-03-22 07:01:54,841 - transformer_training - INFO - Main loop iteration: 311
2025-03-22 07:01:55,120 - transformer_training - INFO - Main loop iteration: 312
2025-03-22 07:01:55,371 - transformer_training - INFO - Main loop iteration: 313
2025-03-22 07:01:55,633 - transformer_training - INFO - Main loop iteration: 314
2025-03-22 07:01:55,893 - transformer_training - INFO - Main loop iteration: 315
2025-03-22 07:01:56,171 - transformer_training - INFO - Main loop iteration: 316
2025-03-22 07:01:56,422 - transformer_training - INFO - Main loop iteration: 317
2025-03-22 07:01:56,683 - transformer_training - INFO - Main loop iteration: 318
2025-03-22 07:01:56,944 - transformer_training - INFO - Main loop iteration: 319
2025-03-22 07:01:57,224 - transformer_training - INFO - Main loop iteration: 320
Iter 320: loss 8.3490, lr 0.000080, 130981.31 tokens/sec
2025-03-22 07:01:57,475 - transformer_training - INFO - Main loop iteration: 321
2025-03-22 07:01:57,737 - transformer_training - INFO - Main loop iteration: 322
2025-03-22 07:01:58,001 - transformer_training - INFO - Main loop iteration: 323
2025-03-22 07:01:58,286 - transformer_training - INFO - Main loop iteration: 324
2025-03-22 07:01:58,546 - transformer_training - INFO - Main loop iteration: 325
2025-03-22 07:01:58,811 - transformer_training - INFO - Main loop iteration: 326
2025-03-22 07:01:59,071 - transformer_training - INFO - Main loop iteration: 327
2025-03-22 07:01:59,350 - transformer_training - INFO - Main loop iteration: 328
2025-03-22 07:01:59,600 - transformer_training - INFO - Main loop iteration: 329
2025-03-22 07:01:59,863 - transformer_training - INFO - Main loop iteration: 330
Iter 330: loss 8.2902, lr 0.000082, 126089.63 tokens/sec
2025-03-22 07:02:00,124 - transformer_training - INFO - Main loop iteration: 331
2025-03-22 07:02:00,402 - transformer_training - INFO - Main loop iteration: 332
2025-03-22 07:02:00,653 - transformer_training - INFO - Main loop iteration: 333
2025-03-22 07:02:00,914 - transformer_training - INFO - Main loop iteration: 334
2025-03-22 07:02:01,173 - transformer_training - INFO - Main loop iteration: 335
2025-03-22 07:02:01,451 - transformer_training - INFO - Main loop iteration: 336
2025-03-22 07:02:01,702 - transformer_training - INFO - Main loop iteration: 337
2025-03-22 07:02:01,963 - transformer_training - INFO - Main loop iteration: 338
2025-03-22 07:02:02,223 - transformer_training - INFO - Main loop iteration: 339
2025-03-22 07:02:02,502 - transformer_training - INFO - Main loop iteration: 340
Iter 340: loss 8.1620, lr 0.000085, 131136.28 tokens/sec
2025-03-22 07:02:02,753 - transformer_training - INFO - Main loop iteration: 341
2025-03-22 07:02:03,013 - transformer_training - INFO - Main loop iteration: 342
2025-03-22 07:02:03,273 - transformer_training - INFO - Main loop iteration: 343
2025-03-22 07:02:03,550 - transformer_training - INFO - Main loop iteration: 344
2025-03-22 07:02:03,801 - transformer_training - INFO - Main loop iteration: 345
2025-03-22 07:02:04,063 - transformer_training - INFO - Main loop iteration: 346
2025-03-22 07:02:04,322 - transformer_training - INFO - Main loop iteration: 347
2025-03-22 07:02:04,600 - transformer_training - INFO - Main loop iteration: 348
2025-03-22 07:02:04,851 - transformer_training - INFO - Main loop iteration: 349
2025-03-22 07:02:05,112 - transformer_training - INFO - Main loop iteration: 350
Iter 350: loss 8.1209, lr 0.000087, 126580.38 tokens/sec
2025-03-22 07:02:05,372 - transformer_training - INFO - Main loop iteration: 351
2025-03-22 07:02:05,780 - transformer_training - INFO - Main loop iteration: 352
2025-03-22 07:02:06,035 - transformer_training - INFO - Main loop iteration: 353
2025-03-22 07:02:06,296 - transformer_training - INFO - Main loop iteration: 354
2025-03-22 07:02:06,557 - transformer_training - INFO - Main loop iteration: 355
2025-03-22 07:02:06,835 - transformer_training - INFO - Main loop iteration: 356
2025-03-22 07:02:07,089 - transformer_training - INFO - Main loop iteration: 357
2025-03-22 07:02:07,350 - transformer_training - INFO - Main loop iteration: 358
2025-03-22 07:02:07,614 - transformer_training - INFO - Main loop iteration: 359
2025-03-22 07:02:07,892 - transformer_training - INFO - Main loop iteration: 360
Iter 360: loss 8.0407, lr 0.000090, 130917.93 tokens/sec
2025-03-22 07:02:08,144 - transformer_training - INFO - Main loop iteration: 361
2025-03-22 07:02:08,408 - transformer_training - INFO - Main loop iteration: 362
2025-03-22 07:02:08,668 - transformer_training - INFO - Main loop iteration: 363
2025-03-22 07:02:08,951 - transformer_training - INFO - Main loop iteration: 364
2025-03-22 07:02:09,202 - transformer_training - INFO - Main loop iteration: 365
2025-03-22 07:02:09,463 - transformer_training - INFO - Main loop iteration: 366
2025-03-22 07:02:09,723 - transformer_training - INFO - Main loop iteration: 367
2025-03-22 07:02:10,001 - transformer_training - INFO - Main loop iteration: 368
2025-03-22 07:02:10,254 - transformer_training - INFO - Main loop iteration: 369
2025-03-22 07:02:10,515 - transformer_training - INFO - Main loop iteration: 370
Iter 370: loss 7.9656, lr 0.000092, 125010.08 tokens/sec
2025-03-22 07:02:10,778 - transformer_training - INFO - Main loop iteration: 371
2025-03-22 07:02:11,058 - transformer_training - INFO - Main loop iteration: 372
2025-03-22 07:02:11,311 - transformer_training - INFO - Main loop iteration: 373
2025-03-22 07:02:11,573 - transformer_training - INFO - Main loop iteration: 374
2025-03-22 07:02:11,832 - transformer_training - INFO - Main loop iteration: 375
2025-03-22 07:02:12,111 - transformer_training - INFO - Main loop iteration: 376
2025-03-22 07:02:12,362 - transformer_training - INFO - Main loop iteration: 377
2025-03-22 07:02:12,624 - transformer_training - INFO - Main loop iteration: 378
2025-03-22 07:02:12,884 - transformer_training - INFO - Main loop iteration: 379
2025-03-22 07:02:13,162 - transformer_training - INFO - Main loop iteration: 380
Iter 380: loss 7.9115, lr 0.000095, 131083.63 tokens/sec
2025-03-22 07:02:13,413 - transformer_training - INFO - Main loop iteration: 381
2025-03-22 07:02:13,674 - transformer_training - INFO - Main loop iteration: 382
2025-03-22 07:02:13,934 - transformer_training - INFO - Main loop iteration: 383
2025-03-22 07:02:14,216 - transformer_training - INFO - Main loop iteration: 384
2025-03-22 07:02:14,468 - transformer_training - INFO - Main loop iteration: 385
2025-03-22 07:02:14,729 - transformer_training - INFO - Main loop iteration: 386
2025-03-22 07:02:14,988 - transformer_training - INFO - Main loop iteration: 387
2025-03-22 07:02:15,267 - transformer_training - INFO - Main loop iteration: 388
2025-03-22 07:02:15,517 - transformer_training - INFO - Main loop iteration: 389
2025-03-22 07:02:15,778 - transformer_training - INFO - Main loop iteration: 390
Iter 390: loss 7.8830, lr 0.000097, 126615.02 tokens/sec
2025-03-22 07:02:16,038 - transformer_training - INFO - Main loop iteration: 391
2025-03-22 07:02:16,316 - transformer_training - INFO - Main loop iteration: 392
2025-03-22 07:02:16,566 - transformer_training - INFO - Main loop iteration: 393
2025-03-22 07:02:16,827 - transformer_training - INFO - Main loop iteration: 394
2025-03-22 07:02:17,087 - transformer_training - INFO - Main loop iteration: 395
2025-03-22 07:02:17,365 - transformer_training - INFO - Main loop iteration: 396
2025-03-22 07:02:17,616 - transformer_training - INFO - Main loop iteration: 397
2025-03-22 07:02:17,877 - transformer_training - INFO - Main loop iteration: 398
2025-03-22 07:02:18,136 - transformer_training - INFO - Main loop iteration: 399
2025-03-22 07:02:18,415 - transformer_training - INFO - Main loop iteration: 400
Iter 400: loss 7.8157, lr 0.000100, 131036.26 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 400: train loss 7.7592, val loss 7.7166
New best model saved with val loss: 7.7166
2025-03-22 07:02:38,689 - transformer_training - INFO - Main loop iteration: 401
2025-03-22 07:02:38,943 - transformer_training - INFO - Main loop iteration: 402
2025-03-22 07:02:39,204 - transformer_training - INFO - Main loop iteration: 403
2025-03-22 07:02:39,481 - transformer_training - INFO - Main loop iteration: 404
2025-03-22 07:02:39,735 - transformer_training - INFO - Main loop iteration: 405
2025-03-22 07:02:39,996 - transformer_training - INFO - Main loop iteration: 406
2025-03-22 07:02:40,255 - transformer_training - INFO - Main loop iteration: 407
2025-03-22 07:02:40,534 - transformer_training - INFO - Main loop iteration: 408
2025-03-22 07:02:40,784 - transformer_training - INFO - Main loop iteration: 409
2025-03-22 07:02:41,046 - transformer_training - INFO - Main loop iteration: 410
Iter 410: loss 7.7564, lr 0.000102, 124162.06 tokens/sec
2025-03-22 07:02:41,311 - transformer_training - INFO - Main loop iteration: 411
2025-03-22 07:02:41,588 - transformer_training - INFO - Main loop iteration: 412
2025-03-22 07:02:41,839 - transformer_training - INFO - Main loop iteration: 413
2025-03-22 07:02:42,100 - transformer_training - INFO - Main loop iteration: 414
2025-03-22 07:02:42,360 - transformer_training - INFO - Main loop iteration: 415
2025-03-22 07:02:42,643 - transformer_training - INFO - Main loop iteration: 416
2025-03-22 07:02:42,896 - transformer_training - INFO - Main loop iteration: 417
2025-03-22 07:02:43,157 - transformer_training - INFO - Main loop iteration: 418
2025-03-22 07:02:43,417 - transformer_training - INFO - Main loop iteration: 419
2025-03-22 07:02:43,695 - transformer_training - INFO - Main loop iteration: 420
Iter 420: loss 7.6755, lr 0.000105, 130487.12 tokens/sec
2025-03-22 07:02:43,947 - transformer_training - INFO - Main loop iteration: 421
2025-03-22 07:02:44,214 - transformer_training - INFO - Main loop iteration: 422
2025-03-22 07:02:44,474 - transformer_training - INFO - Main loop iteration: 423
2025-03-22 07:02:44,756 - transformer_training - INFO - Main loop iteration: 424
2025-03-22 07:02:45,008 - transformer_training - INFO - Main loop iteration: 425
2025-03-22 07:02:45,269 - transformer_training - INFO - Main loop iteration: 426
2025-03-22 07:02:45,528 - transformer_training - INFO - Main loop iteration: 427
2025-03-22 07:02:45,807 - transformer_training - INFO - Main loop iteration: 428
2025-03-22 07:02:46,057 - transformer_training - INFO - Main loop iteration: 429
2025-03-22 07:02:46,319 - transformer_training - INFO - Main loop iteration: 430
Iter 430: loss 7.6627, lr 0.000107, 126480.67 tokens/sec
2025-03-22 07:02:46,579 - transformer_training - INFO - Main loop iteration: 431
2025-03-22 07:02:46,858 - transformer_training - INFO - Main loop iteration: 432
2025-03-22 07:02:47,109 - transformer_training - INFO - Main loop iteration: 433
2025-03-22 07:02:47,370 - transformer_training - INFO - Main loop iteration: 434
2025-03-22 07:02:47,629 - transformer_training - INFO - Main loop iteration: 435
2025-03-22 07:02:47,908 - transformer_training - INFO - Main loop iteration: 436
2025-03-22 07:02:48,158 - transformer_training - INFO - Main loop iteration: 437
2025-03-22 07:02:48,421 - transformer_training - INFO - Main loop iteration: 438
2025-03-22 07:02:48,684 - transformer_training - INFO - Main loop iteration: 439
2025-03-22 07:02:48,963 - transformer_training - INFO - Main loop iteration: 440
Iter 440: loss 7.6115, lr 0.000110, 130904.09 tokens/sec
2025-03-22 07:02:49,214 - transformer_training - INFO - Main loop iteration: 441
2025-03-22 07:02:49,475 - transformer_training - INFO - Main loop iteration: 442
2025-03-22 07:02:49,734 - transformer_training - INFO - Main loop iteration: 443
2025-03-22 07:02:50,013 - transformer_training - INFO - Main loop iteration: 444
2025-03-22 07:02:50,263 - transformer_training - INFO - Main loop iteration: 445
2025-03-22 07:02:50,524 - transformer_training - INFO - Main loop iteration: 446
2025-03-22 07:02:50,784 - transformer_training - INFO - Main loop iteration: 447
2025-03-22 07:02:51,062 - transformer_training - INFO - Main loop iteration: 448
2025-03-22 07:02:51,314 - transformer_training - INFO - Main loop iteration: 449
2025-03-22 07:02:51,575 - transformer_training - INFO - Main loop iteration: 450
Iter 450: loss 7.5080, lr 0.000112, 126584.23 tokens/sec
2025-03-22 07:02:51,835 - transformer_training - INFO - Main loop iteration: 451
2025-03-22 07:02:52,114 - transformer_training - INFO - Main loop iteration: 452
2025-03-22 07:02:52,365 - transformer_training - INFO - Main loop iteration: 453
2025-03-22 07:02:52,627 - transformer_training - INFO - Main loop iteration: 454
2025-03-22 07:02:52,887 - transformer_training - INFO - Main loop iteration: 455
2025-03-22 07:02:53,166 - transformer_training - INFO - Main loop iteration: 456
2025-03-22 07:02:53,417 - transformer_training - INFO - Main loop iteration: 457
2025-03-22 07:02:53,679 - transformer_training - INFO - Main loop iteration: 458
2025-03-22 07:02:53,942 - transformer_training - INFO - Main loop iteration: 459
2025-03-22 07:02:54,220 - transformer_training - INFO - Main loop iteration: 460
Iter 460: loss 7.5054, lr 0.000115, 130674.34 tokens/sec
2025-03-22 07:02:54,471 - transformer_training - INFO - Main loop iteration: 461
2025-03-22 07:02:54,732 - transformer_training - INFO - Main loop iteration: 462
2025-03-22 07:02:54,992 - transformer_training - INFO - Main loop iteration: 463
2025-03-22 07:02:55,269 - transformer_training - INFO - Main loop iteration: 464
2025-03-22 07:02:55,520 - transformer_training - INFO - Main loop iteration: 465
2025-03-22 07:02:55,781 - transformer_training - INFO - Main loop iteration: 466
2025-03-22 07:02:56,040 - transformer_training - INFO - Main loop iteration: 467
2025-03-22 07:02:56,318 - transformer_training - INFO - Main loop iteration: 468
2025-03-22 07:02:56,568 - transformer_training - INFO - Main loop iteration: 469
2025-03-22 07:02:56,829 - transformer_training - INFO - Main loop iteration: 470
Iter 470: loss 7.4798, lr 0.000117, 126597.52 tokens/sec
2025-03-22 07:02:57,089 - transformer_training - INFO - Main loop iteration: 471
2025-03-22 07:02:57,367 - transformer_training - INFO - Main loop iteration: 472
2025-03-22 07:02:57,618 - transformer_training - INFO - Main loop iteration: 473
2025-03-22 07:02:57,879 - transformer_training - INFO - Main loop iteration: 474
2025-03-22 07:02:58,139 - transformer_training - INFO - Main loop iteration: 475
2025-03-22 07:02:58,417 - transformer_training - INFO - Main loop iteration: 476
2025-03-22 07:02:58,668 - transformer_training - INFO - Main loop iteration: 477
2025-03-22 07:02:58,929 - transformer_training - INFO - Main loop iteration: 478
2025-03-22 07:02:59,188 - transformer_training - INFO - Main loop iteration: 479
2025-03-22 07:02:59,467 - transformer_training - INFO - Main loop iteration: 480
Iter 480: loss 7.4288, lr 0.000120, 131035.64 tokens/sec
2025-03-22 07:02:59,718 - transformer_training - INFO - Main loop iteration: 481
2025-03-22 07:02:59,979 - transformer_training - INFO - Main loop iteration: 482
2025-03-22 07:03:00,239 - transformer_training - INFO - Main loop iteration: 483
2025-03-22 07:03:00,517 - transformer_training - INFO - Main loop iteration: 484
2025-03-22 07:03:00,767 - transformer_training - INFO - Main loop iteration: 485
2025-03-22 07:03:01,029 - transformer_training - INFO - Main loop iteration: 486
2025-03-22 07:03:01,289 - transformer_training - INFO - Main loop iteration: 487
2025-03-22 07:03:01,567 - transformer_training - INFO - Main loop iteration: 488
2025-03-22 07:03:01,821 - transformer_training - INFO - Main loop iteration: 489
2025-03-22 07:03:02,082 - transformer_training - INFO - Main loop iteration: 490
Iter 490: loss 7.3570, lr 0.000122, 126568.49 tokens/sec
2025-03-22 07:03:02,342 - transformer_training - INFO - Main loop iteration: 491
2025-03-22 07:03:02,624 - transformer_training - INFO - Main loop iteration: 492
2025-03-22 07:03:02,875 - transformer_training - INFO - Main loop iteration: 493
2025-03-22 07:03:03,136 - transformer_training - INFO - Main loop iteration: 494
2025-03-22 07:03:03,396 - transformer_training - INFO - Main loop iteration: 495
2025-03-22 07:03:03,674 - transformer_training - INFO - Main loop iteration: 496
2025-03-22 07:03:03,925 - transformer_training - INFO - Main loop iteration: 497
2025-03-22 07:03:04,186 - transformer_training - INFO - Main loop iteration: 498
2025-03-22 07:03:04,446 - transformer_training - INFO - Main loop iteration: 499
2025-03-22 07:03:04,724 - transformer_training - INFO - Main loop iteration: 500
Iter 500: loss 7.3777, lr 0.000125, 130909.70 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPTYour base folder is: /workspace/GPT

Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 500: train loss 7.3248, val loss 7.2902
New best model saved with val loss: 7.2902
2025-03-22 07:03:27,557 - transformer_training - INFO - Main loop iteration: 501
2025-03-22 07:03:27,829 - transformer_training - INFO - Main loop iteration: 502
2025-03-22 07:03:28,088 - transformer_training - INFO - Main loop iteration: 503
2025-03-22 07:03:28,365 - transformer_training - INFO - Main loop iteration: 504
2025-03-22 07:03:28,615 - transformer_training - INFO - Main loop iteration: 505
2025-03-22 07:03:28,876 - transformer_training - INFO - Main loop iteration: 506
2025-03-22 07:03:29,136 - transformer_training - INFO - Main loop iteration: 507
2025-03-22 07:03:29,413 - transformer_training - INFO - Main loop iteration: 508
2025-03-22 07:03:29,664 - transformer_training - INFO - Main loop iteration: 509
2025-03-22 07:03:29,926 - transformer_training - INFO - Main loop iteration: 510
Iter 510: loss 7.3279, lr 0.000127, 126904.02 tokens/sec
2025-03-22 07:03:30,185 - transformer_training - INFO - Main loop iteration: 511
2025-03-22 07:03:30,462 - transformer_training - INFO - Main loop iteration: 512
2025-03-22 07:03:30,712 - transformer_training - INFO - Main loop iteration: 513
2025-03-22 07:03:30,973 - transformer_training - INFO - Main loop iteration: 514
2025-03-22 07:03:31,234 - transformer_training - INFO - Main loop iteration: 515
2025-03-22 07:03:31,517 - transformer_training - INFO - Main loop iteration: 516
2025-03-22 07:03:31,771 - transformer_training - INFO - Main loop iteration: 517
2025-03-22 07:03:32,032 - transformer_training - INFO - Main loop iteration: 518
2025-03-22 07:03:32,292 - transformer_training - INFO - Main loop iteration: 519
2025-03-22 07:03:32,570 - transformer_training - INFO - Main loop iteration: 520
Iter 520: loss 7.3326, lr 0.000130, 130905.34 tokens/sec
2025-03-22 07:03:32,821 - transformer_training - INFO - Main loop iteration: 521
2025-03-22 07:03:33,082 - transformer_training - INFO - Main loop iteration: 522
2025-03-22 07:03:33,342 - transformer_training - INFO - Main loop iteration: 523
2025-03-22 07:03:33,619 - transformer_training - INFO - Main loop iteration: 524
2025-03-22 07:03:33,870 - transformer_training - INFO - Main loop iteration: 525
2025-03-22 07:03:34,131 - transformer_training - INFO - Main loop iteration: 526
2025-03-22 07:03:34,390 - transformer_training - INFO - Main loop iteration: 527
2025-03-22 07:03:34,668 - transformer_training - INFO - Main loop iteration: 528
2025-03-22 07:03:34,918 - transformer_training - INFO - Main loop iteration: 529
2025-03-22 07:03:35,179 - transformer_training - INFO - Main loop iteration: 530
Iter 530: loss 7.3581, lr 0.000132, 126604.05 tokens/sec
2025-03-22 07:03:35,439 - transformer_training - INFO - Main loop iteration: 531
2025-03-22 07:03:35,716 - transformer_training - INFO - Main loop iteration: 532
2025-03-22 07:03:35,967 - transformer_training - INFO - Main loop iteration: 533
2025-03-22 07:03:36,228 - transformer_training - INFO - Main loop iteration: 534
2025-03-22 07:03:36,487 - transformer_training - INFO - Main loop iteration: 535
2025-03-22 07:03:36,764 - transformer_training - INFO - Main loop iteration: 536
2025-03-22 07:03:37,015 - transformer_training - INFO - Main loop iteration: 537
2025-03-22 07:03:37,276 - transformer_training - INFO - Main loop iteration: 538
2025-03-22 07:03:37,535 - transformer_training - INFO - Main loop iteration: 539
2025-03-22 07:03:37,813 - transformer_training - INFO - Main loop iteration: 540
Iter 540: loss 7.2392, lr 0.000135, 131042.88 tokens/sec
2025-03-22 07:03:38,063 - transformer_training - INFO - Main loop iteration: 541
2025-03-22 07:03:38,329 - transformer_training - INFO - Main loop iteration: 542
2025-03-22 07:03:38,588 - transformer_training - INFO - Main loop iteration: 543
2025-03-22 07:03:38,865 - transformer_training - INFO - Main loop iteration: 544
2025-03-22 07:03:39,116 - transformer_training - INFO - Main loop iteration: 545
2025-03-22 07:03:39,377 - transformer_training - INFO - Main loop iteration: 546
2025-03-22 07:03:39,636 - transformer_training - INFO - Main loop iteration: 547
2025-03-22 07:03:39,913 - transformer_training - INFO - Main loop iteration: 548
2025-03-22 07:03:40,164 - transformer_training - INFO - Main loop iteration: 549
2025-03-22 07:03:40,425 - transformer_training - INFO - Main loop iteration: 550
Iter 550: loss 7.2518, lr 0.000137, 126660.76 tokens/sec
2025-03-22 07:03:40,684 - transformer_training - INFO - Main loop iteration: 551
2025-03-22 07:03:40,962 - transformer_training - INFO - Main loop iteration: 552
2025-03-22 07:03:41,212 - transformer_training - INFO - Main loop iteration: 553
2025-03-22 07:03:41,473 - transformer_training - INFO - Main loop iteration: 554
2025-03-22 07:03:41,732 - transformer_training - INFO - Main loop iteration: 555
2025-03-22 07:03:42,010 - transformer_training - INFO - Main loop iteration: 556
2025-03-22 07:03:42,260 - transformer_training - INFO - Main loop iteration: 557
2025-03-22 07:03:42,521 - transformer_training - INFO - Main loop iteration: 558
2025-03-22 07:03:42,780 - transformer_training - INFO - Main loop iteration: 559
2025-03-22 07:03:43,058 - transformer_training - INFO - Main loop iteration: 560
Iter 560: loss 7.2127, lr 0.000140, 130980.69 tokens/sec
2025-03-22 07:03:43,309 - transformer_training - INFO - Main loop iteration: 561
2025-03-22 07:03:43,570 - transformer_training - INFO - Main loop iteration: 562
2025-03-22 07:03:43,829 - transformer_training - INFO - Main loop iteration: 563
2025-03-22 07:03:44,107 - transformer_training - INFO - Main loop iteration: 564
2025-03-22 07:03:44,358 - transformer_training - INFO - Main loop iteration: 565
2025-03-22 07:03:44,619 - transformer_training - INFO - Main loop iteration: 566
2025-03-22 07:03:44,878 - transformer_training - INFO - Main loop iteration: 567
2025-03-22 07:03:45,156 - transformer_training - INFO - Main loop iteration: 568
2025-03-22 07:03:45,407 - transformer_training - INFO - Main loop iteration: 569
2025-03-22 07:03:45,668 - transformer_training - INFO - Main loop iteration: 570
Iter 570: loss 7.1421, lr 0.000142, 126672.90 tokens/sec
2025-03-22 07:03:45,927 - transformer_training - INFO - Main loop iteration: 571
2025-03-22 07:03:46,205 - transformer_training - INFO - Main loop iteration: 572
2025-03-22 07:03:46,457 - transformer_training - INFO - Main loop iteration: 573
2025-03-22 07:03:46,719 - transformer_training - INFO - Main loop iteration: 574
2025-03-22 07:03:46,980 - transformer_training - INFO - Main loop iteration: 575
2025-03-22 07:03:47,257 - transformer_training - INFO - Main loop iteration: 576
2025-03-22 07:03:47,508 - transformer_training - INFO - Main loop iteration: 577
2025-03-22 07:03:47,769 - transformer_training - INFO - Main loop iteration: 578
2025-03-22 07:03:48,028 - transformer_training - INFO - Main loop iteration: 579
2025-03-22 07:03:48,306 - transformer_training - INFO - Main loop iteration: 580
Iter 580: loss 7.1438, lr 0.000145, 131075.88 tokens/sec
2025-03-22 07:03:48,557 - transformer_training - INFO - Main loop iteration: 581
2025-03-22 07:03:48,817 - transformer_training - INFO - Main loop iteration: 582
2025-03-22 07:03:49,077 - transformer_training - INFO - Main loop iteration: 583
2025-03-22 07:03:49,354 - transformer_training - INFO - Main loop iteration: 584
2025-03-22 07:03:49,604 - transformer_training - INFO - Main loop iteration: 585
2025-03-22 07:03:49,866 - transformer_training - INFO - Main loop iteration: 586
2025-03-22 07:03:50,125 - transformer_training - INFO - Main loop iteration: 587
2025-03-22 07:03:50,402 - transformer_training - INFO - Main loop iteration: 588
2025-03-22 07:03:50,652 - transformer_training - INFO - Main loop iteration: 589
2025-03-22 07:03:50,913 - transformer_training - INFO - Main loop iteration: 590
Iter 590: loss 7.1136, lr 0.000147, 126697.89 tokens/sec
2025-03-22 07:03:51,173 - transformer_training - INFO - Main loop iteration: 591
2025-03-22 07:03:51,450 - transformer_training - INFO - Main loop iteration: 592
2025-03-22 07:03:51,701 - transformer_training - INFO - Main loop iteration: 593
2025-03-22 07:03:51,962 - transformer_training - INFO - Main loop iteration: 594
2025-03-22 07:03:52,221 - transformer_training - INFO - Main loop iteration: 595
2025-03-22 07:03:52,499 - transformer_training - INFO - Main loop iteration: 596
2025-03-22 07:03:52,750 - transformer_training - INFO - Main loop iteration: 597
2025-03-22 07:03:53,011 - transformer_training - INFO - Main loop iteration: 598
2025-03-22 07:03:53,270 - transformer_training - INFO - Main loop iteration: 599
2025-03-22 07:03:53,547 - transformer_training - INFO - Main loop iteration: 600
Iter 600: loss 7.0677, lr 0.000150, 131000.54 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 600: train loss 7.0275, val loss 7.0017
New best model saved with val loss: 7.0017
2025-03-22 07:04:13,959 - transformer_training - INFO - Main loop iteration: 601
2025-03-22 07:04:14,236 - transformer_training - INFO - Main loop iteration: 602
2025-03-22 07:04:14,496 - transformer_training - INFO - Main loop iteration: 603
2025-03-22 07:04:14,773 - transformer_training - INFO - Main loop iteration: 604
2025-03-22 07:04:15,024 - transformer_training - INFO - Main loop iteration: 605
2025-03-22 07:04:15,286 - transformer_training - INFO - Main loop iteration: 606
2025-03-22 07:04:15,547 - transformer_training - INFO - Main loop iteration: 607
2025-03-22 07:04:15,825 - transformer_training - INFO - Main loop iteration: 608
2025-03-22 07:04:16,076 - transformer_training - INFO - Main loop iteration: 609
2025-03-22 07:04:16,337 - transformer_training - INFO - Main loop iteration: 610
Iter 610: loss 7.0456, lr 0.000152, 125557.57 tokens/sec
2025-03-22 07:04:16,599 - transformer_training - INFO - Main loop iteration: 611
2025-03-22 07:04:16,882 - transformer_training - INFO - Main loop iteration: 612
2025-03-22 07:04:17,132 - transformer_training - INFO - Main loop iteration: 613
2025-03-22 07:04:17,394 - transformer_training - INFO - Main loop iteration: 614
2025-03-22 07:04:17,653 - transformer_training - INFO - Main loop iteration: 615
2025-03-22 07:04:17,931 - transformer_training - INFO - Main loop iteration: 616
2025-03-22 07:04:18,181 - transformer_training - INFO - Main loop iteration: 617
2025-03-22 07:04:18,443 - transformer_training - INFO - Main loop iteration: 618
2025-03-22 07:04:18,702 - transformer_training - INFO - Main loop iteration: 619
2025-03-22 07:04:18,986 - transformer_training - INFO - Main loop iteration: 620
Iter 620: loss 7.0208, lr 0.000155, 131079.50 tokens/sec
2025-03-22 07:04:19,236 - transformer_training - INFO - Main loop iteration: 621
2025-03-22 07:04:19,497 - transformer_training - INFO - Main loop iteration: 622
2025-03-22 07:04:19,761 - transformer_training - INFO - Main loop iteration: 623
2025-03-22 07:04:20,050 - transformer_training - INFO - Main loop iteration: 624
2025-03-22 07:04:20,301 - transformer_training - INFO - Main loop iteration: 625
2025-03-22 07:04:20,562 - transformer_training - INFO - Main loop iteration: 626
2025-03-22 07:04:20,822 - transformer_training - INFO - Main loop iteration: 627
2025-03-22 07:04:21,105 - transformer_training - INFO - Main loop iteration: 628
2025-03-22 07:04:21,355 - transformer_training - INFO - Main loop iteration: 629
2025-03-22 07:04:21,616 - transformer_training - INFO - Main loop iteration: 630
Iter 630: loss 6.9611, lr 0.000157, 126497.32 tokens/sec
2025-03-22 07:04:21,876 - transformer_training - INFO - Main loop iteration: 631
2025-03-22 07:04:22,154 - transformer_training - INFO - Main loop iteration: 632
2025-03-22 07:04:22,405 - transformer_training - INFO - Main loop iteration: 633
2025-03-22 07:04:22,666 - transformer_training - INFO - Main loop iteration: 634
2025-03-22 07:04:22,926 - transformer_training - INFO - Main loop iteration: 635
2025-03-22 07:04:23,205 - transformer_training - INFO - Main loop iteration: 636
2025-03-22 07:04:23,457 - transformer_training - INFO - Main loop iteration: 637
2025-03-22 07:04:23,718 - transformer_training - INFO - Main loop iteration: 638
2025-03-22 07:04:23,980 - transformer_training - INFO - Main loop iteration: 639
2025-03-22 07:04:24,262 - transformer_training - INFO - Main loop iteration: 640
Iter 640: loss 7.0111, lr 0.000160, 131146.79 tokens/sec
2025-03-22 07:04:24,513 - transformer_training - INFO - Main loop iteration: 641
2025-03-22 07:04:24,774 - transformer_training - INFO - Main loop iteration: 642
2025-03-22 07:04:25,033 - transformer_training - INFO - Main loop iteration: 643
2025-03-22 07:04:25,311 - transformer_training - INFO - Main loop iteration: 644
2025-03-22 07:04:25,561 - transformer_training - INFO - Main loop iteration: 645
2025-03-22 07:04:25,823 - transformer_training - INFO - Main loop iteration: 646
2025-03-22 07:04:26,082 - transformer_training - INFO - Main loop iteration: 647
2025-03-22 07:04:26,360 - transformer_training - INFO - Main loop iteration: 648
2025-03-22 07:04:26,610 - transformer_training - INFO - Main loop iteration: 649
2025-03-22 07:04:26,872 - transformer_training - INFO - Main loop iteration: 650
Iter 650: loss 6.9527, lr 0.000162, 126552.64 tokens/sec
2025-03-22 07:04:27,132 - transformer_training - INFO - Main loop iteration: 651
2025-03-22 07:04:27,410 - transformer_training - INFO - Main loop iteration: 652
2025-03-22 07:04:27,661 - transformer_training - INFO - Main loop iteration: 653
2025-03-22 07:04:27,922 - transformer_training - INFO - Main loop iteration: 654
2025-03-22 07:04:28,182 - transformer_training - INFO - Main loop iteration: 655
2025-03-22 07:04:28,464 - transformer_training - INFO - Main loop iteration: 656
2025-03-22 07:04:28,716 - transformer_training - INFO - Main loop iteration: 657
2025-03-22 07:04:28,977 - transformer_training - INFO - Main loop iteration: 658
2025-03-22 07:04:29,237 - transformer_training - INFO - Main loop iteration: 659
2025-03-22 07:04:29,514 - transformer_training - INFO - Main loop iteration: 660
Iter 660: loss 6.9344, lr 0.000165, 130476.72 tokens/sec
2025-03-22 07:04:29,766 - transformer_training - INFO - Main loop iteration: 661
2025-03-22 07:04:30,027 - transformer_training - INFO - Main loop iteration: 662
2025-03-22 07:04:30,287 - transformer_training - INFO - Main loop iteration: 663
2025-03-22 07:04:30,565 - transformer_training - INFO - Main loop iteration: 664
2025-03-22 07:04:30,816 - transformer_training - INFO - Main loop iteration: 665
2025-03-22 07:04:31,078 - transformer_training - INFO - Main loop iteration: 666
2025-03-22 07:04:31,337 - transformer_training - INFO - Main loop iteration: 667
2025-03-22 07:04:31,615 - transformer_training - INFO - Main loop iteration: 668
2025-03-22 07:04:31,865 - transformer_training - INFO - Main loop iteration: 669
2025-03-22 07:04:32,126 - transformer_training - INFO - Main loop iteration: 670
Iter 670: loss 6.9237, lr 0.000167, 126516.76 tokens/sec
2025-03-22 07:04:32,386 - transformer_training - INFO - Main loop iteration: 671
2025-03-22 07:04:32,664 - transformer_training - INFO - Main loop iteration: 672
2025-03-22 07:04:32,915 - transformer_training - INFO - Main loop iteration: 673
2025-03-22 07:04:33,176 - transformer_training - INFO - Main loop iteration: 674
2025-03-22 07:04:33,440 - transformer_training - INFO - Main loop iteration: 675
2025-03-22 07:04:33,722 - transformer_training - INFO - Main loop iteration: 676
2025-03-22 07:04:33,973 - transformer_training - INFO - Main loop iteration: 677
2025-03-22 07:04:34,234 - transformer_training - INFO - Main loop iteration: 678
2025-03-22 07:04:34,494 - transformer_training - INFO - Main loop iteration: 679
2025-03-22 07:04:34,772 - transformer_training - INFO - Main loop iteration: 680
Iter 680: loss 6.8137, lr 0.000170, 130825.34 tokens/sec
2025-03-22 07:04:35,024 - transformer_training - INFO - Main loop iteration: 681
2025-03-22 07:04:35,285 - transformer_training - INFO - Main loop iteration: 682
2025-03-22 07:04:35,545 - transformer_training - INFO - Main loop iteration: 683
2025-03-22 07:04:35,823 - transformer_training - INFO - Main loop iteration: 684
2025-03-22 07:04:36,073 - transformer_training - INFO - Main loop iteration: 685
2025-03-22 07:04:36,335 - transformer_training - INFO - Main loop iteration: 686
2025-03-22 07:04:36,596 - transformer_training - INFO - Main loop iteration: 687
2025-03-22 07:04:36,873 - transformer_training - INFO - Main loop iteration: 688
2025-03-22 07:04:37,124 - transformer_training - INFO - Main loop iteration: 689
2025-03-22 07:04:37,385 - transformer_training - INFO - Main loop iteration: 690
Iter 690: loss 6.8686, lr 0.000172, 126490.21 tokens/sec
2025-03-22 07:04:37,646 - transformer_training - INFO - Main loop iteration: 691
2025-03-22 07:04:37,923 - transformer_training - INFO - Main loop iteration: 692
2025-03-22 07:04:38,173 - transformer_training - INFO - Main loop iteration: 693
2025-03-22 07:04:38,434 - transformer_training - INFO - Main loop iteration: 694
2025-03-22 07:04:38,694 - transformer_training - INFO - Main loop iteration: 695
2025-03-22 07:04:38,972 - transformer_training - INFO - Main loop iteration: 696
2025-03-22 07:04:39,222 - transformer_training - INFO - Main loop iteration: 697
2025-03-22 07:04:39,484 - transformer_training - INFO - Main loop iteration: 698
2025-03-22 07:04:39,743 - transformer_training - INFO - Main loop iteration: 699
2025-03-22 07:04:40,021 - transformer_training - INFO - Main loop iteration: 700
Iter 700: loss 6.8079, lr 0.000175, 130900.85 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 700: train loss 6.7735, val loss 6.7642
New best model saved with val loss: 6.7642
2025-03-22 07:05:00,672 - transformer_training - INFO - Main loop iteration: 701
2025-03-22 07:05:00,949 - transformer_training - INFO - Main loop iteration: 702
2025-03-22 07:05:01,210 - transformer_training - INFO - Main loop iteration: 703
2025-03-22 07:05:01,488 - transformer_training - INFO - Main loop iteration: 704
2025-03-22 07:05:01,738 - transformer_training - INFO - Main loop iteration: 705
2025-03-22 07:05:02,000 - transformer_training - INFO - Main loop iteration: 706
2025-03-22 07:05:02,259 - transformer_training - INFO - Main loop iteration: 707
2025-03-22 07:05:02,538 - transformer_training - INFO - Main loop iteration: 708
2025-03-22 07:05:02,788 - transformer_training - INFO - Main loop iteration: 709
2025-03-22 07:05:03,050 - transformer_training - INFO - Main loop iteration: 710
Iter 710: loss 6.7775, lr 0.000177, 126502.21 tokens/sec
2025-03-22 07:05:03,310 - transformer_training - INFO - Main loop iteration: 711
2025-03-22 07:05:03,587 - transformer_training - INFO - Main loop iteration: 712
2025-03-22 07:05:03,838 - transformer_training - INFO - Main loop iteration: 713
2025-03-22 07:05:04,099 - transformer_training - INFO - Main loop iteration: 714
2025-03-22 07:05:04,358 - transformer_training - INFO - Main loop iteration: 715
2025-03-22 07:05:04,635 - transformer_training - INFO - Main loop iteration: 716
2025-03-22 07:05:04,888 - transformer_training - INFO - Main loop iteration: 717
2025-03-22 07:05:05,149 - transformer_training - INFO - Main loop iteration: 718
2025-03-22 07:05:05,409 - transformer_training - INFO - Main loop iteration: 719
2025-03-22 07:05:05,694 - transformer_training - INFO - Main loop iteration: 720
Iter 720: loss 6.7384, lr 0.000180, 130730.52 tokens/sec
2025-03-22 07:05:05,945 - transformer_training - INFO - Main loop iteration: 721
2025-03-22 07:05:06,214 - transformer_training - INFO - Main loop iteration: 722
2025-03-22 07:05:06,477 - transformer_training - INFO - Main loop iteration: 723
2025-03-22 07:05:06,754 - transformer_training - INFO - Main loop iteration: 724
2025-03-22 07:05:07,004 - transformer_training - INFO - Main loop iteration: 725
2025-03-22 07:05:07,265 - transformer_training - INFO - Main loop iteration: 726
2025-03-22 07:05:07,524 - transformer_training - INFO - Main loop iteration: 727
2025-03-22 07:05:07,801 - transformer_training - INFO - Main loop iteration: 728
2025-03-22 07:05:08,052 - transformer_training - INFO - Main loop iteration: 729
2025-03-22 07:05:08,313 - transformer_training - INFO - Main loop iteration: 730
Iter 730: loss 6.8668, lr 0.000182, 126830.47 tokens/sec
2025-03-22 07:05:08,572 - transformer_training - INFO - Main loop iteration: 731
2025-03-22 07:05:08,849 - transformer_training - INFO - Main loop iteration: 732
2025-03-22 07:05:09,099 - transformer_training - INFO - Main loop iteration: 733
2025-03-22 07:05:09,360 - transformer_training - INFO - Main loop iteration: 734
2025-03-22 07:05:09,619 - transformer_training - INFO - Main loop iteration: 735
2025-03-22 07:05:09,896 - transformer_training - INFO - Main loop iteration: 736
2025-03-22 07:05:10,147 - transformer_training - INFO - Main loop iteration: 737
2025-03-22 07:05:10,408 - transformer_training - INFO - Main loop iteration: 738
2025-03-22 07:05:10,667 - transformer_training - INFO - Main loop iteration: 739
2025-03-22 07:05:10,944 - transformer_training - INFO - Main loop iteration: 740
Iter 740: loss 6.7956, lr 0.000185, 131148.42 tokens/sec
2025-03-22 07:05:11,195 - transformer_training - INFO - Main loop iteration: 741
2025-03-22 07:05:11,460 - transformer_training - INFO - Main loop iteration: 742
2025-03-22 07:05:11,719 - transformer_training - INFO - Main loop iteration: 743
2025-03-22 07:05:11,996 - transformer_training - INFO - Main loop iteration: 744
2025-03-22 07:05:12,247 - transformer_training - INFO - Main loop iteration: 745
2025-03-22 07:05:12,507 - transformer_training - INFO - Main loop iteration: 746
2025-03-22 07:05:12,767 - transformer_training - INFO - Main loop iteration: 747
2025-03-22 07:05:13,044 - transformer_training - INFO - Main loop iteration: 748
2025-03-22 07:05:13,294 - transformer_training - INFO - Main loop iteration: 749
2025-03-22 07:05:13,555 - transformer_training - INFO - Main loop iteration: 750
Iter 750: loss 6.7084, lr 0.000187, 126817.72 tokens/sec
2025-03-22 07:05:13,815 - transformer_training - INFO - Main loop iteration: 751
2025-03-22 07:05:14,092 - transformer_training - INFO - Main loop iteration: 752
2025-03-22 07:05:14,346 - transformer_training - INFO - Main loop iteration: 753
2025-03-22 07:05:14,608 - transformer_training - INFO - Main loop iteration: 754
2025-03-22 07:05:14,867 - transformer_training - INFO - Main loop iteration: 755
2025-03-22 07:05:15,144 - transformer_training - INFO - Main loop iteration: 756
2025-03-22 07:05:15,394 - transformer_training - INFO - Main loop iteration: 757
2025-03-22 07:05:15,655 - transformer_training - INFO - Main loop iteration: 758
2025-03-22 07:05:15,914 - transformer_training - INFO - Main loop iteration: 759
2025-03-22 07:05:16,191 - transformer_training - INFO - Main loop iteration: 760
Iter 760: loss 6.6570, lr 0.000190, 130926.91 tokens/sec
2025-03-22 07:05:16,442 - transformer_training - INFO - Main loop iteration: 761
2025-03-22 07:05:16,703 - transformer_training - INFO - Main loop iteration: 762
2025-03-22 07:05:16,962 - transformer_training - INFO - Main loop iteration: 763
2025-03-22 07:05:17,239 - transformer_training - INFO - Main loop iteration: 764
2025-03-22 07:05:17,490 - transformer_training - INFO - Main loop iteration: 765
2025-03-22 07:05:17,751 - transformer_training - INFO - Main loop iteration: 766
2025-03-22 07:05:18,010 - transformer_training - INFO - Main loop iteration: 767
2025-03-22 07:05:18,288 - transformer_training - INFO - Main loop iteration: 768
2025-03-22 07:05:18,538 - transformer_training - INFO - Main loop iteration: 769
2025-03-22 07:05:18,799 - transformer_training - INFO - Main loop iteration: 770
Iter 770: loss 6.6824, lr 0.000192, 126662.62 tokens/sec
2025-03-22 07:05:19,059 - transformer_training - INFO - Main loop iteration: 771
2025-03-22 07:05:19,336 - transformer_training - INFO - Main loop iteration: 772
2025-03-22 07:05:19,586 - transformer_training - INFO - Main loop iteration: 773
2025-03-22 07:05:19,847 - transformer_training - INFO - Main loop iteration: 774
2025-03-22 07:05:20,107 - transformer_training - INFO - Main loop iteration: 775
2025-03-22 07:05:20,384 - transformer_training - INFO - Main loop iteration: 776
2025-03-22 07:05:20,635 - transformer_training - INFO - Main loop iteration: 777
2025-03-22 07:05:20,895 - transformer_training - INFO - Main loop iteration: 778
2025-03-22 07:05:21,155 - transformer_training - INFO - Main loop iteration: 779
2025-03-22 07:05:21,432 - transformer_training - INFO - Main loop iteration: 780
Iter 780: loss 6.6838, lr 0.000195, 130912.57 tokens/sec
2025-03-22 07:05:21,683 - transformer_training - INFO - Main loop iteration: 781
2025-03-22 07:05:21,944 - transformer_training - INFO - Main loop iteration: 782
2025-03-22 07:05:22,204 - transformer_training - INFO - Main loop iteration: 783
2025-03-22 07:05:22,481 - transformer_training - INFO - Main loop iteration: 784
2025-03-22 07:05:22,731 - transformer_training - INFO - Main loop iteration: 785
2025-03-22 07:05:22,993 - transformer_training - INFO - Main loop iteration: 786
2025-03-22 07:05:23,252 - transformer_training - INFO - Main loop iteration: 787
2025-03-22 07:05:23,530 - transformer_training - INFO - Main loop iteration: 788
2025-03-22 07:05:23,780 - transformer_training - INFO - Main loop iteration: 789
2025-03-22 07:05:24,041 - transformer_training - INFO - Main loop iteration: 790
Iter 790: loss 6.6451, lr 0.000197, 126796.89 tokens/sec
2025-03-22 07:05:24,300 - transformer_training - INFO - Main loop iteration: 791
2025-03-22 07:05:24,577 - transformer_training - INFO - Main loop iteration: 792
2025-03-22 07:05:24,827 - transformer_training - INFO - Main loop iteration: 793
2025-03-22 07:05:25,088 - transformer_training - INFO - Main loop iteration: 794
2025-03-22 07:05:25,348 - transformer_training - INFO - Main loop iteration: 795
2025-03-22 07:05:25,625 - transformer_training - INFO - Main loop iteration: 796
2025-03-22 07:05:25,876 - transformer_training - INFO - Main loop iteration: 797
2025-03-22 07:05:26,137 - transformer_training - INFO - Main loop iteration: 798
2025-03-22 07:05:26,397 - transformer_training - INFO - Main loop iteration: 799
2025-03-22 07:05:26,674 - transformer_training - INFO - Main loop iteration: 800
Iter 800: loss 6.7023, lr 0.000200, 131095.88 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 800: train loss 6.5704, val loss 6.5735
New best model saved with val loss: 6.5735
2025-03-22 07:05:47,238 - transformer_training - INFO - Main loop iteration: 801
2025-03-22 07:05:47,517 - transformer_training - INFO - Main loop iteration: 802
2025-03-22 07:05:47,784 - transformer_training - INFO - Main loop iteration: 803
2025-03-22 07:05:48,066 - transformer_training - INFO - Main loop iteration: 804
2025-03-22 07:05:48,316 - transformer_training - INFO - Main loop iteration: 805
2025-03-22 07:05:48,578 - transformer_training - INFO - Main loop iteration: 806
2025-03-22 07:05:48,837 - transformer_training - INFO - Main loop iteration: 807
2025-03-22 07:05:49,114 - transformer_training - INFO - Main loop iteration: 808
2025-03-22 07:05:49,366 - transformer_training - INFO - Main loop iteration: 809
2025-03-22 07:05:49,626 - transformer_training - INFO - Main loop iteration: 810
Iter 810: loss 6.5808, lr 0.000202, 126896.17 tokens/sec
2025-03-22 07:05:49,886 - transformer_training - INFO - Main loop iteration: 811
2025-03-22 07:05:50,163 - transformer_training - INFO - Main loop iteration: 812
2025-03-22 07:05:50,413 - transformer_training - INFO - Main loop iteration: 813
2025-03-22 07:05:50,672 - transformer_training - INFO - Main loop iteration: 814
2025-03-22 07:05:50,931 - transformer_training - INFO - Main loop iteration: 815
2025-03-22 07:05:51,208 - transformer_training - INFO - Main loop iteration: 816
2025-03-22 07:05:51,457 - transformer_training - INFO - Main loop iteration: 817
2025-03-22 07:05:51,717 - transformer_training - INFO - Main loop iteration: 818
2025-03-22 07:05:51,976 - transformer_training - INFO - Main loop iteration: 819
2025-03-22 07:05:52,253 - transformer_training - INFO - Main loop iteration: 820
Iter 820: loss 6.6134, lr 0.000205, 131632.64 tokens/sec
2025-03-22 07:05:52,503 - transformer_training - INFO - Main loop iteration: 821
2025-03-22 07:05:52,763 - transformer_training - INFO - Main loop iteration: 822
2025-03-22 07:05:53,021 - transformer_training - INFO - Main loop iteration: 823
2025-03-22 07:05:53,298 - transformer_training - INFO - Main loop iteration: 824
2025-03-22 07:05:53,547 - transformer_training - INFO - Main loop iteration: 825
2025-03-22 07:05:53,808 - transformer_training - INFO - Main loop iteration: 826
2025-03-22 07:05:54,067 - transformer_training - INFO - Main loop iteration: 827
2025-03-22 07:05:54,344 - transformer_training - INFO - Main loop iteration: 828
2025-03-22 07:05:54,594 - transformer_training - INFO - Main loop iteration: 829
2025-03-22 07:05:54,854 - transformer_training - INFO - Main loop iteration: 830
Iter 830: loss 6.5416, lr 0.000207, 127089.43 tokens/sec
2025-03-22 07:05:55,113 - transformer_training - INFO - Main loop iteration: 831
2025-03-22 07:05:55,390 - transformer_training - INFO - Main loop iteration: 832
2025-03-22 07:05:55,640 - transformer_training - INFO - Main loop iteration: 833
2025-03-22 07:05:55,901 - transformer_training - INFO - Main loop iteration: 834
2025-03-22 07:05:56,160 - transformer_training - INFO - Main loop iteration: 835
2025-03-22 07:05:56,437 - transformer_training - INFO - Main loop iteration: 836
2025-03-22 07:05:56,686 - transformer_training - INFO - Main loop iteration: 837
2025-03-22 07:05:56,947 - transformer_training - INFO - Main loop iteration: 838
2025-03-22 07:05:57,205 - transformer_training - INFO - Main loop iteration: 839
2025-03-22 07:05:57,482 - transformer_training - INFO - Main loop iteration: 840
Iter 840: loss 6.5885, lr 0.000210, 131204.38 tokens/sec
2025-03-22 07:05:57,733 - transformer_training - INFO - Main loop iteration: 841
2025-03-22 07:05:57,993 - transformer_training - INFO - Main loop iteration: 842
2025-03-22 07:05:58,251 - transformer_training - INFO - Main loop iteration: 843
2025-03-22 07:05:58,528 - transformer_training - INFO - Main loop iteration: 844
2025-03-22 07:05:58,778 - transformer_training - INFO - Main loop iteration: 845
2025-03-22 07:05:59,038 - transformer_training - INFO - Main loop iteration: 846
2025-03-22 07:05:59,297 - transformer_training - INFO - Main loop iteration: 847
2025-03-22 07:05:59,574 - transformer_training - INFO - Main loop iteration: 848
2025-03-22 07:05:59,823 - transformer_training - INFO - Main loop iteration: 849
2025-03-22 07:06:00,084 - transformer_training - INFO - Main loop iteration: 850
Iter 850: loss 6.5130, lr 0.000212, 127003.69 tokens/sec
2025-03-22 07:06:00,343 - transformer_training - INFO - Main loop iteration: 851
2025-03-22 07:06:00,620 - transformer_training - INFO - Main loop iteration: 852
2025-03-22 07:06:00,869 - transformer_training - INFO - Main loop iteration: 853
2025-03-22 07:06:01,130 - transformer_training - INFO - Main loop iteration: 854
2025-03-22 07:06:01,389 - transformer_training - INFO - Main loop iteration: 855
2025-03-22 07:06:01,666 - transformer_training - INFO - Main loop iteration: 856
2025-03-22 07:06:01,916 - transformer_training - INFO - Main loop iteration: 857
2025-03-22 07:06:02,176 - transformer_training - INFO - Main loop iteration: 858
2025-03-22 07:06:02,435 - transformer_training - INFO - Main loop iteration: 859
2025-03-22 07:06:02,712 - transformer_training - INFO - Main loop iteration: 860
Iter 860: loss 6.5549, lr 0.000215, 131610.33 tokens/sec
2025-03-22 07:06:02,962 - transformer_training - INFO - Main loop iteration: 861
2025-03-22 07:06:03,222 - transformer_training - INFO - Main loop iteration: 862
2025-03-22 07:06:03,480 - transformer_training - INFO - Main loop iteration: 863
2025-03-22 07:06:03,757 - transformer_training - INFO - Main loop iteration: 864
2025-03-22 07:06:04,007 - transformer_training - INFO - Main loop iteration: 865
2025-03-22 07:06:04,267 - transformer_training - INFO - Main loop iteration: 866
2025-03-22 07:06:04,526 - transformer_training - INFO - Main loop iteration: 867
2025-03-22 07:06:04,803 - transformer_training - INFO - Main loop iteration: 868
2025-03-22 07:06:05,053 - transformer_training - INFO - Main loop iteration: 869
2025-03-22 07:06:05,313 - transformer_training - INFO - Main loop iteration: 870
Iter 870: loss 6.4753, lr 0.000217, 127045.84 tokens/sec
2025-03-22 07:06:05,572 - transformer_training - INFO - Main loop iteration: 871
2025-03-22 07:06:05,849 - transformer_training - INFO - Main loop iteration: 872
2025-03-22 07:06:06,098 - transformer_training - INFO - Main loop iteration: 873
2025-03-22 07:06:06,359 - transformer_training - INFO - Main loop iteration: 874
2025-03-22 07:06:06,617 - transformer_training - INFO - Main loop iteration: 875
2025-03-22 07:06:06,895 - transformer_training - INFO - Main loop iteration: 876
2025-03-22 07:06:07,148 - transformer_training - INFO - Main loop iteration: 877
2025-03-22 07:06:07,408 - transformer_training - INFO - Main loop iteration: 878
2025-03-22 07:06:07,667 - transformer_training - INFO - Main loop iteration: 879
2025-03-22 07:06:07,944 - transformer_training - INFO - Main loop iteration: 880
Iter 880: loss 6.5015, lr 0.000220, 130828.70 tokens/sec
2025-03-22 07:06:08,195 - transformer_training - INFO - Main loop iteration: 881
2025-03-22 07:06:08,463 - transformer_training - INFO - Main loop iteration: 882
2025-03-22 07:06:08,723 - transformer_training - INFO - Main loop iteration: 883
2025-03-22 07:06:09,001 - transformer_training - INFO - Main loop iteration: 884
2025-03-22 07:06:09,252 - transformer_training - INFO - Main loop iteration: 885
2025-03-22 07:06:09,514 - transformer_training - INFO - Main loop iteration: 886
2025-03-22 07:06:09,774 - transformer_training - INFO - Main loop iteration: 887
2025-03-22 07:06:10,052 - transformer_training - INFO - Main loop iteration: 888
2025-03-22 07:06:10,303 - transformer_training - INFO - Main loop iteration: 889
2025-03-22 07:06:10,565 - transformer_training - INFO - Main loop iteration: 890
Iter 890: loss 6.4484, lr 0.000222, 126309.91 tokens/sec
2025-03-22 07:06:10,825 - transformer_training - INFO - Main loop iteration: 891
2025-03-22 07:06:11,103 - transformer_training - INFO - Main loop iteration: 892
2025-03-22 07:06:11,354 - transformer_training - INFO - Main loop iteration: 893
2025-03-22 07:06:11,616 - transformer_training - INFO - Main loop iteration: 894
2025-03-22 07:06:11,877 - transformer_training - INFO - Main loop iteration: 895
2025-03-22 07:06:12,265 - transformer_training - INFO - Main loop iteration: 896
2025-03-22 07:06:12,516 - transformer_training - INFO - Main loop iteration: 897
2025-03-22 07:06:12,778 - transformer_training - INFO - Main loop iteration: 898
2025-03-22 07:06:13,037 - transformer_training - INFO - Main loop iteration: 899
2025-03-22 07:06:13,315 - transformer_training - INFO - Main loop iteration: 900
Iter 900: loss 6.4424, lr 0.000225, 130794.84 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 900: train loss 6.4167, val loss 6.4054
New best model saved with val loss: 6.4054
2025-03-22 07:06:33,384 - transformer_training - INFO - Main loop iteration: 901
2025-03-22 07:06:33,635 - transformer_training - INFO - Main loop iteration: 902
2025-03-22 07:06:33,895 - transformer_training - INFO - Main loop iteration: 903
2025-03-22 07:06:34,170 - transformer_training - INFO - Main loop iteration: 904
2025-03-22 07:06:34,420 - transformer_training - INFO - Main loop iteration: 905
2025-03-22 07:06:34,680 - transformer_training - INFO - Main loop iteration: 906
2025-03-22 07:06:34,938 - transformer_training - INFO - Main loop iteration: 907
2025-03-22 07:06:35,215 - transformer_training - INFO - Main loop iteration: 908
2025-03-22 07:06:35,464 - transformer_training - INFO - Main loop iteration: 909
2025-03-22 07:06:35,725 - transformer_training - INFO - Main loop iteration: 910
Iter 910: loss 6.4586, lr 0.000227, 127149.27 tokens/sec
2025-03-22 07:06:35,983 - transformer_training - INFO - Main loop iteration: 911
2025-03-22 07:06:36,260 - transformer_training - INFO - Main loop iteration: 912
2025-03-22 07:06:36,509 - transformer_training - INFO - Main loop iteration: 913
2025-03-22 07:06:36,770 - transformer_training - INFO - Main loop iteration: 914
2025-03-22 07:06:37,028 - transformer_training - INFO - Main loop iteration: 915
2025-03-22 07:06:37,305 - transformer_training - INFO - Main loop iteration: 916
2025-03-22 07:06:37,554 - transformer_training - INFO - Main loop iteration: 917
2025-03-22 07:06:37,815 - transformer_training - INFO - Main loop iteration: 918
2025-03-22 07:06:38,073 - transformer_training - INFO - Main loop iteration: 919
2025-03-22 07:06:38,351 - transformer_training - INFO - Main loop iteration: 920
Iter 920: loss 6.3594, lr 0.000230, 131865.40 tokens/sec
2025-03-22 07:06:38,600 - transformer_training - INFO - Main loop iteration: 921
2025-03-22 07:06:38,867 - transformer_training - INFO - Main loop iteration: 922
2025-03-22 07:06:39,126 - transformer_training - INFO - Main loop iteration: 923
2025-03-22 07:06:39,403 - transformer_training - INFO - Main loop iteration: 924
2025-03-22 07:06:39,651 - transformer_training - INFO - Main loop iteration: 925
2025-03-22 07:06:39,912 - transformer_training - INFO - Main loop iteration: 926
2025-03-22 07:06:40,171 - transformer_training - INFO - Main loop iteration: 927
2025-03-22 07:06:40,447 - transformer_training - INFO - Main loop iteration: 928
2025-03-22 07:06:40,709 - transformer_training - INFO - Main loop iteration: 929
2025-03-22 07:06:40,996 - transformer_training - INFO - Main loop iteration: 930
Iter 930: loss 6.4239, lr 0.000232, 125992.53 tokens/sec
2025-03-22 07:06:41,257 - transformer_training - INFO - Main loop iteration: 931
2025-03-22 07:06:41,534 - transformer_training - INFO - Main loop iteration: 932
2025-03-22 07:06:41,783 - transformer_training - INFO - Main loop iteration: 933
2025-03-22 07:06:42,043 - transformer_training - INFO - Main loop iteration: 934
2025-03-22 07:06:42,302 - transformer_training - INFO - Main loop iteration: 935
2025-03-22 07:06:42,579 - transformer_training - INFO - Main loop iteration: 936
2025-03-22 07:06:42,829 - transformer_training - INFO - Main loop iteration: 937
2025-03-22 07:06:43,090 - transformer_training - INFO - Main loop iteration: 938
2025-03-22 07:06:43,349 - transformer_training - INFO - Main loop iteration: 939
2025-03-22 07:06:43,626 - transformer_training - INFO - Main loop iteration: 940
Iter 940: loss 6.4045, lr 0.000235, 129759.89 tokens/sec
2025-03-22 07:06:43,879 - transformer_training - INFO - Main loop iteration: 941
2025-03-22 07:06:44,139 - transformer_training - INFO - Main loop iteration: 942
2025-03-22 07:06:44,397 - transformer_training - INFO - Main loop iteration: 943
2025-03-22 07:06:44,674 - transformer_training - INFO - Main loop iteration: 944
2025-03-22 07:06:44,923 - transformer_training - INFO - Main loop iteration: 945
2025-03-22 07:06:45,183 - transformer_training - INFO - Main loop iteration: 946
2025-03-22 07:06:45,442 - transformer_training - INFO - Main loop iteration: 947
2025-03-22 07:06:45,719 - transformer_training - INFO - Main loop iteration: 948
2025-03-22 07:06:45,969 - transformer_training - INFO - Main loop iteration: 949
2025-03-22 07:06:46,230 - transformer_training - INFO - Main loop iteration: 950
Iter 950: loss 6.3791, lr 0.000237, 127141.51 tokens/sec
2025-03-22 07:06:46,488 - transformer_training - INFO - Main loop iteration: 951
2025-03-22 07:06:46,765 - transformer_training - INFO - Main loop iteration: 952
2025-03-22 07:06:47,014 - transformer_training - INFO - Main loop iteration: 953
2025-03-22 07:06:47,275 - transformer_training - INFO - Main loop iteration: 954
2025-03-22 07:06:47,533 - transformer_training - INFO - Main loop iteration: 955
2025-03-22 07:06:47,810 - transformer_training - INFO - Main loop iteration: 956
2025-03-22 07:06:48,059 - transformer_training - INFO - Main loop iteration: 957
2025-03-22 07:06:48,320 - transformer_training - INFO - Main loop iteration: 958
2025-03-22 07:06:48,579 - transformer_training - INFO - Main loop iteration: 959
2025-03-22 07:06:48,857 - transformer_training - INFO - Main loop iteration: 960
Iter 960: loss 6.3276, lr 0.000240, 131989.12 tokens/sec
2025-03-22 07:06:49,106 - transformer_training - INFO - Main loop iteration: 961
2025-03-22 07:06:49,366 - transformer_training - INFO - Main loop iteration: 962
2025-03-22 07:06:49,624 - transformer_training - INFO - Main loop iteration: 963
2025-03-22 07:06:49,901 - transformer_training - INFO - Main loop iteration: 964
2025-03-22 07:06:50,153 - transformer_training - INFO - Main loop iteration: 965
2025-03-22 07:06:50,414 - transformer_training - INFO - Main loop iteration: 966
2025-03-22 07:06:50,672 - transformer_training - INFO - Main loop iteration: 967
2025-03-22 07:06:50,949 - transformer_training - INFO - Main loop iteration: 968
2025-03-22 07:06:51,198 - transformer_training - INFO - Main loop iteration: 969
2025-03-22 07:06:51,458 - transformer_training - INFO - Main loop iteration: 970
Iter 970: loss 6.3727, lr 0.000242, 127344.48 tokens/sec
2025-03-22 07:06:51,716 - transformer_training - INFO - Main loop iteration: 971
2025-03-22 07:06:51,993 - transformer_training - INFO - Main loop iteration: 972
2025-03-22 07:06:52,245 - transformer_training - INFO - Main loop iteration: 973
2025-03-22 07:06:52,505 - transformer_training - INFO - Main loop iteration: 974
2025-03-22 07:06:52,763 - transformer_training - INFO - Main loop iteration: 975
2025-03-22 07:06:53,040 - transformer_training - INFO - Main loop iteration: 976
2025-03-22 07:06:53,289 - transformer_training - INFO - Main loop iteration: 977
2025-03-22 07:06:53,550 - transformer_training - INFO - Main loop iteration: 978
2025-03-22 07:06:53,808 - transformer_training - INFO - Main loop iteration: 979
2025-03-22 07:06:54,085 - transformer_training - INFO - Main loop iteration: 980
Iter 980: loss 6.3812, lr 0.000245, 129615.86 tokens/sec
2025-03-22 07:06:54,338 - transformer_training - INFO - Main loop iteration: 981
2025-03-22 07:06:54,598 - transformer_training - INFO - Main loop iteration: 982
2025-03-22 07:06:54,857 - transformer_training - INFO - Main loop iteration: 983
2025-03-22 07:06:55,133 - transformer_training - INFO - Main loop iteration: 984
2025-03-22 07:06:55,382 - transformer_training - INFO - Main loop iteration: 985
2025-03-22 07:06:55,642 - transformer_training - INFO - Main loop iteration: 986
2025-03-22 07:06:55,900 - transformer_training - INFO - Main loop iteration: 987
2025-03-22 07:06:56,177 - transformer_training - INFO - Main loop iteration: 988
2025-03-22 07:06:56,430 - transformer_training - INFO - Main loop iteration: 989
2025-03-22 07:06:56,690 - transformer_training - INFO - Main loop iteration: 990
Iter 990: loss 6.3610, lr 0.000247, 127222.36 tokens/sec
2025-03-22 07:06:56,948 - transformer_training - INFO - Main loop iteration: 991
2025-03-22 07:06:57,225 - transformer_training - INFO - Main loop iteration: 992
2025-03-22 07:06:57,477 - transformer_training - INFO - Main loop iteration: 993
2025-03-22 07:06:57,737 - transformer_training - INFO - Main loop iteration: 994
2025-03-22 07:06:57,995 - transformer_training - INFO - Main loop iteration: 995
2025-03-22 07:06:58,272 - transformer_training - INFO - Main loop iteration: 996
2025-03-22 07:06:58,521 - transformer_training - INFO - Main loop iteration: 997
2025-03-22 07:06:58,781 - transformer_training - INFO - Main loop iteration: 998
2025-03-22 07:06:59,040 - transformer_training - INFO - Main loop iteration: 999
2025-03-22 07:06:59,317 - transformer_training - INFO - Main loop iteration: 1000
Iter 1000: loss 6.3067, lr 0.000250, 131593.19 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 1000: train loss 6.2719, val loss 6.2674
New best model saved with val loss: 6.2674
2025-03-22 07:07:22,307 - transformer_training - INFO - Main loop iteration: 1001
2025-03-22 07:07:22,584 - transformer_training - INFO - Main loop iteration: 1002
2025-03-22 07:07:22,846 - transformer_training - INFO - Main loop iteration: 1003
2025-03-22 07:07:23,123 - transformer_training - INFO - Main loop iteration: 1004
2025-03-22 07:07:23,373 - transformer_training - INFO - Main loop iteration: 1005
2025-03-22 07:07:23,634 - transformer_training - INFO - Main loop iteration: 1006
2025-03-22 07:07:23,894 - transformer_training - INFO - Main loop iteration: 1007
2025-03-22 07:07:24,172 - transformer_training - INFO - Main loop iteration: 1008
2025-03-22 07:07:24,422 - transformer_training - INFO - Main loop iteration: 1009
2025-03-22 07:07:24,682 - transformer_training - INFO - Main loop iteration: 1010
Iter 1010: loss 6.2787, lr 0.000252, 127006.98 tokens/sec
2025-03-22 07:07:24,943 - transformer_training - INFO - Main loop iteration: 1011
2025-03-22 07:07:25,222 - transformer_training - INFO - Main loop iteration: 1012
2025-03-22 07:07:25,475 - transformer_training - INFO - Main loop iteration: 1013
2025-03-22 07:07:25,737 - transformer_training - INFO - Main loop iteration: 1014
2025-03-22 07:07:25,997 - transformer_training - INFO - Main loop iteration: 1015
2025-03-22 07:07:26,277 - transformer_training - INFO - Main loop iteration: 1016
2025-03-22 07:07:26,526 - transformer_training - INFO - Main loop iteration: 1017
2025-03-22 07:07:26,788 - transformer_training - INFO - Main loop iteration: 1018
2025-03-22 07:07:27,053 - transformer_training - INFO - Main loop iteration: 1019
2025-03-22 07:07:27,339 - transformer_training - INFO - Main loop iteration: 1020
Iter 1020: loss 6.3151, lr 0.000255, 131903.24 tokens/sec
2025-03-22 07:07:27,588 - transformer_training - INFO - Main loop iteration: 1021
2025-03-22 07:07:27,849 - transformer_training - INFO - Main loop iteration: 1022
2025-03-22 07:07:28,110 - transformer_training - INFO - Main loop iteration: 1023
2025-03-22 07:07:28,387 - transformer_training - INFO - Main loop iteration: 1024
2025-03-22 07:07:28,636 - transformer_training - INFO - Main loop iteration: 1025
2025-03-22 07:07:28,897 - transformer_training - INFO - Main loop iteration: 1026
2025-03-22 07:07:29,157 - transformer_training - INFO - Main loop iteration: 1027
2025-03-22 07:07:29,436 - transformer_training - INFO - Main loop iteration: 1028
2025-03-22 07:07:29,685 - transformer_training - INFO - Main loop iteration: 1029
2025-03-22 07:07:30,060 - transformer_training - INFO - Main loop iteration: 1030
Iter 1030: loss 6.2931, lr 0.000257, 126357.76 tokens/sec
2025-03-22 07:07:30,320 - transformer_training - INFO - Main loop iteration: 1031
2025-03-22 07:07:30,597 - transformer_training - INFO - Main loop iteration: 1032
2025-03-22 07:07:30,848 - transformer_training - INFO - Main loop iteration: 1033
2025-03-22 07:07:31,109 - transformer_training - INFO - Main loop iteration: 1034
2025-03-22 07:07:31,369 - transformer_training - INFO - Main loop iteration: 1035
2025-03-22 07:07:31,647 - transformer_training - INFO - Main loop iteration: 1036
2025-03-22 07:07:31,897 - transformer_training - INFO - Main loop iteration: 1037
2025-03-22 07:07:32,158 - transformer_training - INFO - Main loop iteration: 1038
2025-03-22 07:07:32,417 - transformer_training - INFO - Main loop iteration: 1039
2025-03-22 07:07:32,695 - transformer_training - INFO - Main loop iteration: 1040
Iter 1040: loss 6.1902, lr 0.000260, 130727.28 tokens/sec
2025-03-22 07:07:32,947 - transformer_training - INFO - Main loop iteration: 1041
2025-03-22 07:07:33,207 - transformer_training - INFO - Main loop iteration: 1042
2025-03-22 07:07:33,471 - transformer_training - INFO - Main loop iteration: 1043
2025-03-22 07:07:33,749 - transformer_training - INFO - Main loop iteration: 1044
2025-03-22 07:07:33,999 - transformer_training - INFO - Main loop iteration: 1045
2025-03-22 07:07:34,260 - transformer_training - INFO - Main loop iteration: 1046
2025-03-22 07:07:34,520 - transformer_training - INFO - Main loop iteration: 1047
2025-03-22 07:07:34,798 - transformer_training - INFO - Main loop iteration: 1048
2025-03-22 07:07:35,048 - transformer_training - INFO - Main loop iteration: 1049
2025-03-22 07:07:35,309 - transformer_training - INFO - Main loop iteration: 1050
Iter 1050: loss 6.2992, lr 0.000262, 123507.55 tokens/sec
2025-03-22 07:07:35,575 - transformer_training - INFO - Main loop iteration: 1051
2025-03-22 07:07:35,859 - transformer_training - INFO - Main loop iteration: 1052
2025-03-22 07:07:36,110 - transformer_training - INFO - Main loop iteration: 1053
2025-03-22 07:07:36,371 - transformer_training - INFO - Main loop iteration: 1054
2025-03-22 07:07:36,637 - transformer_training - INFO - Main loop iteration: 1055
2025-03-22 07:07:36,914 - transformer_training - INFO - Main loop iteration: 1056
2025-03-22 07:07:37,164 - transformer_training - INFO - Main loop iteration: 1057
2025-03-22 07:07:37,426 - transformer_training - INFO - Main loop iteration: 1058
2025-03-22 07:07:37,685 - transformer_training - INFO - Main loop iteration: 1059
2025-03-22 07:07:37,963 - transformer_training - INFO - Main loop iteration: 1060
Iter 1060: loss 6.2597, lr 0.000265, 131315.20 tokens/sec
2025-03-22 07:07:38,214 - transformer_training - INFO - Main loop iteration: 1061
2025-03-22 07:07:38,481 - transformer_training - INFO - Main loop iteration: 1062
2025-03-22 07:07:38,741 - transformer_training - INFO - Main loop iteration: 1063
2025-03-22 07:07:39,025 - transformer_training - INFO - Main loop iteration: 1064
2025-03-22 07:07:39,275 - transformer_training - INFO - Main loop iteration: 1065
2025-03-22 07:07:39,536 - transformer_training - INFO - Main loop iteration: 1066
2025-03-22 07:07:39,796 - transformer_training - INFO - Main loop iteration: 1067
2025-03-22 07:07:40,073 - transformer_training - INFO - Main loop iteration: 1068
2025-03-22 07:07:40,324 - transformer_training - INFO - Main loop iteration: 1069
2025-03-22 07:07:40,586 - transformer_training - INFO - Main loop iteration: 1070
Iter 1070: loss 6.2595, lr 0.000267, 123734.49 tokens/sec
2025-03-22 07:07:40,851 - transformer_training - INFO - Main loop iteration: 1071
2025-03-22 07:07:41,129 - transformer_training - INFO - Main loop iteration: 1072
2025-03-22 07:07:41,379 - transformer_training - INFO - Main loop iteration: 1073
2025-03-22 07:07:41,641 - transformer_training - INFO - Main loop iteration: 1074
2025-03-22 07:07:41,900 - transformer_training - INFO - Main loop iteration: 1075
2025-03-22 07:07:42,178 - transformer_training - INFO - Main loop iteration: 1076
2025-03-22 07:07:42,429 - transformer_training - INFO - Main loop iteration: 1077
2025-03-22 07:07:42,690 - transformer_training - INFO - Main loop iteration: 1078
2025-03-22 07:07:42,950 - transformer_training - INFO - Main loop iteration: 1079
2025-03-22 07:07:43,227 - transformer_training - INFO - Main loop iteration: 1080
Iter 1080: loss 6.2272, lr 0.000270, 130740.22 tokens/sec
2025-03-22 07:07:43,479 - transformer_training - INFO - Main loop iteration: 1081
2025-03-22 07:07:43,740 - transformer_training - INFO - Main loop iteration: 1082
2025-03-22 07:07:43,999 - transformer_training - INFO - Main loop iteration: 1083
2025-03-22 07:07:44,283 - transformer_training - INFO - Main loop iteration: 1084
2025-03-22 07:07:44,534 - transformer_training - INFO - Main loop iteration: 1085
2025-03-22 07:07:44,795 - transformer_training - INFO - Main loop iteration: 1086
2025-03-22 07:07:45,055 - transformer_training - INFO - Main loop iteration: 1087
2025-03-22 07:07:45,332 - transformer_training - INFO - Main loop iteration: 1088
2025-03-22 07:07:45,583 - transformer_training - INFO - Main loop iteration: 1089
2025-03-22 07:07:45,844 - transformer_training - INFO - Main loop iteration: 1090
Iter 1090: loss 6.2452, lr 0.000272, 126570.59 tokens/sec
2025-03-22 07:07:46,104 - transformer_training - INFO - Main loop iteration: 1091
2025-03-22 07:07:46,382 - transformer_training - INFO - Main loop iteration: 1092
2025-03-22 07:07:46,632 - transformer_training - INFO - Main loop iteration: 1093
2025-03-22 07:07:46,894 - transformer_training - INFO - Main loop iteration: 1094
2025-03-22 07:07:47,153 - transformer_training - INFO - Main loop iteration: 1095
2025-03-22 07:07:47,431 - transformer_training - INFO - Main loop iteration: 1096
2025-03-22 07:07:47,682 - transformer_training - INFO - Main loop iteration: 1097
2025-03-22 07:07:47,943 - transformer_training - INFO - Main loop iteration: 1098
2025-03-22 07:07:48,204 - transformer_training - INFO - Main loop iteration: 1099
2025-03-22 07:07:48,480 - transformer_training - INFO - Main loop iteration: 1100
Iter 1100: loss 6.2537, lr 0.000275, 129915.79 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 1100: train loss 6.1613, val loss 6.1537
New best model saved with val loss: 6.1537
2025-03-22 07:08:09,337 - transformer_training - INFO - Main loop iteration: 1101
2025-03-22 07:08:09,614 - transformer_training - INFO - Main loop iteration: 1102
2025-03-22 07:08:09,875 - transformer_training - INFO - Main loop iteration: 1103
2025-03-22 07:08:10,152 - transformer_training - INFO - Main loop iteration: 1104
2025-03-22 07:08:10,402 - transformer_training - INFO - Main loop iteration: 1105
2025-03-22 07:08:10,663 - transformer_training - INFO - Main loop iteration: 1106
2025-03-22 07:08:10,922 - transformer_training - INFO - Main loop iteration: 1107
2025-03-22 07:08:11,200 - transformer_training - INFO - Main loop iteration: 1108
2025-03-22 07:08:11,450 - transformer_training - INFO - Main loop iteration: 1109
2025-03-22 07:08:11,711 - transformer_training - INFO - Main loop iteration: 1110
Iter 1110: loss 6.3104, lr 0.000277, 126738.43 tokens/sec
2025-03-22 07:08:11,971 - transformer_training - INFO - Main loop iteration: 1111
2025-03-22 07:08:12,248 - transformer_training - INFO - Main loop iteration: 1112
2025-03-22 07:08:12,498 - transformer_training - INFO - Main loop iteration: 1113
2025-03-22 07:08:12,759 - transformer_training - INFO - Main loop iteration: 1114
2025-03-22 07:08:13,018 - transformer_training - INFO - Main loop iteration: 1115
2025-03-22 07:08:13,296 - transformer_training - INFO - Main loop iteration: 1116
2025-03-22 07:08:13,546 - transformer_training - INFO - Main loop iteration: 1117
2025-03-22 07:08:13,806 - transformer_training - INFO - Main loop iteration: 1118
2025-03-22 07:08:14,066 - transformer_training - INFO - Main loop iteration: 1119
2025-03-22 07:08:14,343 - transformer_training - INFO - Main loop iteration: 1120
Iter 1120: loss 6.2063, lr 0.000280, 131349.59 tokens/sec
2025-03-22 07:08:14,593 - transformer_training - INFO - Main loop iteration: 1121
2025-03-22 07:08:14,854 - transformer_training - INFO - Main loop iteration: 1122
2025-03-22 07:08:15,113 - transformer_training - INFO - Main loop iteration: 1123
2025-03-22 07:08:15,391 - transformer_training - INFO - Main loop iteration: 1124
2025-03-22 07:08:15,641 - transformer_training - INFO - Main loop iteration: 1125
2025-03-22 07:08:15,902 - transformer_training - INFO - Main loop iteration: 1126
2025-03-22 07:08:16,161 - transformer_training - INFO - Main loop iteration: 1127
2025-03-22 07:08:16,440 - transformer_training - INFO - Main loop iteration: 1128
2025-03-22 07:08:16,690 - transformer_training - INFO - Main loop iteration: 1129
2025-03-22 07:08:16,951 - transformer_training - INFO - Main loop iteration: 1130
Iter 1130: loss 6.1998, lr 0.000282, 126600.90 tokens/sec
2025-03-22 07:08:17,211 - transformer_training - INFO - Main loop iteration: 1131
2025-03-22 07:08:17,489 - transformer_training - INFO - Main loop iteration: 1132
2025-03-22 07:08:17,739 - transformer_training - INFO - Main loop iteration: 1133
2025-03-22 07:08:18,000 - transformer_training - INFO - Main loop iteration: 1134
2025-03-22 07:08:18,267 - transformer_training - INFO - Main loop iteration: 1135
2025-03-22 07:08:18,551 - transformer_training - INFO - Main loop iteration: 1136
2025-03-22 07:08:18,801 - transformer_training - INFO - Main loop iteration: 1137
2025-03-22 07:08:19,062 - transformer_training - INFO - Main loop iteration: 1138
2025-03-22 07:08:19,322 - transformer_training - INFO - Main loop iteration: 1139
2025-03-22 07:08:19,600 - transformer_training - INFO - Main loop iteration: 1140
Iter 1140: loss 6.2389, lr 0.000285, 131190.36 tokens/sec
2025-03-22 07:08:19,851 - transformer_training - INFO - Main loop iteration: 1141
2025-03-22 07:08:20,112 - transformer_training - INFO - Main loop iteration: 1142
2025-03-22 07:08:20,372 - transformer_training - INFO - Main loop iteration: 1143
2025-03-22 07:08:20,650 - transformer_training - INFO - Main loop iteration: 1144
2025-03-22 07:08:20,900 - transformer_training - INFO - Main loop iteration: 1145
2025-03-22 07:08:21,161 - transformer_training - INFO - Main loop iteration: 1146
2025-03-22 07:08:21,421 - transformer_training - INFO - Main loop iteration: 1147
2025-03-22 07:08:21,700 - transformer_training - INFO - Main loop iteration: 1148
2025-03-22 07:08:21,950 - transformer_training - INFO - Main loop iteration: 1149
2025-03-22 07:08:22,211 - transformer_training - INFO - Main loop iteration: 1150
Iter 1150: loss 6.1970, lr 0.000287, 126502.79 tokens/sec
2025-03-22 07:08:22,471 - transformer_training - INFO - Main loop iteration: 1151
2025-03-22 07:08:22,749 - transformer_training - INFO - Main loop iteration: 1152
2025-03-22 07:08:22,999 - transformer_training - INFO - Main loop iteration: 1153
2025-03-22 07:08:23,261 - transformer_training - INFO - Main loop iteration: 1154
2025-03-22 07:08:23,520 - transformer_training - INFO - Main loop iteration: 1155
2025-03-22 07:08:23,799 - transformer_training - INFO - Main loop iteration: 1156
2025-03-22 07:08:24,049 - transformer_training - INFO - Main loop iteration: 1157
2025-03-22 07:08:24,310 - transformer_training - INFO - Main loop iteration: 1158
2025-03-22 07:08:24,569 - transformer_training - INFO - Main loop iteration: 1159
2025-03-22 07:08:24,848 - transformer_training - INFO - Main loop iteration: 1160
Iter 1160: loss 6.1881, lr 0.000290, 131063.50 tokens/sec
2025-03-22 07:08:25,098 - transformer_training - INFO - Main loop iteration: 1161
2025-03-22 07:08:25,359 - transformer_training - INFO - Main loop iteration: 1162
2025-03-22 07:08:25,619 - transformer_training - INFO - Main loop iteration: 1163
2025-03-22 07:08:25,897 - transformer_training - INFO - Main loop iteration: 1164
2025-03-22 07:08:26,147 - transformer_training - INFO - Main loop iteration: 1165
2025-03-22 07:08:26,408 - transformer_training - INFO - Main loop iteration: 1166
2025-03-22 07:08:26,668 - transformer_training - INFO - Main loop iteration: 1167
2025-03-22 07:08:26,946 - transformer_training - INFO - Main loop iteration: 1168
2025-03-22 07:08:27,196 - transformer_training - INFO - Main loop iteration: 1169
2025-03-22 07:08:27,458 - transformer_training - INFO - Main loop iteration: 1170
Iter 1170: loss 6.1453, lr 0.000292, 126390.18 tokens/sec
2025-03-22 07:08:27,718 - transformer_training - INFO - Main loop iteration: 1171
2025-03-22 07:08:27,996 - transformer_training - INFO - Main loop iteration: 1172
2025-03-22 07:08:28,246 - transformer_training - INFO - Main loop iteration: 1173
2025-03-22 07:08:28,507 - transformer_training - INFO - Main loop iteration: 1174
2025-03-22 07:08:28,768 - transformer_training - INFO - Main loop iteration: 1175
2025-03-22 07:08:29,046 - transformer_training - INFO - Main loop iteration: 1176
2025-03-22 07:08:29,296 - transformer_training - INFO - Main loop iteration: 1177
2025-03-22 07:08:29,558 - transformer_training - INFO - Main loop iteration: 1178
2025-03-22 07:08:29,818 - transformer_training - INFO - Main loop iteration: 1179
2025-03-22 07:08:30,100 - transformer_training - INFO - Main loop iteration: 1180
Iter 1180: loss 6.1415, lr 0.000295, 129885.71 tokens/sec
2025-03-22 07:08:30,353 - transformer_training - INFO - Main loop iteration: 1181
2025-03-22 07:08:30,614 - transformer_training - INFO - Main loop iteration: 1182
2025-03-22 07:08:30,875 - transformer_training - INFO - Main loop iteration: 1183
2025-03-22 07:08:31,152 - transformer_training - INFO - Main loop iteration: 1184
2025-03-22 07:08:31,403 - transformer_training - INFO - Main loop iteration: 1185
2025-03-22 07:08:31,665 - transformer_training - INFO - Main loop iteration: 1186
2025-03-22 07:08:31,925 - transformer_training - INFO - Main loop iteration: 1187
2025-03-22 07:08:32,205 - transformer_training - INFO - Main loop iteration: 1188
2025-03-22 07:08:32,455 - transformer_training - INFO - Main loop iteration: 1189
2025-03-22 07:08:32,716 - transformer_training - INFO - Main loop iteration: 1190
Iter 1190: loss 6.0423, lr 0.000297, 126469.73 tokens/sec
2025-03-22 07:08:32,977 - transformer_training - INFO - Main loop iteration: 1191
2025-03-22 07:08:33,254 - transformer_training - INFO - Main loop iteration: 1192
2025-03-22 07:08:33,504 - transformer_training - INFO - Main loop iteration: 1193
2025-03-22 07:08:33,767 - transformer_training - INFO - Main loop iteration: 1194
2025-03-22 07:08:34,027 - transformer_training - INFO - Main loop iteration: 1195
2025-03-22 07:08:34,309 - transformer_training - INFO - Main loop iteration: 1196
2025-03-22 07:08:34,560 - transformer_training - INFO - Main loop iteration: 1197
2025-03-22 07:08:34,822 - transformer_training - INFO - Main loop iteration: 1198
2025-03-22 07:08:35,082 - transformer_training - INFO - Main loop iteration: 1199
2025-03-22 07:08:35,360 - transformer_training - INFO - Main loop iteration: 1200
Iter 1200: loss 6.1654, lr 0.000300, 131121.02 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 1200: train loss 6.0521, val loss 6.0365
New best model saved with val loss: 6.0365
2025-03-22 07:08:56,346 - transformer_training - INFO - Main loop iteration: 1201
2025-03-22 07:08:56,627 - transformer_training - INFO - Main loop iteration: 1202
2025-03-22 07:08:56,889 - transformer_training - INFO - Main loop iteration: 1203
2025-03-22 07:08:57,167 - transformer_training - INFO - Main loop iteration: 1204
2025-03-22 07:08:57,417 - transformer_training - INFO - Main loop iteration: 1205
2025-03-22 07:08:57,679 - transformer_training - INFO - Main loop iteration: 1206
2025-03-22 07:08:57,939 - transformer_training - INFO - Main loop iteration: 1207
2025-03-22 07:08:58,216 - transformer_training - INFO - Main loop iteration: 1208
2025-03-22 07:08:58,470 - transformer_training - INFO - Main loop iteration: 1209
2025-03-22 07:08:58,732 - transformer_training - INFO - Main loop iteration: 1210
Iter 1210: loss 6.1867, lr 0.000302, 125997.85 tokens/sec
2025-03-22 07:08:58,993 - transformer_training - INFO - Main loop iteration: 1211
2025-03-22 07:08:59,270 - transformer_training - INFO - Main loop iteration: 1212
2025-03-22 07:08:59,521 - transformer_training - INFO - Main loop iteration: 1213
2025-03-22 07:08:59,782 - transformer_training - INFO - Main loop iteration: 1214
2025-03-22 07:09:00,048 - transformer_training - INFO - Main loop iteration: 1215
2025-03-22 07:09:00,326 - transformer_training - INFO - Main loop iteration: 1216
2025-03-22 07:09:00,576 - transformer_training - INFO - Main loop iteration: 1217
2025-03-22 07:09:00,838 - transformer_training - INFO - Main loop iteration: 1218
2025-03-22 07:09:01,097 - transformer_training - INFO - Main loop iteration: 1219
2025-03-22 07:09:01,376 - transformer_training - INFO - Main loop iteration: 1220
Iter 1220: loss 6.0981, lr 0.000305, 131308.05 tokens/sec
2025-03-22 07:09:01,626 - transformer_training - INFO - Main loop iteration: 1221
2025-03-22 07:09:01,887 - transformer_training - INFO - Main loop iteration: 1222
2025-03-22 07:09:02,156 - transformer_training - INFO - Main loop iteration: 1223
2025-03-22 07:09:02,434 - transformer_training - INFO - Main loop iteration: 1224
2025-03-22 07:09:02,688 - transformer_training - INFO - Main loop iteration: 1225
2025-03-22 07:09:02,950 - transformer_training - INFO - Main loop iteration: 1226
2025-03-22 07:09:03,209 - transformer_training - INFO - Main loop iteration: 1227
2025-03-22 07:09:03,487 - transformer_training - INFO - Main loop iteration: 1228
2025-03-22 07:09:03,739 - transformer_training - INFO - Main loop iteration: 1229
2025-03-22 07:09:04,000 - transformer_training - INFO - Main loop iteration: 1230
Iter 1230: loss 6.1046, lr 0.000307, 126733.05 tokens/sec
2025-03-22 07:09:04,260 - transformer_training - INFO - Main loop iteration: 1231
2025-03-22 07:09:04,543 - transformer_training - INFO - Main loop iteration: 1232
2025-03-22 07:09:04,793 - transformer_training - INFO - Main loop iteration: 1233
2025-03-22 07:09:05,054 - transformer_training - INFO - Main loop iteration: 1234
2025-03-22 07:09:05,313 - transformer_training - INFO - Main loop iteration: 1235
2025-03-22 07:09:05,591 - transformer_training - INFO - Main loop iteration: 1236
2025-03-22 07:09:05,841 - transformer_training - INFO - Main loop iteration: 1237
2025-03-22 07:09:06,103 - transformer_training - INFO - Main loop iteration: 1238
2025-03-22 07:09:06,362 - transformer_training - INFO - Main loop iteration: 1239
2025-03-22 07:09:06,640 - transformer_training - INFO - Main loop iteration: 1240
Iter 1240: loss 6.1615, lr 0.000310, 131284.22 tokens/sec
2025-03-22 07:09:06,891 - transformer_training - INFO - Main loop iteration: 1241
2025-03-22 07:09:07,152 - transformer_training - INFO - Main loop iteration: 1242
2025-03-22 07:09:07,412 - transformer_training - INFO - Main loop iteration: 1243
2025-03-22 07:09:07,689 - transformer_training - INFO - Main loop iteration: 1244
2025-03-22 07:09:07,940 - transformer_training - INFO - Main loop iteration: 1245
2025-03-22 07:09:08,201 - transformer_training - INFO - Main loop iteration: 1246
2025-03-22 07:09:08,460 - transformer_training - INFO - Main loop iteration: 1247
2025-03-22 07:09:08,738 - transformer_training - INFO - Main loop iteration: 1248
2025-03-22 07:09:08,991 - transformer_training - INFO - Main loop iteration: 1249
2025-03-22 07:09:09,253 - transformer_training - INFO - Main loop iteration: 1250
Iter 1250: loss 6.0861, lr 0.000312, 126361.71 tokens/sec
2025-03-22 07:09:09,513 - transformer_training - INFO - Main loop iteration: 1251
2025-03-22 07:09:09,802 - transformer_training - INFO - Main loop iteration: 1252
2025-03-22 07:09:10,052 - transformer_training - INFO - Main loop iteration: 1253
2025-03-22 07:09:10,313 - transformer_training - INFO - Main loop iteration: 1254
2025-03-22 07:09:10,574 - transformer_training - INFO - Main loop iteration: 1255
2025-03-22 07:09:10,852 - transformer_training - INFO - Main loop iteration: 1256
2025-03-22 07:09:11,103 - transformer_training - INFO - Main loop iteration: 1257
2025-03-22 07:09:11,364 - transformer_training - INFO - Main loop iteration: 1258
2025-03-22 07:09:11,624 - transformer_training - INFO - Main loop iteration: 1259
2025-03-22 07:09:11,902 - transformer_training - INFO - Main loop iteration: 1260
Iter 1260: loss 6.1285, lr 0.000315, 131040.26 tokens/sec
2025-03-22 07:09:12,153 - transformer_training - INFO - Main loop iteration: 1261
2025-03-22 07:09:12,414 - transformer_training - INFO - Main loop iteration: 1262
2025-03-22 07:09:12,674 - transformer_training - INFO - Main loop iteration: 1263
2025-03-22 07:09:12,953 - transformer_training - INFO - Main loop iteration: 1264
2025-03-22 07:09:13,203 - transformer_training - INFO - Main loop iteration: 1265
2025-03-22 07:09:13,465 - transformer_training - INFO - Main loop iteration: 1266
2025-03-22 07:09:13,725 - transformer_training - INFO - Main loop iteration: 1267
2025-03-22 07:09:14,003 - transformer_training - INFO - Main loop iteration: 1268
2025-03-22 07:09:14,254 - transformer_training - INFO - Main loop iteration: 1269
2025-03-22 07:09:14,515 - transformer_training - INFO - Main loop iteration: 1270
Iter 1270: loss 6.0715, lr 0.000317, 126403.31 tokens/sec
2025-03-22 07:09:14,776 - transformer_training - INFO - Main loop iteration: 1271
2025-03-22 07:09:15,053 - transformer_training - INFO - Main loop iteration: 1272
2025-03-22 07:09:15,304 - transformer_training - INFO - Main loop iteration: 1273
2025-03-22 07:09:15,565 - transformer_training - INFO - Main loop iteration: 1274
2025-03-22 07:09:15,825 - transformer_training - INFO - Main loop iteration: 1275
2025-03-22 07:09:16,104 - transformer_training - INFO - Main loop iteration: 1276
2025-03-22 07:09:16,356 - transformer_training - INFO - Main loop iteration: 1277
2025-03-22 07:09:16,617 - transformer_training - INFO - Main loop iteration: 1278
2025-03-22 07:09:16,881 - transformer_training - INFO - Main loop iteration: 1279
2025-03-22 07:09:17,158 - transformer_training - INFO - Main loop iteration: 1280
Iter 1280: loss 6.0872, lr 0.000320, 129205.95 tokens/sec
2025-03-22 07:09:17,413 - transformer_training - INFO - Main loop iteration: 1281
2025-03-22 07:09:17,674 - transformer_training - INFO - Main loop iteration: 1282
2025-03-22 07:09:17,933 - transformer_training - INFO - Main loop iteration: 1283
2025-03-22 07:09:18,210 - transformer_training - INFO - Main loop iteration: 1284
2025-03-22 07:09:18,461 - transformer_training - INFO - Main loop iteration: 1285
2025-03-22 07:09:18,722 - transformer_training - INFO - Main loop iteration: 1286
2025-03-22 07:09:18,981 - transformer_training - INFO - Main loop iteration: 1287
2025-03-22 07:09:19,259 - transformer_training - INFO - Main loop iteration: 1288
2025-03-22 07:09:19,509 - transformer_training - INFO - Main loop iteration: 1289
2025-03-22 07:09:19,770 - transformer_training - INFO - Main loop iteration: 1290
Iter 1290: loss 6.0596, lr 0.000322, 126680.95 tokens/sec
2025-03-22 07:09:20,029 - transformer_training - INFO - Main loop iteration: 1291
2025-03-22 07:09:20,306 - transformer_training - INFO - Main loop iteration: 1292
2025-03-22 07:09:20,557 - transformer_training - INFO - Main loop iteration: 1293
2025-03-22 07:09:20,817 - transformer_training - INFO - Main loop iteration: 1294
2025-03-22 07:09:21,077 - transformer_training - INFO - Main loop iteration: 1295
2025-03-22 07:09:21,354 - transformer_training - INFO - Main loop iteration: 1296
2025-03-22 07:09:21,604 - transformer_training - INFO - Main loop iteration: 1297
2025-03-22 07:09:21,865 - transformer_training - INFO - Main loop iteration: 1298
2025-03-22 07:09:22,124 - transformer_training - INFO - Main loop iteration: 1299
2025-03-22 07:09:22,402 - transformer_training - INFO - Main loop iteration: 1300
Iter 1300: loss 6.1048, lr 0.000325, 131191.48 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 1300: train loss 5.9667, val loss 5.9507
New best model saved with val loss: 5.9507
2025-03-22 07:09:42,987 - transformer_training - INFO - Main loop iteration: 1301
2025-03-22 07:09:43,268 - transformer_training - INFO - Main loop iteration: 1302
2025-03-22 07:09:43,529 - transformer_training - INFO - Main loop iteration: 1303
2025-03-22 07:09:43,806 - transformer_training - INFO - Main loop iteration: 1304
2025-03-22 07:09:44,057 - transformer_training - INFO - Main loop iteration: 1305
2025-03-22 07:09:44,318 - transformer_training - INFO - Main loop iteration: 1306
2025-03-22 07:09:44,578 - transformer_training - INFO - Main loop iteration: 1307
2025-03-22 07:09:44,857 - transformer_training - INFO - Main loop iteration: 1308
2025-03-22 07:09:45,107 - transformer_training - INFO - Main loop iteration: 1309
2025-03-22 07:09:45,368 - transformer_training - INFO - Main loop iteration: 1310
Iter 1310: loss 6.0276, lr 0.000327, 127116.81 tokens/sec
2025-03-22 07:09:45,628 - transformer_training - INFO - Main loop iteration: 1311
2025-03-22 07:09:45,907 - transformer_training - INFO - Main loop iteration: 1312
2025-03-22 07:09:46,157 - transformer_training - INFO - Main loop iteration: 1313
2025-03-22 07:09:46,418 - transformer_training - INFO - Main loop iteration: 1314
2025-03-22 07:09:46,678 - transformer_training - INFO - Main loop iteration: 1315
2025-03-22 07:09:46,957 - transformer_training - INFO - Main loop iteration: 1316
2025-03-22 07:09:47,209 - transformer_training - INFO - Main loop iteration: 1317
2025-03-22 07:09:47,470 - transformer_training - INFO - Main loop iteration: 1318
2025-03-22 07:09:47,730 - transformer_training - INFO - Main loop iteration: 1319
2025-03-22 07:09:48,009 - transformer_training - INFO - Main loop iteration: 1320
Iter 1320: loss 5.9725, lr 0.000330, 131418.04 tokens/sec
2025-03-22 07:09:48,259 - transformer_training - INFO - Main loop iteration: 1321
2025-03-22 07:09:48,520 - transformer_training - INFO - Main loop iteration: 1322
2025-03-22 07:09:48,781 - transformer_training - INFO - Main loop iteration: 1323
2025-03-22 07:09:49,060 - transformer_training - INFO - Main loop iteration: 1324
2025-03-22 07:09:49,329 - transformer_training - INFO - Main loop iteration: 1325
2025-03-22 07:09:49,603 - transformer_training - INFO - Main loop iteration: 1326
2025-03-22 07:09:49,873 - transformer_training - INFO - Main loop iteration: 1327
2025-03-22 07:09:50,153 - transformer_training - INFO - Main loop iteration: 1328
2025-03-22 07:09:50,405 - transformer_training - INFO - Main loop iteration: 1329
2025-03-22 07:09:50,665 - transformer_training - INFO - Main loop iteration: 1330
Iter 1330: loss 5.9536, lr 0.000332, 127176.45 tokens/sec
2025-03-22 07:09:50,925 - transformer_training - INFO - Main loop iteration: 1331
2025-03-22 07:09:51,204 - transformer_training - INFO - Main loop iteration: 1332
2025-03-22 07:09:51,454 - transformer_training - INFO - Main loop iteration: 1333
2025-03-22 07:09:51,715 - transformer_training - INFO - Main loop iteration: 1334
2025-03-22 07:09:51,975 - transformer_training - INFO - Main loop iteration: 1335
2025-03-22 07:09:52,256 - transformer_training - INFO - Main loop iteration: 1336
2025-03-22 07:09:52,508 - transformer_training - INFO - Main loop iteration: 1337
2025-03-22 07:09:52,775 - transformer_training - INFO - Main loop iteration: 1338
2025-03-22 07:09:53,036 - transformer_training - INFO - Main loop iteration: 1339
2025-03-22 07:09:53,316 - transformer_training - INFO - Main loop iteration: 1340
Iter 1340: loss 5.9492, lr 0.000335, 131359.13 tokens/sec
2025-03-22 07:09:53,567 - transformer_training - INFO - Main loop iteration: 1341
2025-03-22 07:09:53,828 - transformer_training - INFO - Main loop iteration: 1342
2025-03-22 07:09:54,088 - transformer_training - INFO - Main loop iteration: 1343
2025-03-22 07:09:54,368 - transformer_training - INFO - Main loop iteration: 1344
2025-03-22 07:09:54,618 - transformer_training - INFO - Main loop iteration: 1345
2025-03-22 07:09:54,879 - transformer_training - INFO - Main loop iteration: 1346
2025-03-22 07:09:55,139 - transformer_training - INFO - Main loop iteration: 1347
2025-03-22 07:09:55,418 - transformer_training - INFO - Main loop iteration: 1348
2025-03-22 07:09:55,668 - transformer_training - INFO - Main loop iteration: 1349
2025-03-22 07:09:55,930 - transformer_training - INFO - Main loop iteration: 1350
Iter 1350: loss 5.9185, lr 0.000337, 127107.76 tokens/sec
2025-03-22 07:09:56,190 - transformer_training - INFO - Main loop iteration: 1351
2025-03-22 07:09:56,469 - transformer_training - INFO - Main loop iteration: 1352
2025-03-22 07:09:56,722 - transformer_training - INFO - Main loop iteration: 1353
2025-03-22 07:09:56,983 - transformer_training - INFO - Main loop iteration: 1354
2025-03-22 07:09:57,243 - transformer_training - INFO - Main loop iteration: 1355
2025-03-22 07:09:57,522 - transformer_training - INFO - Main loop iteration: 1356
2025-03-22 07:09:57,771 - transformer_training - INFO - Main loop iteration: 1357
2025-03-22 07:09:58,033 - transformer_training - INFO - Main loop iteration: 1358
2025-03-22 07:09:58,293 - transformer_training - INFO - Main loop iteration: 1359
2025-03-22 07:09:58,576 - transformer_training - INFO - Main loop iteration: 1360
Iter 1360: loss 5.9924, lr 0.000340, 131642.85 tokens/sec
2025-03-22 07:09:58,825 - transformer_training - INFO - Main loop iteration: 1361
2025-03-22 07:09:59,086 - transformer_training - INFO - Main loop iteration: 1362
2025-03-22 07:09:59,347 - transformer_training - INFO - Main loop iteration: 1363
2025-03-22 07:09:59,626 - transformer_training - INFO - Main loop iteration: 1364
2025-03-22 07:09:59,876 - transformer_training - INFO - Main loop iteration: 1365
2025-03-22 07:10:00,137 - transformer_training - INFO - Main loop iteration: 1366
2025-03-22 07:10:00,397 - transformer_training - INFO - Main loop iteration: 1367
2025-03-22 07:10:00,676 - transformer_training - INFO - Main loop iteration: 1368
2025-03-22 07:10:00,925 - transformer_training - INFO - Main loop iteration: 1369
2025-03-22 07:10:01,186 - transformer_training - INFO - Main loop iteration: 1370
Iter 1370: loss 6.0107, lr 0.000342, 127071.80 tokens/sec
2025-03-22 07:10:01,447 - transformer_training - INFO - Main loop iteration: 1371
2025-03-22 07:10:01,730 - transformer_training - INFO - Main loop iteration: 1372
2025-03-22 07:10:01,980 - transformer_training - INFO - Main loop iteration: 1373
2025-03-22 07:10:02,241 - transformer_training - INFO - Main loop iteration: 1374
2025-03-22 07:10:02,501 - transformer_training - INFO - Main loop iteration: 1375
2025-03-22 07:10:02,780 - transformer_training - INFO - Main loop iteration: 1376
2025-03-22 07:10:03,029 - transformer_training - INFO - Main loop iteration: 1377
2025-03-22 07:10:03,291 - transformer_training - INFO - Main loop iteration: 1378
2025-03-22 07:10:03,550 - transformer_training - INFO - Main loop iteration: 1379
2025-03-22 07:10:03,829 - transformer_training - INFO - Main loop iteration: 1380
Iter 1380: loss 5.8788, lr 0.000345, 131577.32 tokens/sec
2025-03-22 07:10:04,078 - transformer_training - INFO - Main loop iteration: 1381
2025-03-22 07:10:04,339 - transformer_training - INFO - Main loop iteration: 1382
2025-03-22 07:10:04,599 - transformer_training - INFO - Main loop iteration: 1383
2025-03-22 07:10:04,878 - transformer_training - INFO - Main loop iteration: 1384
2025-03-22 07:10:05,128 - transformer_training - INFO - Main loop iteration: 1385
2025-03-22 07:10:05,389 - transformer_training - INFO - Main loop iteration: 1386
2025-03-22 07:10:05,649 - transformer_training - INFO - Main loop iteration: 1387
2025-03-22 07:10:05,928 - transformer_training - INFO - Main loop iteration: 1388
2025-03-22 07:10:06,308 - transformer_training - INFO - Main loop iteration: 1389
2025-03-22 07:10:06,569 - transformer_training - INFO - Main loop iteration: 1390
Iter 1390: loss 5.9394, lr 0.000347, 126971.66 tokens/sec
2025-03-22 07:10:06,829 - transformer_training - INFO - Main loop iteration: 1391
2025-03-22 07:10:07,107 - transformer_training - INFO - Main loop iteration: 1392
2025-03-22 07:10:07,358 - transformer_training - INFO - Main loop iteration: 1393
2025-03-22 07:10:07,620 - transformer_training - INFO - Main loop iteration: 1394
2025-03-22 07:10:07,884 - transformer_training - INFO - Main loop iteration: 1395
2025-03-22 07:10:08,166 - transformer_training - INFO - Main loop iteration: 1396
2025-03-22 07:10:08,419 - transformer_training - INFO - Main loop iteration: 1397
2025-03-22 07:10:08,680 - transformer_training - INFO - Main loop iteration: 1398
2025-03-22 07:10:08,939 - transformer_training - INFO - Main loop iteration: 1399
2025-03-22 07:10:09,217 - transformer_training - INFO - Main loop iteration: 1400
Iter 1400: loss 5.8951, lr 0.000350, 130978.44 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPTYour base folder is: /workspace/GPT

Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 1400: train loss 5.8878, val loss 5.8630
New best model saved with val loss: 5.8630
2025-03-22 07:10:29,784 - transformer_training - INFO - Main loop iteration: 1401
2025-03-22 07:10:30,060 - transformer_training - INFO - Main loop iteration: 1402
2025-03-22 07:10:30,319 - transformer_training - INFO - Main loop iteration: 1403
2025-03-22 07:10:30,597 - transformer_training - INFO - Main loop iteration: 1404
2025-03-22 07:10:30,848 - transformer_training - INFO - Main loop iteration: 1405
2025-03-22 07:10:31,109 - transformer_training - INFO - Main loop iteration: 1406
2025-03-22 07:10:31,370 - transformer_training - INFO - Main loop iteration: 1407
2025-03-22 07:10:31,652 - transformer_training - INFO - Main loop iteration: 1408
2025-03-22 07:10:31,905 - transformer_training - INFO - Main loop iteration: 1409
2025-03-22 07:10:32,166 - transformer_training - INFO - Main loop iteration: 1410
Iter 1410: loss 5.9621, lr 0.000352, 126659.82 tokens/sec
2025-03-22 07:10:32,426 - transformer_training - INFO - Main loop iteration: 1411
2025-03-22 07:10:32,703 - transformer_training - INFO - Main loop iteration: 1412
2025-03-22 07:10:32,954 - transformer_training - INFO - Main loop iteration: 1413
2025-03-22 07:10:33,216 - transformer_training - INFO - Main loop iteration: 1414
2025-03-22 07:10:33,475 - transformer_training - INFO - Main loop iteration: 1415
2025-03-22 07:10:33,753 - transformer_training - INFO - Main loop iteration: 1416
2025-03-22 07:10:34,004 - transformer_training - INFO - Main loop iteration: 1417
2025-03-22 07:10:34,264 - transformer_training - INFO - Main loop iteration: 1418
2025-03-22 07:10:34,524 - transformer_training - INFO - Main loop iteration: 1419
2025-03-22 07:10:34,802 - transformer_training - INFO - Main loop iteration: 1420
Iter 1420: loss 5.9898, lr 0.000355, 129632.61 tokens/sec
2025-03-22 07:10:35,056 - transformer_training - INFO - Main loop iteration: 1421
2025-03-22 07:10:35,316 - transformer_training - INFO - Main loop iteration: 1422
2025-03-22 07:10:35,576 - transformer_training - INFO - Main loop iteration: 1423
2025-03-22 07:10:35,853 - transformer_training - INFO - Main loop iteration: 1424
2025-03-22 07:10:36,104 - transformer_training - INFO - Main loop iteration: 1425
2025-03-22 07:10:36,365 - transformer_training - INFO - Main loop iteration: 1426
2025-03-22 07:10:36,624 - transformer_training - INFO - Main loop iteration: 1427
2025-03-22 07:10:36,903 - transformer_training - INFO - Main loop iteration: 1428
2025-03-22 07:10:37,155 - transformer_training - INFO - Main loop iteration: 1429
2025-03-22 07:10:37,415 - transformer_training - INFO - Main loop iteration: 1430
Iter 1430: loss 5.9992, lr 0.000357, 126610.58 tokens/sec
2025-03-22 07:10:37,675 - transformer_training - INFO - Main loop iteration: 1431
2025-03-22 07:10:37,953 - transformer_training - INFO - Main loop iteration: 1432
2025-03-22 07:10:38,203 - transformer_training - INFO - Main loop iteration: 1433
2025-03-22 07:10:38,464 - transformer_training - INFO - Main loop iteration: 1434
2025-03-22 07:10:38,724 - transformer_training - INFO - Main loop iteration: 1435
2025-03-22 07:10:39,002 - transformer_training - INFO - Main loop iteration: 1436
2025-03-22 07:10:39,255 - transformer_training - INFO - Main loop iteration: 1437
2025-03-22 07:10:39,517 - transformer_training - INFO - Main loop iteration: 1438
2025-03-22 07:10:39,777 - transformer_training - INFO - Main loop iteration: 1439
2025-03-22 07:10:40,055 - transformer_training - INFO - Main loop iteration: 1440
Iter 1440: loss 5.9915, lr 0.000360, 131119.02 tokens/sec
2025-03-22 07:10:40,306 - transformer_training - INFO - Main loop iteration: 1441
2025-03-22 07:10:40,566 - transformer_training - INFO - Main loop iteration: 1442
2025-03-22 07:10:40,825 - transformer_training - INFO - Main loop iteration: 1443
2025-03-22 07:10:41,104 - transformer_training - INFO - Main loop iteration: 1444
2025-03-22 07:10:41,354 - transformer_training - INFO - Main loop iteration: 1445
2025-03-22 07:10:41,615 - transformer_training - INFO - Main loop iteration: 1446
2025-03-22 07:10:41,875 - transformer_training - INFO - Main loop iteration: 1447
2025-03-22 07:10:42,153 - transformer_training - INFO - Main loop iteration: 1448
2025-03-22 07:10:42,404 - transformer_training - INFO - Main loop iteration: 1449
2025-03-22 07:10:42,665 - transformer_training - INFO - Main loop iteration: 1450
Iter 1450: loss 5.9394, lr 0.000362, 126536.56 tokens/sec
2025-03-22 07:10:42,925 - transformer_training - INFO - Main loop iteration: 1451
2025-03-22 07:10:43,202 - transformer_training - INFO - Main loop iteration: 1452
2025-03-22 07:10:43,456 - transformer_training - INFO - Main loop iteration: 1453
2025-03-22 07:10:43,717 - transformer_training - INFO - Main loop iteration: 1454
2025-03-22 07:10:43,977 - transformer_training - INFO - Main loop iteration: 1455
2025-03-22 07:10:44,255 - transformer_training - INFO - Main loop iteration: 1456
2025-03-22 07:10:44,506 - transformer_training - INFO - Main loop iteration: 1457
2025-03-22 07:10:44,767 - transformer_training - INFO - Main loop iteration: 1458
2025-03-22 07:10:45,026 - transformer_training - INFO - Main loop iteration: 1459
2025-03-22 07:10:45,304 - transformer_training - INFO - Main loop iteration: 1460
Iter 1460: loss 6.0605, lr 0.000365, 130863.71 tokens/sec
2025-03-22 07:10:45,555 - transformer_training - INFO - Main loop iteration: 1461
2025-03-22 07:10:45,817 - transformer_training - INFO - Main loop iteration: 1462
2025-03-22 07:10:46,077 - transformer_training - INFO - Main loop iteration: 1463
2025-03-22 07:10:46,359 - transformer_training - INFO - Main loop iteration: 1464
2025-03-22 07:10:46,610 - transformer_training - INFO - Main loop iteration: 1465
2025-03-22 07:10:46,871 - transformer_training - INFO - Main loop iteration: 1466
2025-03-22 07:10:47,131 - transformer_training - INFO - Main loop iteration: 1467
2025-03-22 07:10:47,409 - transformer_training - INFO - Main loop iteration: 1468
2025-03-22 07:10:47,659 - transformer_training - INFO - Main loop iteration: 1469
2025-03-22 07:10:47,920 - transformer_training - INFO - Main loop iteration: 1470
Iter 1470: loss 5.9273, lr 0.000367, 126614.78 tokens/sec
2025-03-22 07:10:48,180 - transformer_training - INFO - Main loop iteration: 1471
2025-03-22 07:10:48,458 - transformer_training - INFO - Main loop iteration: 1472
2025-03-22 07:10:48,708 - transformer_training - INFO - Main loop iteration: 1473
2025-03-22 07:10:48,970 - transformer_training - INFO - Main loop iteration: 1474
2025-03-22 07:10:49,230 - transformer_training - INFO - Main loop iteration: 1475
2025-03-22 07:10:49,508 - transformer_training - INFO - Main loop iteration: 1476
2025-03-22 07:10:49,758 - transformer_training - INFO - Main loop iteration: 1477
2025-03-22 07:10:50,020 - transformer_training - INFO - Main loop iteration: 1478
2025-03-22 07:10:50,280 - transformer_training - INFO - Main loop iteration: 1479
2025-03-22 07:10:50,561 - transformer_training - INFO - Main loop iteration: 1480
Iter 1480: loss 5.8205, lr 0.000370, 130976.94 tokens/sec
2025-03-22 07:10:50,812 - transformer_training - INFO - Main loop iteration: 1481
2025-03-22 07:10:51,074 - transformer_training - INFO - Main loop iteration: 1482
2025-03-22 07:10:51,333 - transformer_training - INFO - Main loop iteration: 1483
2025-03-22 07:10:51,615 - transformer_training - INFO - Main loop iteration: 1484
2025-03-22 07:10:51,865 - transformer_training - INFO - Main loop iteration: 1485
2025-03-22 07:10:52,126 - transformer_training - INFO - Main loop iteration: 1486
2025-03-22 07:10:52,386 - transformer_training - INFO - Main loop iteration: 1487
2025-03-22 07:10:52,663 - transformer_training - INFO - Main loop iteration: 1488
2025-03-22 07:10:52,914 - transformer_training - INFO - Main loop iteration: 1489
2025-03-22 07:10:53,176 - transformer_training - INFO - Main loop iteration: 1490
Iter 1490: loss 5.8809, lr 0.000372, 126561.50 tokens/sec
2025-03-22 07:10:53,435 - transformer_training - INFO - Main loop iteration: 1491
2025-03-22 07:10:53,713 - transformer_training - INFO - Main loop iteration: 1492
2025-03-22 07:10:53,963 - transformer_training - INFO - Main loop iteration: 1493
2025-03-22 07:10:54,225 - transformer_training - INFO - Main loop iteration: 1494
2025-03-22 07:10:54,485 - transformer_training - INFO - Main loop iteration: 1495
2025-03-22 07:10:54,764 - transformer_training - INFO - Main loop iteration: 1496
2025-03-22 07:10:55,020 - transformer_training - INFO - Main loop iteration: 1497
2025-03-22 07:10:55,281 - transformer_training - INFO - Main loop iteration: 1498
2025-03-22 07:10:55,541 - transformer_training - INFO - Main loop iteration: 1499
2025-03-22 07:10:55,818 - transformer_training - INFO - Main loop iteration: 1500
Iter 1500: loss 5.8648, lr 0.000375, 129554.16 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 1500: train loss 5.8009, val loss 5.7779
New best model saved with val loss: 5.7779
2025-03-22 07:11:18,193 - transformer_training - INFO - Main loop iteration: 1501
2025-03-22 07:11:18,470 - transformer_training - INFO - Main loop iteration: 1502
2025-03-22 07:11:18,731 - transformer_training - INFO - Main loop iteration: 1503
2025-03-22 07:11:19,009 - transformer_training - INFO - Main loop iteration: 1504
2025-03-22 07:11:19,259 - transformer_training - INFO - Main loop iteration: 1505
2025-03-22 07:11:19,521 - transformer_training - INFO - Main loop iteration: 1506
2025-03-22 07:11:19,782 - transformer_training - INFO - Main loop iteration: 1507
2025-03-22 07:11:20,060 - transformer_training - INFO - Main loop iteration: 1508
2025-03-22 07:11:20,311 - transformer_training - INFO - Main loop iteration: 1509
2025-03-22 07:11:20,573 - transformer_training - INFO - Main loop iteration: 1510
Iter 1510: loss 5.9114, lr 0.000377, 126398.43 tokens/sec
2025-03-22 07:11:20,833 - transformer_training - INFO - Main loop iteration: 1511
2025-03-22 07:11:21,111 - transformer_training - INFO - Main loop iteration: 1512
2025-03-22 07:11:21,361 - transformer_training - INFO - Main loop iteration: 1513
2025-03-22 07:11:21,623 - transformer_training - INFO - Main loop iteration: 1514
2025-03-22 07:11:21,883 - transformer_training - INFO - Main loop iteration: 1515
2025-03-22 07:11:22,162 - transformer_training - INFO - Main loop iteration: 1516
2025-03-22 07:11:22,413 - transformer_training - INFO - Main loop iteration: 1517
2025-03-22 07:11:22,675 - transformer_training - INFO - Main loop iteration: 1518
2025-03-22 07:11:22,935 - transformer_training - INFO - Main loop iteration: 1519
2025-03-22 07:11:23,213 - transformer_training - INFO - Main loop iteration: 1520
Iter 1520: loss 5.7735, lr 0.000380, 129467.88 tokens/sec
2025-03-22 07:11:23,467 - transformer_training - INFO - Main loop iteration: 1521
2025-03-22 07:11:23,728 - transformer_training - INFO - Main loop iteration: 1522
2025-03-22 07:11:23,989 - transformer_training - INFO - Main loop iteration: 1523
2025-03-22 07:11:24,271 - transformer_training - INFO - Main loop iteration: 1524
2025-03-22 07:11:24,522 - transformer_training - INFO - Main loop iteration: 1525
2025-03-22 07:11:24,783 - transformer_training - INFO - Main loop iteration: 1526
2025-03-22 07:11:25,043 - transformer_training - INFO - Main loop iteration: 1527
2025-03-22 07:11:25,326 - transformer_training - INFO - Main loop iteration: 1528
2025-03-22 07:11:25,577 - transformer_training - INFO - Main loop iteration: 1529
2025-03-22 07:11:25,839 - transformer_training - INFO - Main loop iteration: 1530
Iter 1530: loss 5.8047, lr 0.000382, 87896.29 tokens/sec
2025-03-22 07:11:26,213 - transformer_training - INFO - Main loop iteration: 1531
2025-03-22 07:11:26,491 - transformer_training - INFO - Main loop iteration: 1532
2025-03-22 07:11:26,742 - transformer_training - INFO - Main loop iteration: 1533
2025-03-22 07:11:27,004 - transformer_training - INFO - Main loop iteration: 1534
2025-03-22 07:11:27,264 - transformer_training - INFO - Main loop iteration: 1535
2025-03-22 07:11:27,548 - transformer_training - INFO - Main loop iteration: 1536
2025-03-22 07:11:27,805 - transformer_training - INFO - Main loop iteration: 1537
2025-03-22 07:11:28,066 - transformer_training - INFO - Main loop iteration: 1538
2025-03-22 07:11:28,326 - transformer_training - INFO - Main loop iteration: 1539
2025-03-22 07:11:28,604 - transformer_training - INFO - Main loop iteration: 1540
Iter 1540: loss 5.8570, lr 0.000385, 130428.80 tokens/sec
2025-03-22 07:11:28,856 - transformer_training - INFO - Main loop iteration: 1541
2025-03-22 07:11:29,117 - transformer_training - INFO - Main loop iteration: 1542
2025-03-22 07:11:29,377 - transformer_training - INFO - Main loop iteration: 1543
2025-03-22 07:11:29,656 - transformer_training - INFO - Main loop iteration: 1544
2025-03-22 07:11:29,907 - transformer_training - INFO - Main loop iteration: 1545
2025-03-22 07:11:30,168 - transformer_training - INFO - Main loop iteration: 1546
2025-03-22 07:11:30,428 - transformer_training - INFO - Main loop iteration: 1547
2025-03-22 07:11:30,706 - transformer_training - INFO - Main loop iteration: 1548
2025-03-22 07:11:30,957 - transformer_training - INFO - Main loop iteration: 1549
2025-03-22 07:11:31,219 - transformer_training - INFO - Main loop iteration: 1550
Iter 1550: loss 5.7779, lr 0.000387, 126508.84 tokens/sec
2025-03-22 07:11:31,479 - transformer_training - INFO - Main loop iteration: 1551
2025-03-22 07:11:31,757 - transformer_training - INFO - Main loop iteration: 1552
2025-03-22 07:11:32,008 - transformer_training - INFO - Main loop iteration: 1553
2025-03-22 07:11:32,270 - transformer_training - INFO - Main loop iteration: 1554
2025-03-22 07:11:32,529 - transformer_training - INFO - Main loop iteration: 1555
2025-03-22 07:11:32,808 - transformer_training - INFO - Main loop iteration: 1556
2025-03-22 07:11:33,058 - transformer_training - INFO - Main loop iteration: 1557
2025-03-22 07:11:33,320 - transformer_training - INFO - Main loop iteration: 1558
2025-03-22 07:11:33,579 - transformer_training - INFO - Main loop iteration: 1559
2025-03-22 07:11:33,858 - transformer_training - INFO - Main loop iteration: 1560
Iter 1560: loss 5.8433, lr 0.000390, 128922.22 tokens/sec
2025-03-22 07:11:34,113 - transformer_training - INFO - Main loop iteration: 1561
2025-03-22 07:11:34,378 - transformer_training - INFO - Main loop iteration: 1562
2025-03-22 07:11:34,638 - transformer_training - INFO - Main loop iteration: 1563
2025-03-22 07:11:34,917 - transformer_training - INFO - Main loop iteration: 1564
2025-03-22 07:11:35,168 - transformer_training - INFO - Main loop iteration: 1565
2025-03-22 07:11:35,429 - transformer_training - INFO - Main loop iteration: 1566
2025-03-22 07:11:35,689 - transformer_training - INFO - Main loop iteration: 1567
2025-03-22 07:11:35,972 - transformer_training - INFO - Main loop iteration: 1568
2025-03-22 07:11:36,223 - transformer_training - INFO - Main loop iteration: 1569
2025-03-22 07:11:36,484 - transformer_training - INFO - Main loop iteration: 1570
Iter 1570: loss 5.7548, lr 0.000392, 126235.08 tokens/sec
2025-03-22 07:11:36,745 - transformer_training - INFO - Main loop iteration: 1571
2025-03-22 07:11:37,023 - transformer_training - INFO - Main loop iteration: 1572
2025-03-22 07:11:37,274 - transformer_training - INFO - Main loop iteration: 1573
2025-03-22 07:11:37,536 - transformer_training - INFO - Main loop iteration: 1574
2025-03-22 07:11:37,796 - transformer_training - INFO - Main loop iteration: 1575
2025-03-22 07:11:38,074 - transformer_training - INFO - Main loop iteration: 1576
2025-03-22 07:11:38,325 - transformer_training - INFO - Main loop iteration: 1577
2025-03-22 07:11:38,586 - transformer_training - INFO - Main loop iteration: 1578
2025-03-22 07:11:38,846 - transformer_training - INFO - Main loop iteration: 1579
2025-03-22 07:11:39,124 - transformer_training - INFO - Main loop iteration: 1580
Iter 1580: loss 5.8359, lr 0.000395, 130944.12 tokens/sec
2025-03-22 07:11:39,375 - transformer_training - INFO - Main loop iteration: 1581
2025-03-22 07:11:39,637 - transformer_training - INFO - Main loop iteration: 1582
2025-03-22 07:11:39,897 - transformer_training - INFO - Main loop iteration: 1583
2025-03-22 07:11:40,175 - transformer_training - INFO - Main loop iteration: 1584
2025-03-22 07:11:40,427 - transformer_training - INFO - Main loop iteration: 1585
2025-03-22 07:11:40,688 - transformer_training - INFO - Main loop iteration: 1586
2025-03-22 07:11:40,948 - transformer_training - INFO - Main loop iteration: 1587
2025-03-22 07:11:41,226 - transformer_training - INFO - Main loop iteration: 1588
2025-03-22 07:11:41,477 - transformer_training - INFO - Main loop iteration: 1589
2025-03-22 07:11:41,738 - transformer_training - INFO - Main loop iteration: 1590
Iter 1590: loss 5.8417, lr 0.000397, 126499.53 tokens/sec
2025-03-22 07:11:41,998 - transformer_training - INFO - Main loop iteration: 1591
2025-03-22 07:11:42,276 - transformer_training - INFO - Main loop iteration: 1592
2025-03-22 07:11:42,527 - transformer_training - INFO - Main loop iteration: 1593
2025-03-22 07:11:42,789 - transformer_training - INFO - Main loop iteration: 1594
2025-03-22 07:11:43,049 - transformer_training - INFO - Main loop iteration: 1595
2025-03-22 07:11:43,327 - transformer_training - INFO - Main loop iteration: 1596
2025-03-22 07:11:43,578 - transformer_training - INFO - Main loop iteration: 1597
2025-03-22 07:11:43,838 - transformer_training - INFO - Main loop iteration: 1598
2025-03-22 07:11:44,097 - transformer_training - INFO - Main loop iteration: 1599
2025-03-22 07:11:44,375 - transformer_training - INFO - Main loop iteration: 1600
Iter 1600: loss 5.8283, lr 0.000400, 131762.62 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 1600: train loss 5.7156, val loss 5.6872
New best model saved with val loss: 5.6872
2025-03-22 07:12:05,211 - transformer_training - INFO - Main loop iteration: 1601
2025-03-22 07:12:05,470 - transformer_training - INFO - Main loop iteration: 1602
2025-03-22 07:12:05,731 - transformer_training - INFO - Main loop iteration: 1603
2025-03-22 07:12:06,008 - transformer_training - INFO - Main loop iteration: 1604
2025-03-22 07:12:06,262 - transformer_training - INFO - Main loop iteration: 1605
2025-03-22 07:12:06,522 - transformer_training - INFO - Main loop iteration: 1606
2025-03-22 07:12:06,782 - transformer_training - INFO - Main loop iteration: 1607
2025-03-22 07:12:07,061 - transformer_training - INFO - Main loop iteration: 1608
2025-03-22 07:12:07,311 - transformer_training - INFO - Main loop iteration: 1609
2025-03-22 07:12:07,576 - transformer_training - INFO - Main loop iteration: 1610
Iter 1610: loss 5.7921, lr 0.000402, 127206.23 tokens/sec
2025-03-22 07:12:07,836 - transformer_training - INFO - Main loop iteration: 1611
2025-03-22 07:12:08,120 - transformer_training - INFO - Main loop iteration: 1612
2025-03-22 07:12:08,374 - transformer_training - INFO - Main loop iteration: 1613
2025-03-22 07:12:08,635 - transformer_training - INFO - Main loop iteration: 1614
2025-03-22 07:12:08,895 - transformer_training - INFO - Main loop iteration: 1615
2025-03-22 07:12:09,173 - transformer_training - INFO - Main loop iteration: 1616
2025-03-22 07:12:09,422 - transformer_training - INFO - Main loop iteration: 1617
2025-03-22 07:12:09,683 - transformer_training - INFO - Main loop iteration: 1618
2025-03-22 07:12:09,948 - transformer_training - INFO - Main loop iteration: 1619
2025-03-22 07:12:10,227 - transformer_training - INFO - Main loop iteration: 1620
Iter 1620: loss 5.8078, lr 0.000405, 131780.56 tokens/sec
2025-03-22 07:12:10,476 - transformer_training - INFO - Main loop iteration: 1621
2025-03-22 07:12:10,738 - transformer_training - INFO - Main loop iteration: 1622
2025-03-22 07:12:10,997 - transformer_training - INFO - Main loop iteration: 1623
2025-03-22 07:12:11,276 - transformer_training - INFO - Main loop iteration: 1624
2025-03-22 07:12:11,526 - transformer_training - INFO - Main loop iteration: 1625
2025-03-22 07:12:11,787 - transformer_training - INFO - Main loop iteration: 1626
2025-03-22 07:12:12,047 - transformer_training - INFO - Main loop iteration: 1627
2025-03-22 07:12:12,326 - transformer_training - INFO - Main loop iteration: 1628
2025-03-22 07:12:12,578 - transformer_training - INFO - Main loop iteration: 1629
2025-03-22 07:12:12,839 - transformer_training - INFO - Main loop iteration: 1630
Iter 1630: loss 5.7612, lr 0.000407, 127185.16 tokens/sec
2025-03-22 07:12:13,099 - transformer_training - INFO - Main loop iteration: 1631
2025-03-22 07:12:13,378 - transformer_training - INFO - Main loop iteration: 1632
2025-03-22 07:12:13,629 - transformer_training - INFO - Main loop iteration: 1633
2025-03-22 07:12:13,890 - transformer_training - INFO - Main loop iteration: 1634
2025-03-22 07:12:14,156 - transformer_training - INFO - Main loop iteration: 1635
2025-03-22 07:12:14,436 - transformer_training - INFO - Main loop iteration: 1636
2025-03-22 07:12:14,685 - transformer_training - INFO - Main loop iteration: 1637
2025-03-22 07:12:14,946 - transformer_training - INFO - Main loop iteration: 1638
2025-03-22 07:12:15,206 - transformer_training - INFO - Main loop iteration: 1639
2025-03-22 07:12:15,491 - transformer_training - INFO - Main loop iteration: 1640
Iter 1640: loss 5.7103, lr 0.000410, 131697.85 tokens/sec
2025-03-22 07:12:15,741 - transformer_training - INFO - Main loop iteration: 1641
2025-03-22 07:12:16,001 - transformer_training - INFO - Main loop iteration: 1642
2025-03-22 07:12:16,267 - transformer_training - INFO - Main loop iteration: 1643
2025-03-22 07:12:16,546 - transformer_training - INFO - Main loop iteration: 1644
2025-03-22 07:12:16,795 - transformer_training - INFO - Main loop iteration: 1645
2025-03-22 07:12:17,057 - transformer_training - INFO - Main loop iteration: 1646
2025-03-22 07:12:17,323 - transformer_training - INFO - Main loop iteration: 1647
2025-03-22 07:12:17,602 - transformer_training - INFO - Main loop iteration: 1648
2025-03-22 07:12:17,851 - transformer_training - INFO - Main loop iteration: 1649
2025-03-22 07:12:18,112 - transformer_training - INFO - Main loop iteration: 1650
Iter 1650: loss 5.7315, lr 0.000412, 127209.64 tokens/sec
2025-03-22 07:12:18,372 - transformer_training - INFO - Main loop iteration: 1651
2025-03-22 07:12:18,657 - transformer_training - INFO - Main loop iteration: 1652
2025-03-22 07:12:18,906 - transformer_training - INFO - Main loop iteration: 1653
2025-03-22 07:12:19,167 - transformer_training - INFO - Main loop iteration: 1654
2025-03-22 07:12:19,427 - transformer_training - INFO - Main loop iteration: 1655
2025-03-22 07:12:19,706 - transformer_training - INFO - Main loop iteration: 1656
2025-03-22 07:12:19,955 - transformer_training - INFO - Main loop iteration: 1657
2025-03-22 07:12:20,216 - transformer_training - INFO - Main loop iteration: 1658
2025-03-22 07:12:20,475 - transformer_training - INFO - Main loop iteration: 1659
2025-03-22 07:12:20,755 - transformer_training - INFO - Main loop iteration: 1660
Iter 1660: loss 5.7279, lr 0.000415, 131793.32 tokens/sec
2025-03-22 07:12:21,004 - transformer_training - INFO - Main loop iteration: 1661
2025-03-22 07:12:21,268 - transformer_training - INFO - Main loop iteration: 1662
2025-03-22 07:12:21,532 - transformer_training - INFO - Main loop iteration: 1663
2025-03-22 07:12:21,811 - transformer_training - INFO - Main loop iteration: 1664
2025-03-22 07:12:22,061 - transformer_training - INFO - Main loop iteration: 1665
2025-03-22 07:12:22,324 - transformer_training - INFO - Main loop iteration: 1666
2025-03-22 07:12:22,584 - transformer_training - INFO - Main loop iteration: 1667
2025-03-22 07:12:22,865 - transformer_training - INFO - Main loop iteration: 1668
2025-03-22 07:12:23,115 - transformer_training - INFO - Main loop iteration: 1669
2025-03-22 07:12:23,377 - transformer_training - INFO - Main loop iteration: 1670
Iter 1670: loss 5.7023, lr 0.000417, 127153.15 tokens/sec
2025-03-22 07:12:23,637 - transformer_training - INFO - Main loop iteration: 1671
2025-03-22 07:12:23,916 - transformer_training - INFO - Main loop iteration: 1672
2025-03-22 07:12:24,169 - transformer_training - INFO - Main loop iteration: 1673
2025-03-22 07:12:24,430 - transformer_training - INFO - Main loop iteration: 1674
2025-03-22 07:12:24,689 - transformer_training - INFO - Main loop iteration: 1675
2025-03-22 07:12:24,968 - transformer_training - INFO - Main loop iteration: 1676
2025-03-22 07:12:25,217 - transformer_training - INFO - Main loop iteration: 1677
2025-03-22 07:12:25,479 - transformer_training - INFO - Main loop iteration: 1678
2025-03-22 07:12:25,739 - transformer_training - INFO - Main loop iteration: 1679
2025-03-22 07:12:26,018 - transformer_training - INFO - Main loop iteration: 1680
Iter 1680: loss 5.7510, lr 0.000420, 131476.24 tokens/sec
2025-03-22 07:12:26,268 - transformer_training - INFO - Main loop iteration: 1681
2025-03-22 07:12:26,529 - transformer_training - INFO - Main loop iteration: 1682
2025-03-22 07:12:26,789 - transformer_training - INFO - Main loop iteration: 1683
2025-03-22 07:12:27,068 - transformer_training - INFO - Main loop iteration: 1684
2025-03-22 07:12:27,319 - transformer_training - INFO - Main loop iteration: 1685
2025-03-22 07:12:27,583 - transformer_training - INFO - Main loop iteration: 1686
2025-03-22 07:12:27,843 - transformer_training - INFO - Main loop iteration: 1687
2025-03-22 07:12:28,123 - transformer_training - INFO - Main loop iteration: 1688
2025-03-22 07:12:28,376 - transformer_training - INFO - Main loop iteration: 1689
2025-03-22 07:12:28,638 - transformer_training - INFO - Main loop iteration: 1690
Iter 1690: loss 5.7229, lr 0.000422, 127144.57 tokens/sec
2025-03-22 07:12:28,898 - transformer_training - INFO - Main loop iteration: 1691
2025-03-22 07:12:29,188 - transformer_training - INFO - Main loop iteration: 1692
2025-03-22 07:12:29,439 - transformer_training - INFO - Main loop iteration: 1693
2025-03-22 07:12:29,701 - transformer_training - INFO - Main loop iteration: 1694
2025-03-22 07:12:29,965 - transformer_training - INFO - Main loop iteration: 1695
2025-03-22 07:12:30,244 - transformer_training - INFO - Main loop iteration: 1696
2025-03-22 07:12:30,496 - transformer_training - INFO - Main loop iteration: 1697
2025-03-22 07:12:30,757 - transformer_training - INFO - Main loop iteration: 1698
2025-03-22 07:12:31,017 - transformer_training - INFO - Main loop iteration: 1699
2025-03-22 07:12:31,300 - transformer_training - INFO - Main loop iteration: 1700
Iter 1700: loss 5.6694, lr 0.000425, 131432.61 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 1700: train loss 5.6006, val loss 5.5777
New best model saved with val loss: 5.5777
2025-03-22 07:12:52,006 - transformer_training - INFO - Main loop iteration: 1701
2025-03-22 07:12:52,258 - transformer_training - INFO - Main loop iteration: 1702
2025-03-22 07:12:52,518 - transformer_training - INFO - Main loop iteration: 1703
2025-03-22 07:12:52,794 - transformer_training - INFO - Main loop iteration: 1704
2025-03-22 07:12:53,042 - transformer_training - INFO - Main loop iteration: 1705
2025-03-22 07:12:53,304 - transformer_training - INFO - Main loop iteration: 1706
2025-03-22 07:12:53,563 - transformer_training - INFO - Main loop iteration: 1707
2025-03-22 07:12:53,841 - transformer_training - INFO - Main loop iteration: 1708
2025-03-22 07:12:54,090 - transformer_training - INFO - Main loop iteration: 1709
2025-03-22 07:12:54,350 - transformer_training - INFO - Main loop iteration: 1710
Iter 1710: loss 5.6935, lr 0.000427, 127072.74 tokens/sec
2025-03-22 07:12:54,609 - transformer_training - INFO - Main loop iteration: 1711
2025-03-22 07:12:54,890 - transformer_training - INFO - Main loop iteration: 1712
2025-03-22 07:12:55,140 - transformer_training - INFO - Main loop iteration: 1713
2025-03-22 07:12:55,402 - transformer_training - INFO - Main loop iteration: 1714
2025-03-22 07:12:55,661 - transformer_training - INFO - Main loop iteration: 1715
2025-03-22 07:12:55,939 - transformer_training - INFO - Main loop iteration: 1716
2025-03-22 07:12:56,190 - transformer_training - INFO - Main loop iteration: 1717
2025-03-22 07:12:56,452 - transformer_training - INFO - Main loop iteration: 1718
2025-03-22 07:12:56,712 - transformer_training - INFO - Main loop iteration: 1719
2025-03-22 07:12:56,990 - transformer_training - INFO - Main loop iteration: 1720
Iter 1720: loss 5.7306, lr 0.000430, 128618.19 tokens/sec
2025-03-22 07:12:57,245 - transformer_training - INFO - Main loop iteration: 1721
2025-03-22 07:12:57,507 - transformer_training - INFO - Main loop iteration: 1722
2025-03-22 07:12:57,766 - transformer_training - INFO - Main loop iteration: 1723
2025-03-22 07:12:58,050 - transformer_training - INFO - Main loop iteration: 1724
2025-03-22 07:12:58,301 - transformer_training - INFO - Main loop iteration: 1725
2025-03-22 07:12:58,562 - transformer_training - INFO - Main loop iteration: 1726
2025-03-22 07:12:58,822 - transformer_training - INFO - Main loop iteration: 1727
2025-03-22 07:12:59,104 - transformer_training - INFO - Main loop iteration: 1728
2025-03-22 07:12:59,354 - transformer_training - INFO - Main loop iteration: 1729
2025-03-22 07:12:59,615 - transformer_training - INFO - Main loop iteration: 1730
Iter 1730: loss 5.6751, lr 0.000432, 126764.50 tokens/sec
2025-03-22 07:12:59,875 - transformer_training - INFO - Main loop iteration: 1731
2025-03-22 07:13:00,152 - transformer_training - INFO - Main loop iteration: 1732
2025-03-22 07:13:00,403 - transformer_training - INFO - Main loop iteration: 1733
2025-03-22 07:13:00,664 - transformer_training - INFO - Main loop iteration: 1734
2025-03-22 07:13:00,923 - transformer_training - INFO - Main loop iteration: 1735
2025-03-22 07:13:01,201 - transformer_training - INFO - Main loop iteration: 1736
2025-03-22 07:13:01,452 - transformer_training - INFO - Main loop iteration: 1737
2025-03-22 07:13:01,713 - transformer_training - INFO - Main loop iteration: 1738
2025-03-22 07:13:01,973 - transformer_training - INFO - Main loop iteration: 1739
2025-03-22 07:13:02,251 - transformer_training - INFO - Main loop iteration: 1740
Iter 1740: loss 5.5632, lr 0.000435, 130924.42 tokens/sec
2025-03-22 07:13:02,502 - transformer_training - INFO - Main loop iteration: 1741
2025-03-22 07:13:02,763 - transformer_training - INFO - Main loop iteration: 1742
2025-03-22 07:13:03,023 - transformer_training - INFO - Main loop iteration: 1743
2025-03-22 07:13:03,305 - transformer_training - INFO - Main loop iteration: 1744
2025-03-22 07:13:03,556 - transformer_training - INFO - Main loop iteration: 1745
2025-03-22 07:13:03,816 - transformer_training - INFO - Main loop iteration: 1746
2025-03-22 07:13:04,075 - transformer_training - INFO - Main loop iteration: 1747
2025-03-22 07:13:04,359 - transformer_training - INFO - Main loop iteration: 1748
2025-03-22 07:13:04,609 - transformer_training - INFO - Main loop iteration: 1749
2025-03-22 07:13:04,871 - transformer_training - INFO - Main loop iteration: 1750
Iter 1750: loss 5.7229, lr 0.000437, 126582.71 tokens/sec
2025-03-22 07:13:05,131 - transformer_training - INFO - Main loop iteration: 1751
2025-03-22 07:13:05,408 - transformer_training - INFO - Main loop iteration: 1752
2025-03-22 07:13:05,659 - transformer_training - INFO - Main loop iteration: 1753
2025-03-22 07:13:05,920 - transformer_training - INFO - Main loop iteration: 1754
2025-03-22 07:13:06,180 - transformer_training - INFO - Main loop iteration: 1755
2025-03-22 07:13:06,458 - transformer_training - INFO - Main loop iteration: 1756
2025-03-22 07:13:06,709 - transformer_training - INFO - Main loop iteration: 1757
2025-03-22 07:13:06,969 - transformer_training - INFO - Main loop iteration: 1758
2025-03-22 07:13:07,229 - transformer_training - INFO - Main loop iteration: 1759
2025-03-22 07:13:07,507 - transformer_training - INFO - Main loop iteration: 1760
Iter 1760: loss 5.5723, lr 0.000440, 131065.00 tokens/sec
2025-03-22 07:13:07,758 - transformer_training - INFO - Main loop iteration: 1761
2025-03-22 07:13:08,019 - transformer_training - INFO - Main loop iteration: 1762
2025-03-22 07:13:08,279 - transformer_training - INFO - Main loop iteration: 1763
2025-03-22 07:13:08,556 - transformer_training - INFO - Main loop iteration: 1764
2025-03-22 07:13:08,807 - transformer_training - INFO - Main loop iteration: 1765
2025-03-22 07:13:09,069 - transformer_training - INFO - Main loop iteration: 1766
2025-03-22 07:13:09,328 - transformer_training - INFO - Main loop iteration: 1767
2025-03-22 07:13:09,606 - transformer_training - INFO - Main loop iteration: 1768
2025-03-22 07:13:09,857 - transformer_training - INFO - Main loop iteration: 1769
2025-03-22 07:13:10,118 - transformer_training - INFO - Main loop iteration: 1770
Iter 1770: loss 5.6328, lr 0.000442, 126550.66 tokens/sec
2025-03-22 07:13:10,378 - transformer_training - INFO - Main loop iteration: 1771
2025-03-22 07:13:10,655 - transformer_training - INFO - Main loop iteration: 1772
2025-03-22 07:13:10,907 - transformer_training - INFO - Main loop iteration: 1773
2025-03-22 07:13:11,168 - transformer_training - INFO - Main loop iteration: 1774
2025-03-22 07:13:11,428 - transformer_training - INFO - Main loop iteration: 1775
2025-03-22 07:13:11,706 - transformer_training - INFO - Main loop iteration: 1776
2025-03-22 07:13:11,956 - transformer_training - INFO - Main loop iteration: 1777
2025-03-22 07:13:12,218 - transformer_training - INFO - Main loop iteration: 1778
2025-03-22 07:13:12,477 - transformer_training - INFO - Main loop iteration: 1779
2025-03-22 07:13:12,755 - transformer_training - INFO - Main loop iteration: 1780
Iter 1780: loss 5.5333, lr 0.000445, 129079.02 tokens/sec
2025-03-22 07:13:13,010 - transformer_training - INFO - Main loop iteration: 1781
2025-03-22 07:13:13,271 - transformer_training - INFO - Main loop iteration: 1782
2025-03-22 07:13:13,530 - transformer_training - INFO - Main loop iteration: 1783
2025-03-22 07:13:13,808 - transformer_training - INFO - Main loop iteration: 1784
2025-03-22 07:13:14,063 - transformer_training - INFO - Main loop iteration: 1785
2025-03-22 07:13:14,324 - transformer_training - INFO - Main loop iteration: 1786
2025-03-22 07:13:14,584 - transformer_training - INFO - Main loop iteration: 1787
2025-03-22 07:13:14,862 - transformer_training - INFO - Main loop iteration: 1788
2025-03-22 07:13:15,112 - transformer_training - INFO - Main loop iteration: 1789
2025-03-22 07:13:15,374 - transformer_training - INFO - Main loop iteration: 1790
Iter 1790: loss 5.6452, lr 0.000447, 126521.19 tokens/sec
2025-03-22 07:13:15,634 - transformer_training - INFO - Main loop iteration: 1791
2025-03-22 07:13:15,912 - transformer_training - INFO - Main loop iteration: 1792
2025-03-22 07:13:16,166 - transformer_training - INFO - Main loop iteration: 1793
2025-03-22 07:13:16,427 - transformer_training - INFO - Main loop iteration: 1794
2025-03-22 07:13:16,687 - transformer_training - INFO - Main loop iteration: 1795
2025-03-22 07:13:16,965 - transformer_training - INFO - Main loop iteration: 1796
2025-03-22 07:13:17,216 - transformer_training - INFO - Main loop iteration: 1797
2025-03-22 07:13:17,477 - transformer_training - INFO - Main loop iteration: 1798
2025-03-22 07:13:17,743 - transformer_training - INFO - Main loop iteration: 1799
2025-03-22 07:13:18,023 - transformer_training - INFO - Main loop iteration: 1800
Iter 1800: loss 5.5138, lr 0.000450, 130961.47 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 1800: train loss 5.5175, val loss 5.4894
New best model saved with val loss: 5.4894
2025-03-22 07:13:38,762 - transformer_training - INFO - Main loop iteration: 1801
2025-03-22 07:13:39,039 - transformer_training - INFO - Main loop iteration: 1802
2025-03-22 07:13:39,306 - transformer_training - INFO - Main loop iteration: 1803
2025-03-22 07:13:39,585 - transformer_training - INFO - Main loop iteration: 1804
2025-03-22 07:13:39,835 - transformer_training - INFO - Main loop iteration: 1805
2025-03-22 07:13:40,097 - transformer_training - INFO - Main loop iteration: 1806
2025-03-22 07:13:40,357 - transformer_training - INFO - Main loop iteration: 1807
2025-03-22 07:13:40,636 - transformer_training - INFO - Main loop iteration: 1808
2025-03-22 07:13:40,886 - transformer_training - INFO - Main loop iteration: 1809
2025-03-22 07:13:41,147 - transformer_training - INFO - Main loop iteration: 1810
Iter 1810: loss 5.5734, lr 0.000452, 127130.22 tokens/sec
2025-03-22 07:13:41,408 - transformer_training - INFO - Main loop iteration: 1811
2025-03-22 07:13:41,687 - transformer_training - INFO - Main loop iteration: 1812
2025-03-22 07:13:41,936 - transformer_training - INFO - Main loop iteration: 1813
2025-03-22 07:13:42,197 - transformer_training - INFO - Main loop iteration: 1814
2025-03-22 07:13:42,457 - transformer_training - INFO - Main loop iteration: 1815
2025-03-22 07:13:42,737 - transformer_training - INFO - Main loop iteration: 1816
2025-03-22 07:13:42,986 - transformer_training - INFO - Main loop iteration: 1817
2025-03-22 07:13:43,248 - transformer_training - INFO - Main loop iteration: 1818
2025-03-22 07:13:43,508 - transformer_training - INFO - Main loop iteration: 1819
2025-03-22 07:13:43,787 - transformer_training - INFO - Main loop iteration: 1820
Iter 1820: loss 5.5906, lr 0.000455, 131809.50 tokens/sec
2025-03-22 07:13:44,037 - transformer_training - INFO - Main loop iteration: 1821
2025-03-22 07:13:44,298 - transformer_training - INFO - Main loop iteration: 1822
2025-03-22 07:13:44,564 - transformer_training - INFO - Main loop iteration: 1823
2025-03-22 07:13:44,844 - transformer_training - INFO - Main loop iteration: 1824
2025-03-22 07:13:45,096 - transformer_training - INFO - Main loop iteration: 1825
2025-03-22 07:13:45,357 - transformer_training - INFO - Main loop iteration: 1826
2025-03-22 07:13:45,617 - transformer_training - INFO - Main loop iteration: 1827
2025-03-22 07:13:45,896 - transformer_training - INFO - Main loop iteration: 1828
2025-03-22 07:13:46,146 - transformer_training - INFO - Main loop iteration: 1829
2025-03-22 07:13:46,408 - transformer_training - INFO - Main loop iteration: 1830
Iter 1830: loss 5.5708, lr 0.000457, 124976.88 tokens/sec
2025-03-22 07:13:46,672 - transformer_training - INFO - Main loop iteration: 1831
2025-03-22 07:13:46,956 - transformer_training - INFO - Main loop iteration: 1832
2025-03-22 07:13:47,206 - transformer_training - INFO - Main loop iteration: 1833
2025-03-22 07:13:47,468 - transformer_training - INFO - Main loop iteration: 1834
2025-03-22 07:13:47,732 - transformer_training - INFO - Main loop iteration: 1835
2025-03-22 07:13:48,012 - transformer_training - INFO - Main loop iteration: 1836
2025-03-22 07:13:48,261 - transformer_training - INFO - Main loop iteration: 1837
2025-03-22 07:13:48,522 - transformer_training - INFO - Main loop iteration: 1838
2025-03-22 07:13:48,782 - transformer_training - INFO - Main loop iteration: 1839
2025-03-22 07:13:49,062 - transformer_training - INFO - Main loop iteration: 1840
Iter 1840: loss 5.5378, lr 0.000460, 131714.89 tokens/sec
2025-03-22 07:13:49,312 - transformer_training - INFO - Main loop iteration: 1841
2025-03-22 07:13:49,573 - transformer_training - INFO - Main loop iteration: 1842
2025-03-22 07:13:49,833 - transformer_training - INFO - Main loop iteration: 1843
2025-03-22 07:13:50,112 - transformer_training - INFO - Main loop iteration: 1844
2025-03-22 07:13:50,362 - transformer_training - INFO - Main loop iteration: 1845
2025-03-22 07:13:50,623 - transformer_training - INFO - Main loop iteration: 1846
2025-03-22 07:13:50,887 - transformer_training - INFO - Main loop iteration: 1847
2025-03-22 07:13:51,166 - transformer_training - INFO - Main loop iteration: 1848
2025-03-22 07:13:51,416 - transformer_training - INFO - Main loop iteration: 1849
2025-03-22 07:13:51,677 - transformer_training - INFO - Main loop iteration: 1850
Iter 1850: loss 5.5544, lr 0.000462, 127231.55 tokens/sec
2025-03-22 07:13:51,937 - transformer_training - INFO - Main loop iteration: 1851
2025-03-22 07:13:52,216 - transformer_training - INFO - Main loop iteration: 1852
2025-03-22 07:13:52,466 - transformer_training - INFO - Main loop iteration: 1853
2025-03-22 07:13:52,727 - transformer_training - INFO - Main loop iteration: 1854
2025-03-22 07:13:52,987 - transformer_training - INFO - Main loop iteration: 1855
2025-03-22 07:13:53,267 - transformer_training - INFO - Main loop iteration: 1856
2025-03-22 07:13:53,516 - transformer_training - INFO - Main loop iteration: 1857
2025-03-22 07:13:53,778 - transformer_training - INFO - Main loop iteration: 1858
2025-03-22 07:13:54,038 - transformer_training - INFO - Main loop iteration: 1859
2025-03-22 07:13:54,317 - transformer_training - INFO - Main loop iteration: 1860
Iter 1860: loss 5.4818, lr 0.000465, 131080.38 tokens/sec
2025-03-22 07:13:54,568 - transformer_training - INFO - Main loop iteration: 1861
2025-03-22 07:13:54,828 - transformer_training - INFO - Main loop iteration: 1862
2025-03-22 07:13:55,093 - transformer_training - INFO - Main loop iteration: 1863
2025-03-22 07:13:55,372 - transformer_training - INFO - Main loop iteration: 1864
2025-03-22 07:13:55,622 - transformer_training - INFO - Main loop iteration: 1865
2025-03-22 07:13:55,883 - transformer_training - INFO - Main loop iteration: 1866
2025-03-22 07:13:56,143 - transformer_training - INFO - Main loop iteration: 1867
2025-03-22 07:13:56,423 - transformer_training - INFO - Main loop iteration: 1868
2025-03-22 07:13:56,672 - transformer_training - INFO - Main loop iteration: 1869
2025-03-22 07:13:56,933 - transformer_training - INFO - Main loop iteration: 1870
Iter 1870: loss 5.5587, lr 0.000467, 127070.86 tokens/sec
2025-03-22 07:13:57,194 - transformer_training - INFO - Main loop iteration: 1871
2025-03-22 07:13:57,473 - transformer_training - INFO - Main loop iteration: 1872
2025-03-22 07:13:57,722 - transformer_training - INFO - Main loop iteration: 1873
2025-03-22 07:13:57,984 - transformer_training - INFO - Main loop iteration: 1874
2025-03-22 07:13:58,244 - transformer_training - INFO - Main loop iteration: 1875
2025-03-22 07:13:58,527 - transformer_training - INFO - Main loop iteration: 1876
2025-03-22 07:13:58,779 - transformer_training - INFO - Main loop iteration: 1877
2025-03-22 07:13:59,040 - transformer_training - INFO - Main loop iteration: 1878
2025-03-22 07:13:59,304 - transformer_training - INFO - Main loop iteration: 1879
2025-03-22 07:13:59,584 - transformer_training - INFO - Main loop iteration: 1880
Iter 1880: loss 5.5382, lr 0.000470, 131607.43 tokens/sec
2025-03-22 07:13:59,834 - transformer_training - INFO - Main loop iteration: 1881
2025-03-22 07:14:00,095 - transformer_training - INFO - Main loop iteration: 1882
2025-03-22 07:14:00,354 - transformer_training - INFO - Main loop iteration: 1883
2025-03-22 07:14:00,634 - transformer_training - INFO - Main loop iteration: 1884
2025-03-22 07:14:00,884 - transformer_training - INFO - Main loop iteration: 1885
2025-03-22 07:14:01,145 - transformer_training - INFO - Main loop iteration: 1886
2025-03-22 07:14:01,405 - transformer_training - INFO - Main loop iteration: 1887
2025-03-22 07:14:01,684 - transformer_training - INFO - Main loop iteration: 1888
2025-03-22 07:14:01,933 - transformer_training - INFO - Main loop iteration: 1889
2025-03-22 07:14:02,194 - transformer_training - INFO - Main loop iteration: 1890
Iter 1890: loss 5.4371, lr 0.000472, 127350.15 tokens/sec
2025-03-22 07:14:02,454 - transformer_training - INFO - Main loop iteration: 1891
2025-03-22 07:14:02,733 - transformer_training - INFO - Main loop iteration: 1892
2025-03-22 07:14:02,982 - transformer_training - INFO - Main loop iteration: 1893
2025-03-22 07:14:03,356 - transformer_training - INFO - Main loop iteration: 1894
2025-03-22 07:14:03,615 - transformer_training - INFO - Main loop iteration: 1895
2025-03-22 07:14:03,895 - transformer_training - INFO - Main loop iteration: 1896
2025-03-22 07:14:04,148 - transformer_training - INFO - Main loop iteration: 1897
2025-03-22 07:14:04,409 - transformer_training - INFO - Main loop iteration: 1898
2025-03-22 07:14:04,669 - transformer_training - INFO - Main loop iteration: 1899
2025-03-22 07:14:04,948 - transformer_training - INFO - Main loop iteration: 1900
Iter 1900: loss 5.4111, lr 0.000475, 131335.91 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 1900: train loss 5.4178, val loss 5.3803
New best model saved with val loss: 5.3803
2025-03-22 07:14:25,210 - transformer_training - INFO - Main loop iteration: 1901
2025-03-22 07:14:25,465 - transformer_training - INFO - Main loop iteration: 1902
2025-03-22 07:14:25,727 - transformer_training - INFO - Main loop iteration: 1903
2025-03-22 07:14:26,004 - transformer_training - INFO - Main loop iteration: 1904
2025-03-22 07:14:26,255 - transformer_training - INFO - Main loop iteration: 1905
2025-03-22 07:14:26,517 - transformer_training - INFO - Main loop iteration: 1906
2025-03-22 07:14:26,777 - transformer_training - INFO - Main loop iteration: 1907
2025-03-22 07:14:27,061 - transformer_training - INFO - Main loop iteration: 1908
2025-03-22 07:14:27,312 - transformer_training - INFO - Main loop iteration: 1909
2025-03-22 07:14:27,574 - transformer_training - INFO - Main loop iteration: 1910
Iter 1910: loss 5.4789, lr 0.000477, 126657.25 tokens/sec
2025-03-22 07:14:27,833 - transformer_training - INFO - Main loop iteration: 1911
2025-03-22 07:14:28,111 - transformer_training - INFO - Main loop iteration: 1912
2025-03-22 07:14:28,361 - transformer_training - INFO - Main loop iteration: 1913
2025-03-22 07:14:28,622 - transformer_training - INFO - Main loop iteration: 1914
2025-03-22 07:14:28,883 - transformer_training - INFO - Main loop iteration: 1915
2025-03-22 07:14:29,161 - transformer_training - INFO - Main loop iteration: 1916
2025-03-22 07:14:29,412 - transformer_training - INFO - Main loop iteration: 1917
2025-03-22 07:14:29,672 - transformer_training - INFO - Main loop iteration: 1918
2025-03-22 07:14:29,933 - transformer_training - INFO - Main loop iteration: 1919
2025-03-22 07:14:30,211 - transformer_training - INFO - Main loop iteration: 1920
Iter 1920: loss 5.4905, lr 0.000480, 129034.67 tokens/sec
2025-03-22 07:14:30,466 - transformer_training - INFO - Main loop iteration: 1921
2025-03-22 07:14:30,727 - transformer_training - INFO - Main loop iteration: 1922
2025-03-22 07:14:30,986 - transformer_training - INFO - Main loop iteration: 1923
2025-03-22 07:14:31,265 - transformer_training - INFO - Main loop iteration: 1924
2025-03-22 07:14:31,516 - transformer_training - INFO - Main loop iteration: 1925
2025-03-22 07:14:31,777 - transformer_training - INFO - Main loop iteration: 1926
2025-03-22 07:14:32,037 - transformer_training - INFO - Main loop iteration: 1927
2025-03-22 07:14:32,315 - transformer_training - INFO - Main loop iteration: 1928
2025-03-22 07:14:32,566 - transformer_training - INFO - Main loop iteration: 1929
2025-03-22 07:14:32,827 - transformer_training - INFO - Main loop iteration: 1930
Iter 1930: loss 5.5323, lr 0.000482, 126583.06 tokens/sec
2025-03-22 07:14:33,087 - transformer_training - INFO - Main loop iteration: 1931
2025-03-22 07:14:33,364 - transformer_training - INFO - Main loop iteration: 1932
2025-03-22 07:14:33,615 - transformer_training - INFO - Main loop iteration: 1933
2025-03-22 07:14:33,875 - transformer_training - INFO - Main loop iteration: 1934
2025-03-22 07:14:34,134 - transformer_training - INFO - Main loop iteration: 1935
2025-03-22 07:14:34,414 - transformer_training - INFO - Main loop iteration: 1936
2025-03-22 07:14:34,665 - transformer_training - INFO - Main loop iteration: 1937
2025-03-22 07:14:34,925 - transformer_training - INFO - Main loop iteration: 1938
2025-03-22 07:14:35,191 - transformer_training - INFO - Main loop iteration: 1939
2025-03-22 07:14:35,469 - transformer_training - INFO - Main loop iteration: 1940
Iter 1940: loss 5.5373, lr 0.000485, 130896.86 tokens/sec
2025-03-22 07:14:35,720 - transformer_training - INFO - Main loop iteration: 1941
2025-03-22 07:14:35,980 - transformer_training - INFO - Main loop iteration: 1942
2025-03-22 07:14:36,242 - transformer_training - INFO - Main loop iteration: 1943
2025-03-22 07:14:36,520 - transformer_training - INFO - Main loop iteration: 1944
2025-03-22 07:14:36,771 - transformer_training - INFO - Main loop iteration: 1945
2025-03-22 07:14:37,031 - transformer_training - INFO - Main loop iteration: 1946
2025-03-22 07:14:37,292 - transformer_training - INFO - Main loop iteration: 1947
2025-03-22 07:14:37,570 - transformer_training - INFO - Main loop iteration: 1948
2025-03-22 07:14:37,821 - transformer_training - INFO - Main loop iteration: 1949
2025-03-22 07:14:38,080 - transformer_training - INFO - Main loop iteration: 1950
Iter 1950: loss 5.4018, lr 0.000487, 126246.22 tokens/sec
2025-03-22 07:14:38,342 - transformer_training - INFO - Main loop iteration: 1951
2025-03-22 07:14:38,619 - transformer_training - INFO - Main loop iteration: 1952
2025-03-22 07:14:38,871 - transformer_training - INFO - Main loop iteration: 1953
2025-03-22 07:14:39,132 - transformer_training - INFO - Main loop iteration: 1954
2025-03-22 07:14:39,392 - transformer_training - INFO - Main loop iteration: 1955
2025-03-22 07:14:39,669 - transformer_training - INFO - Main loop iteration: 1956
2025-03-22 07:14:39,920 - transformer_training - INFO - Main loop iteration: 1957
2025-03-22 07:14:40,181 - transformer_training - INFO - Main loop iteration: 1958
2025-03-22 07:14:40,441 - transformer_training - INFO - Main loop iteration: 1959
2025-03-22 07:14:40,719 - transformer_training - INFO - Main loop iteration: 1960
Iter 1960: loss 5.4607, lr 0.000490, 130195.90 tokens/sec
2025-03-22 07:14:40,972 - transformer_training - INFO - Main loop iteration: 1961
2025-03-22 07:14:41,233 - transformer_training - INFO - Main loop iteration: 1962
2025-03-22 07:14:41,493 - transformer_training - INFO - Main loop iteration: 1963
2025-03-22 07:14:41,771 - transformer_training - INFO - Main loop iteration: 1964
2025-03-22 07:14:42,022 - transformer_training - INFO - Main loop iteration: 1965
2025-03-22 07:14:42,283 - transformer_training - INFO - Main loop iteration: 1966
2025-03-22 07:14:42,543 - transformer_training - INFO - Main loop iteration: 1967
2025-03-22 07:14:42,821 - transformer_training - INFO - Main loop iteration: 1968
2025-03-22 07:14:43,072 - transformer_training - INFO - Main loop iteration: 1969
2025-03-22 07:14:43,333 - transformer_training - INFO - Main loop iteration: 1970
Iter 1970: loss 5.3643, lr 0.000492, 126423.78 tokens/sec
2025-03-22 07:14:43,593 - transformer_training - INFO - Main loop iteration: 1971
2025-03-22 07:14:43,871 - transformer_training - INFO - Main loop iteration: 1972
2025-03-22 07:14:44,122 - transformer_training - INFO - Main loop iteration: 1973
2025-03-22 07:14:44,382 - transformer_training - INFO - Main loop iteration: 1974
2025-03-22 07:14:44,642 - transformer_training - INFO - Main loop iteration: 1975
2025-03-22 07:14:44,920 - transformer_training - INFO - Main loop iteration: 1976
2025-03-22 07:14:45,171 - transformer_training - INFO - Main loop iteration: 1977
2025-03-22 07:14:45,432 - transformer_training - INFO - Main loop iteration: 1978
2025-03-22 07:14:45,691 - transformer_training - INFO - Main loop iteration: 1979
2025-03-22 07:14:45,969 - transformer_training - INFO - Main loop iteration: 1980
Iter 1980: loss 5.3552, lr 0.000495, 131075.13 tokens/sec
2025-03-22 07:14:46,220 - transformer_training - INFO - Main loop iteration: 1981
2025-03-22 07:14:46,479 - transformer_training - INFO - Main loop iteration: 1982
2025-03-22 07:14:46,739 - transformer_training - INFO - Main loop iteration: 1983
2025-03-22 07:14:47,019 - transformer_training - INFO - Main loop iteration: 1984
2025-03-22 07:14:47,270 - transformer_training - INFO - Main loop iteration: 1985
2025-03-22 07:14:47,529 - transformer_training - INFO - Main loop iteration: 1986
2025-03-22 07:14:47,791 - transformer_training - INFO - Main loop iteration: 1987
2025-03-22 07:14:48,069 - transformer_training - INFO - Main loop iteration: 1988
2025-03-22 07:14:48,319 - transformer_training - INFO - Main loop iteration: 1989
2025-03-22 07:14:48,579 - transformer_training - INFO - Main loop iteration: 1990
Iter 1990: loss 5.4765, lr 0.000497, 126425.40 tokens/sec
2025-03-22 07:14:48,841 - transformer_training - INFO - Main loop iteration: 1991
2025-03-22 07:14:49,119 - transformer_training - INFO - Main loop iteration: 1992
2025-03-22 07:14:49,369 - transformer_training - INFO - Main loop iteration: 1993
2025-03-22 07:14:49,631 - transformer_training - INFO - Main loop iteration: 1994
2025-03-22 07:14:49,891 - transformer_training - INFO - Main loop iteration: 1995
2025-03-22 07:14:50,170 - transformer_training - INFO - Main loop iteration: 1996
2025-03-22 07:14:50,425 - transformer_training - INFO - Main loop iteration: 1997
2025-03-22 07:14:50,687 - transformer_training - INFO - Main loop iteration: 1998
2025-03-22 07:14:50,947 - transformer_training - INFO - Main loop iteration: 1999
2025-03-22 07:14:51,224 - transformer_training - INFO - Main loop iteration: 2000
Iter 2000: loss 5.4013, lr 0.000500, 130801.19 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 2000: train loss 5.3106, val loss 5.2663
New best model saved with val loss: 5.2663
2025-03-22 07:15:14,174 - transformer_training - INFO - Main loop iteration: 2001
2025-03-22 07:15:14,450 - transformer_training - INFO - Main loop iteration: 2002
2025-03-22 07:15:14,712 - transformer_training - INFO - Main loop iteration: 2003
2025-03-22 07:15:14,989 - transformer_training - INFO - Main loop iteration: 2004
2025-03-22 07:15:15,240 - transformer_training - INFO - Main loop iteration: 2005
2025-03-22 07:15:15,501 - transformer_training - INFO - Main loop iteration: 2006
2025-03-22 07:15:15,760 - transformer_training - INFO - Main loop iteration: 2007
2025-03-22 07:15:16,038 - transformer_training - INFO - Main loop iteration: 2008
2025-03-22 07:15:16,288 - transformer_training - INFO - Main loop iteration: 2009
2025-03-22 07:15:16,549 - transformer_training - INFO - Main loop iteration: 2010
Iter 2010: loss 5.3966, lr 0.000500, 127187.40 tokens/sec
2025-03-22 07:15:16,809 - transformer_training - INFO - Main loop iteration: 2011
2025-03-22 07:15:17,088 - transformer_training - INFO - Main loop iteration: 2012
2025-03-22 07:15:17,338 - transformer_training - INFO - Main loop iteration: 2013
2025-03-22 07:15:17,599 - transformer_training - INFO - Main loop iteration: 2014
2025-03-22 07:15:17,859 - transformer_training - INFO - Main loop iteration: 2015
2025-03-22 07:15:18,138 - transformer_training - INFO - Main loop iteration: 2016
2025-03-22 07:15:18,388 - transformer_training - INFO - Main loop iteration: 2017
2025-03-22 07:15:18,651 - transformer_training - INFO - Main loop iteration: 2018
2025-03-22 07:15:18,911 - transformer_training - INFO - Main loop iteration: 2019
2025-03-22 07:15:19,189 - transformer_training - INFO - Main loop iteration: 2020
Iter 2020: loss 5.3778, lr 0.000500, 131522.92 tokens/sec
2025-03-22 07:15:19,439 - transformer_training - INFO - Main loop iteration: 2021
2025-03-22 07:15:19,700 - transformer_training - INFO - Main loop iteration: 2022
2025-03-22 07:15:19,960 - transformer_training - INFO - Main loop iteration: 2023
2025-03-22 07:15:20,239 - transformer_training - INFO - Main loop iteration: 2024
2025-03-22 07:15:20,489 - transformer_training - INFO - Main loop iteration: 2025
2025-03-22 07:15:20,751 - transformer_training - INFO - Main loop iteration: 2026
2025-03-22 07:15:21,135 - transformer_training - INFO - Main loop iteration: 2027
2025-03-22 07:15:21,413 - transformer_training - INFO - Main loop iteration: 2028
2025-03-22 07:15:21,664 - transformer_training - INFO - Main loop iteration: 2029
2025-03-22 07:15:21,925 - transformer_training - INFO - Main loop iteration: 2030
Iter 2030: loss 5.3494, lr 0.000500, 126933.20 tokens/sec
2025-03-22 07:15:22,184 - transformer_training - INFO - Main loop iteration: 2031
2025-03-22 07:15:22,463 - transformer_training - INFO - Main loop iteration: 2032
2025-03-22 07:15:22,714 - transformer_training - INFO - Main loop iteration: 2033
2025-03-22 07:15:22,974 - transformer_training - INFO - Main loop iteration: 2034
2025-03-22 07:15:23,233 - transformer_training - INFO - Main loop iteration: 2035
2025-03-22 07:15:23,513 - transformer_training - INFO - Main loop iteration: 2036
2025-03-22 07:15:23,763 - transformer_training - INFO - Main loop iteration: 2037
2025-03-22 07:15:24,024 - transformer_training - INFO - Main loop iteration: 2038
2025-03-22 07:15:24,290 - transformer_training - INFO - Main loop iteration: 2039
2025-03-22 07:15:24,569 - transformer_training - INFO - Main loop iteration: 2040
Iter 2040: loss 5.3646, lr 0.000500, 131042.26 tokens/sec
2025-03-22 07:15:24,820 - transformer_training - INFO - Main loop iteration: 2041
2025-03-22 07:15:25,080 - transformer_training - INFO - Main loop iteration: 2042
2025-03-22 07:15:25,341 - transformer_training - INFO - Main loop iteration: 2043
2025-03-22 07:15:25,618 - transformer_training - INFO - Main loop iteration: 2044
2025-03-22 07:15:25,873 - transformer_training - INFO - Main loop iteration: 2045
2025-03-22 07:15:26,133 - transformer_training - INFO - Main loop iteration: 2046
2025-03-22 07:15:26,394 - transformer_training - INFO - Main loop iteration: 2047
2025-03-22 07:15:26,673 - transformer_training - INFO - Main loop iteration: 2048
2025-03-22 07:15:26,923 - transformer_training - INFO - Main loop iteration: 2049
2025-03-22 07:15:27,184 - transformer_training - INFO - Main loop iteration: 2050
Iter 2050: loss 5.3408, lr 0.000500, 124135.14 tokens/sec
2025-03-22 07:15:27,450 - transformer_training - INFO - Main loop iteration: 2051
2025-03-22 07:15:27,729 - transformer_training - INFO - Main loop iteration: 2052
2025-03-22 07:15:27,980 - transformer_training - INFO - Main loop iteration: 2053
2025-03-22 07:15:28,243 - transformer_training - INFO - Main loop iteration: 2054
2025-03-22 07:15:28,504 - transformer_training - INFO - Main loop iteration: 2055
2025-03-22 07:15:28,782 - transformer_training - INFO - Main loop iteration: 2056
2025-03-22 07:15:29,036 - transformer_training - INFO - Main loop iteration: 2057
2025-03-22 07:15:29,298 - transformer_training - INFO - Main loop iteration: 2058
2025-03-22 07:15:29,558 - transformer_training - INFO - Main loop iteration: 2059
2025-03-22 07:15:29,837 - transformer_training - INFO - Main loop iteration: 2060
Iter 2060: loss 5.3406, lr 0.000500, 130796.46 tokens/sec
2025-03-22 07:15:30,088 - transformer_training - INFO - Main loop iteration: 2061
2025-03-22 07:15:30,350 - transformer_training - INFO - Main loop iteration: 2062
2025-03-22 07:15:30,610 - transformer_training - INFO - Main loop iteration: 2063
2025-03-22 07:15:30,889 - transformer_training - INFO - Main loop iteration: 2064
2025-03-22 07:15:31,140 - transformer_training - INFO - Main loop iteration: 2065
2025-03-22 07:15:31,401 - transformer_training - INFO - Main loop iteration: 2066
2025-03-22 07:15:31,662 - transformer_training - INFO - Main loop iteration: 2067
2025-03-22 07:15:31,940 - transformer_training - INFO - Main loop iteration: 2068
2025-03-22 07:15:32,190 - transformer_training - INFO - Main loop iteration: 2069
2025-03-22 07:15:32,451 - transformer_training - INFO - Main loop iteration: 2070
Iter 2070: loss 5.3275, lr 0.000500, 126494.29 tokens/sec
2025-03-22 07:15:32,712 - transformer_training - INFO - Main loop iteration: 2071
2025-03-22 07:15:32,990 - transformer_training - INFO - Main loop iteration: 2072
2025-03-22 07:15:33,260 - transformer_training - INFO - Main loop iteration: 2073
2025-03-22 07:15:33,534 - transformer_training - INFO - Main loop iteration: 2074
2025-03-22 07:15:33,795 - transformer_training - INFO - Main loop iteration: 2075
2025-03-22 07:15:34,073 - transformer_training - INFO - Main loop iteration: 2076
2025-03-22 07:15:34,324 - transformer_training - INFO - Main loop iteration: 2077
2025-03-22 07:15:34,585 - transformer_training - INFO - Main loop iteration: 2078
2025-03-22 07:15:34,847 - transformer_training - INFO - Main loop iteration: 2079
2025-03-22 07:15:35,125 - transformer_training - INFO - Main loop iteration: 2080
Iter 2080: loss 5.3203, lr 0.000500, 128835.93 tokens/sec
2025-03-22 07:15:35,380 - transformer_training - INFO - Main loop iteration: 2081
2025-03-22 07:15:35,643 - transformer_training - INFO - Main loop iteration: 2082
2025-03-22 07:15:35,903 - transformer_training - INFO - Main loop iteration: 2083
2025-03-22 07:15:36,181 - transformer_training - INFO - Main loop iteration: 2084
2025-03-22 07:15:36,431 - transformer_training - INFO - Main loop iteration: 2085
2025-03-22 07:15:36,692 - transformer_training - INFO - Main loop iteration: 2086
2025-03-22 07:15:36,954 - transformer_training - INFO - Main loop iteration: 2087
2025-03-22 07:15:37,232 - transformer_training - INFO - Main loop iteration: 2088
2025-03-22 07:15:37,484 - transformer_training - INFO - Main loop iteration: 2089
2025-03-22 07:15:37,746 - transformer_training - INFO - Main loop iteration: 2090
Iter 2090: loss 5.3624, lr 0.000500, 126469.26 tokens/sec
2025-03-22 07:15:38,006 - transformer_training - INFO - Main loop iteration: 2091
2025-03-22 07:15:38,284 - transformer_training - INFO - Main loop iteration: 2092
2025-03-22 07:15:38,535 - transformer_training - INFO - Main loop iteration: 2093
2025-03-22 07:15:38,797 - transformer_training - INFO - Main loop iteration: 2094
2025-03-22 07:15:39,057 - transformer_training - INFO - Main loop iteration: 2095
2025-03-22 07:15:39,335 - transformer_training - INFO - Main loop iteration: 2096
2025-03-22 07:15:39,586 - transformer_training - INFO - Main loop iteration: 2097
2025-03-22 07:15:39,846 - transformer_training - INFO - Main loop iteration: 2098
2025-03-22 07:15:40,108 - transformer_training - INFO - Main loop iteration: 2099
2025-03-22 07:15:40,387 - transformer_training - INFO - Main loop iteration: 2100
Iter 2100: loss 5.3086, lr 0.000500, 130736.11 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 2100: train loss 5.2196, val loss 5.1706
New best model saved with val loss: 5.1706
2025-03-22 07:16:01,155 - transformer_training - INFO - Main loop iteration: 2101
2025-03-22 07:16:01,431 - transformer_training - INFO - Main loop iteration: 2102
2025-03-22 07:16:01,694 - transformer_training - INFO - Main loop iteration: 2103
2025-03-22 07:16:01,971 - transformer_training - INFO - Main loop iteration: 2104
2025-03-22 07:16:02,222 - transformer_training - INFO - Main loop iteration: 2105
2025-03-22 07:16:02,483 - transformer_training - INFO - Main loop iteration: 2106
2025-03-22 07:16:02,743 - transformer_training - INFO - Main loop iteration: 2107
2025-03-22 07:16:03,022 - transformer_training - INFO - Main loop iteration: 2108
2025-03-22 07:16:03,273 - transformer_training - INFO - Main loop iteration: 2109
2025-03-22 07:16:03,535 - transformer_training - INFO - Main loop iteration: 2110
Iter 2110: loss 5.2704, lr 0.000500, 126211.32 tokens/sec
2025-03-22 07:16:03,795 - transformer_training - INFO - Main loop iteration: 2111
2025-03-22 07:16:04,078 - transformer_training - INFO - Main loop iteration: 2112
2025-03-22 07:16:04,332 - transformer_training - INFO - Main loop iteration: 2113
2025-03-22 07:16:04,594 - transformer_training - INFO - Main loop iteration: 2114
2025-03-22 07:16:04,858 - transformer_training - INFO - Main loop iteration: 2115
2025-03-22 07:16:05,136 - transformer_training - INFO - Main loop iteration: 2116
2025-03-22 07:16:05,387 - transformer_training - INFO - Main loop iteration: 2117
2025-03-22 07:16:05,649 - transformer_training - INFO - Main loop iteration: 2118
2025-03-22 07:16:05,908 - transformer_training - INFO - Main loop iteration: 2119
2025-03-22 07:16:06,187 - transformer_training - INFO - Main loop iteration: 2120
Iter 2120: loss 5.2512, lr 0.000500, 130587.30 tokens/sec
2025-03-22 07:16:06,439 - transformer_training - INFO - Main loop iteration: 2121
2025-03-22 07:16:06,700 - transformer_training - INFO - Main loop iteration: 2122
2025-03-22 07:16:06,960 - transformer_training - INFO - Main loop iteration: 2123
2025-03-22 07:16:07,246 - transformer_training - INFO - Main loop iteration: 2124
2025-03-22 07:16:07,496 - transformer_training - INFO - Main loop iteration: 2125
2025-03-22 07:16:07,758 - transformer_training - INFO - Main loop iteration: 2126
2025-03-22 07:16:08,017 - transformer_training - INFO - Main loop iteration: 2127
2025-03-22 07:16:08,295 - transformer_training - INFO - Main loop iteration: 2128
2025-03-22 07:16:08,547 - transformer_training - INFO - Main loop iteration: 2129
2025-03-22 07:16:08,808 - transformer_training - INFO - Main loop iteration: 2130
Iter 2130: loss 5.2998, lr 0.000500, 126345.91 tokens/sec
2025-03-22 07:16:09,068 - transformer_training - INFO - Main loop iteration: 2131
2025-03-22 07:16:09,350 - transformer_training - INFO - Main loop iteration: 2132
2025-03-22 07:16:09,601 - transformer_training - INFO - Main loop iteration: 2133
2025-03-22 07:16:09,861 - transformer_training - INFO - Main loop iteration: 2134
2025-03-22 07:16:10,123 - transformer_training - INFO - Main loop iteration: 2135
2025-03-22 07:16:10,401 - transformer_training - INFO - Main loop iteration: 2136
2025-03-22 07:16:10,652 - transformer_training - INFO - Main loop iteration: 2137
2025-03-22 07:16:10,914 - transformer_training - INFO - Main loop iteration: 2138
2025-03-22 07:16:11,174 - transformer_training - INFO - Main loop iteration: 2139
2025-03-22 07:16:11,452 - transformer_training - INFO - Main loop iteration: 2140
Iter 2140: loss 5.2621, lr 0.000500, 129294.19 tokens/sec
2025-03-22 07:16:11,706 - transformer_training - INFO - Main loop iteration: 2141
2025-03-22 07:16:11,968 - transformer_training - INFO - Main loop iteration: 2142
2025-03-22 07:16:12,229 - transformer_training - INFO - Main loop iteration: 2143
2025-03-22 07:16:12,508 - transformer_training - INFO - Main loop iteration: 2144
2025-03-22 07:16:12,758 - transformer_training - INFO - Main loop iteration: 2145
2025-03-22 07:16:13,018 - transformer_training - INFO - Main loop iteration: 2146
2025-03-22 07:16:13,279 - transformer_training - INFO - Main loop iteration: 2147
2025-03-22 07:16:13,557 - transformer_training - INFO - Main loop iteration: 2148
2025-03-22 07:16:13,807 - transformer_training - INFO - Main loop iteration: 2149
2025-03-22 07:16:14,067 - transformer_training - INFO - Main loop iteration: 2150
Iter 2150: loss 5.3446, lr 0.000500, 126454.37 tokens/sec
2025-03-22 07:16:14,329 - transformer_training - INFO - Main loop iteration: 2151
2025-03-22 07:16:14,611 - transformer_training - INFO - Main loop iteration: 2152
2025-03-22 07:16:14,861 - transformer_training - INFO - Main loop iteration: 2153
2025-03-22 07:16:15,122 - transformer_training - INFO - Main loop iteration: 2154
2025-03-22 07:16:15,382 - transformer_training - INFO - Main loop iteration: 2155
2025-03-22 07:16:15,664 - transformer_training - INFO - Main loop iteration: 2156
2025-03-22 07:16:15,914 - transformer_training - INFO - Main loop iteration: 2157
2025-03-22 07:16:16,175 - transformer_training - INFO - Main loop iteration: 2158
2025-03-22 07:16:16,434 - transformer_training - INFO - Main loop iteration: 2159
2025-03-22 07:16:16,717 - transformer_training - INFO - Main loop iteration: 2160
Iter 2160: loss 5.3002, lr 0.000500, 130967.33 tokens/sec
2025-03-22 07:16:16,968 - transformer_training - INFO - Main loop iteration: 2161
2025-03-22 07:16:17,227 - transformer_training - INFO - Main loop iteration: 2162
2025-03-22 07:16:17,488 - transformer_training - INFO - Main loop iteration: 2163
2025-03-22 07:16:17,766 - transformer_training - INFO - Main loop iteration: 2164
2025-03-22 07:16:18,018 - transformer_training - INFO - Main loop iteration: 2165
2025-03-22 07:16:18,279 - transformer_training - INFO - Main loop iteration: 2166
2025-03-22 07:16:18,538 - transformer_training - INFO - Main loop iteration: 2167
2025-03-22 07:16:18,821 - transformer_training - INFO - Main loop iteration: 2168
2025-03-22 07:16:19,071 - transformer_training - INFO - Main loop iteration: 2169
2025-03-22 07:16:19,332 - transformer_training - INFO - Main loop iteration: 2170
Iter 2170: loss 5.2077, lr 0.000500, 126559.28 tokens/sec
2025-03-22 07:16:19,592 - transformer_training - INFO - Main loop iteration: 2171
2025-03-22 07:16:19,870 - transformer_training - INFO - Main loop iteration: 2172
2025-03-22 07:16:20,123 - transformer_training - INFO - Main loop iteration: 2173
2025-03-22 07:16:20,383 - transformer_training - INFO - Main loop iteration: 2174
2025-03-22 07:16:20,643 - transformer_training - INFO - Main loop iteration: 2175
2025-03-22 07:16:20,921 - transformer_training - INFO - Main loop iteration: 2176
2025-03-22 07:16:21,172 - transformer_training - INFO - Main loop iteration: 2177
2025-03-22 07:16:21,432 - transformer_training - INFO - Main loop iteration: 2178
2025-03-22 07:16:21,692 - transformer_training - INFO - Main loop iteration: 2179
2025-03-22 07:16:21,970 - transformer_training - INFO - Main loop iteration: 2180
Iter 2180: loss 5.2758, lr 0.000500, 131149.67 tokens/sec
2025-03-22 07:16:22,221 - transformer_training - INFO - Main loop iteration: 2181
2025-03-22 07:16:22,482 - transformer_training - INFO - Main loop iteration: 2182
2025-03-22 07:16:22,741 - transformer_training - INFO - Main loop iteration: 2183
2025-03-22 07:16:23,019 - transformer_training - INFO - Main loop iteration: 2184
2025-03-22 07:16:23,269 - transformer_training - INFO - Main loop iteration: 2185
2025-03-22 07:16:23,529 - transformer_training - INFO - Main loop iteration: 2186
2025-03-22 07:16:23,790 - transformer_training - INFO - Main loop iteration: 2187
2025-03-22 07:16:24,068 - transformer_training - INFO - Main loop iteration: 2188
2025-03-22 07:16:24,318 - transformer_training - INFO - Main loop iteration: 2189
2025-03-22 07:16:24,579 - transformer_training - INFO - Main loop iteration: 2190
Iter 2190: loss 5.2791, lr 0.000500, 126810.34 tokens/sec
2025-03-22 07:16:24,839 - transformer_training - INFO - Main loop iteration: 2191
2025-03-22 07:16:25,121 - transformer_training - INFO - Main loop iteration: 2192
2025-03-22 07:16:25,372 - transformer_training - INFO - Main loop iteration: 2193
2025-03-22 07:16:25,633 - transformer_training - INFO - Main loop iteration: 2194
2025-03-22 07:16:25,892 - transformer_training - INFO - Main loop iteration: 2195
2025-03-22 07:16:26,170 - transformer_training - INFO - Main loop iteration: 2196
2025-03-22 07:16:26,420 - transformer_training - INFO - Main loop iteration: 2197
2025-03-22 07:16:26,682 - transformer_training - INFO - Main loop iteration: 2198
2025-03-22 07:16:26,941 - transformer_training - INFO - Main loop iteration: 2199
2025-03-22 07:16:27,219 - transformer_training - INFO - Main loop iteration: 2200
Iter 2200: loss 5.2510, lr 0.000500, 130729.52 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 2200: train loss 5.1241, val loss 5.0734
New best model saved with val loss: 5.0734
2025-03-22 07:16:47,340 - transformer_training - INFO - Main loop iteration: 2201
2025-03-22 07:16:47,593 - transformer_training - INFO - Main loop iteration: 2202
2025-03-22 07:16:47,852 - transformer_training - INFO - Main loop iteration: 2203
2025-03-22 07:16:48,130 - transformer_training - INFO - Main loop iteration: 2204
2025-03-22 07:16:48,379 - transformer_training - INFO - Main loop iteration: 2205
2025-03-22 07:16:48,641 - transformer_training - INFO - Main loop iteration: 2206
2025-03-22 07:16:48,900 - transformer_training - INFO - Main loop iteration: 2207
2025-03-22 07:16:49,179 - transformer_training - INFO - Main loop iteration: 2208
2025-03-22 07:16:49,431 - transformer_training - INFO - Main loop iteration: 2209
2025-03-22 07:16:49,693 - transformer_training - INFO - Main loop iteration: 2210
Iter 2210: loss 5.0651, lr 0.000500, 127171.86 tokens/sec
2025-03-22 07:16:49,953 - transformer_training - INFO - Main loop iteration: 2211
2025-03-22 07:16:50,247 - transformer_training - INFO - Main loop iteration: 2212
2025-03-22 07:16:50,496 - transformer_training - INFO - Main loop iteration: 2213
2025-03-22 07:16:50,757 - transformer_training - INFO - Main loop iteration: 2214
2025-03-22 07:16:51,017 - transformer_training - INFO - Main loop iteration: 2215
2025-03-22 07:16:51,296 - transformer_training - INFO - Main loop iteration: 2216
2025-03-22 07:16:51,545 - transformer_training - INFO - Main loop iteration: 2217
2025-03-22 07:16:51,808 - transformer_training - INFO - Main loop iteration: 2218
2025-03-22 07:16:52,068 - transformer_training - INFO - Main loop iteration: 2219
2025-03-22 07:16:52,345 - transformer_training - INFO - Main loop iteration: 2220
Iter 2220: loss 5.2123, lr 0.000500, 131064.75 tokens/sec
2025-03-22 07:16:52,596 - transformer_training - INFO - Main loop iteration: 2221
2025-03-22 07:16:52,857 - transformer_training - INFO - Main loop iteration: 2222
2025-03-22 07:16:53,117 - transformer_training - INFO - Main loop iteration: 2223
2025-03-22 07:16:53,394 - transformer_training - INFO - Main loop iteration: 2224
2025-03-22 07:16:53,645 - transformer_training - INFO - Main loop iteration: 2225
2025-03-22 07:16:53,906 - transformer_training - INFO - Main loop iteration: 2226
2025-03-22 07:16:54,165 - transformer_training - INFO - Main loop iteration: 2227
2025-03-22 07:16:54,443 - transformer_training - INFO - Main loop iteration: 2228
2025-03-22 07:16:54,694 - transformer_training - INFO - Main loop iteration: 2229
2025-03-22 07:16:54,956 - transformer_training - INFO - Main loop iteration: 2230
Iter 2230: loss 5.2538, lr 0.000500, 126490.68 tokens/sec
2025-03-22 07:16:55,216 - transformer_training - INFO - Main loop iteration: 2231
2025-03-22 07:16:55,493 - transformer_training - INFO - Main loop iteration: 2232
2025-03-22 07:16:55,744 - transformer_training - INFO - Main loop iteration: 2233
2025-03-22 07:16:56,005 - transformer_training - INFO - Main loop iteration: 2234
2025-03-22 07:16:56,265 - transformer_training - INFO - Main loop iteration: 2235
2025-03-22 07:16:56,543 - transformer_training - INFO - Main loop iteration: 2236
2025-03-22 07:16:56,794 - transformer_training - INFO - Main loop iteration: 2237
2025-03-22 07:16:57,055 - transformer_training - INFO - Main loop iteration: 2238
2025-03-22 07:16:57,315 - transformer_training - INFO - Main loop iteration: 2239
2025-03-22 07:16:57,593 - transformer_training - INFO - Main loop iteration: 2240
Iter 2240: loss 5.1924, lr 0.000500, 129479.83 tokens/sec
2025-03-22 07:16:57,846 - transformer_training - INFO - Main loop iteration: 2241
2025-03-22 07:16:58,107 - transformer_training - INFO - Main loop iteration: 2242
2025-03-22 07:16:58,367 - transformer_training - INFO - Main loop iteration: 2243
2025-03-22 07:16:58,644 - transformer_training - INFO - Main loop iteration: 2244
2025-03-22 07:16:58,897 - transformer_training - INFO - Main loop iteration: 2245
2025-03-22 07:16:59,159 - transformer_training - INFO - Main loop iteration: 2246
2025-03-22 07:16:59,418 - transformer_training - INFO - Main loop iteration: 2247
2025-03-22 07:16:59,700 - transformer_training - INFO - Main loop iteration: 2248
2025-03-22 07:16:59,951 - transformer_training - INFO - Main loop iteration: 2249
2025-03-22 07:17:00,212 - transformer_training - INFO - Main loop iteration: 2250
Iter 2250: loss 5.2587, lr 0.000500, 126641.62 tokens/sec
2025-03-22 07:17:00,472 - transformer_training - INFO - Main loop iteration: 2251
2025-03-22 07:17:00,750 - transformer_training - INFO - Main loop iteration: 2252
2025-03-22 07:17:01,000 - transformer_training - INFO - Main loop iteration: 2253
2025-03-22 07:17:01,261 - transformer_training - INFO - Main loop iteration: 2254
2025-03-22 07:17:01,521 - transformer_training - INFO - Main loop iteration: 2255
2025-03-22 07:17:01,799 - transformer_training - INFO - Main loop iteration: 2256
2025-03-22 07:17:02,050 - transformer_training - INFO - Main loop iteration: 2257
2025-03-22 07:17:02,312 - transformer_training - INFO - Main loop iteration: 2258
2025-03-22 07:17:02,571 - transformer_training - INFO - Main loop iteration: 2259
2025-03-22 07:17:02,856 - transformer_training - INFO - Main loop iteration: 2260
Iter 2260: loss 5.2564, lr 0.000500, 130936.14 tokens/sec
2025-03-22 07:17:03,107 - transformer_training - INFO - Main loop iteration: 2261
2025-03-22 07:17:03,369 - transformer_training - INFO - Main loop iteration: 2262
2025-03-22 07:17:03,628 - transformer_training - INFO - Main loop iteration: 2263
2025-03-22 07:17:03,906 - transformer_training - INFO - Main loop iteration: 2264
2025-03-22 07:17:04,157 - transformer_training - INFO - Main loop iteration: 2265
2025-03-22 07:17:04,419 - transformer_training - INFO - Main loop iteration: 2266
2025-03-22 07:17:04,679 - transformer_training - INFO - Main loop iteration: 2267
2025-03-22 07:17:04,956 - transformer_training - INFO - Main loop iteration: 2268
2025-03-22 07:17:05,207 - transformer_training - INFO - Main loop iteration: 2269
2025-03-22 07:17:05,468 - transformer_training - INFO - Main loop iteration: 2270
Iter 2270: loss 5.1701, lr 0.000500, 126414.94 tokens/sec
2025-03-22 07:17:05,728 - transformer_training - INFO - Main loop iteration: 2271
2025-03-22 07:17:06,012 - transformer_training - INFO - Main loop iteration: 2272
2025-03-22 07:17:06,266 - transformer_training - INFO - Main loop iteration: 2273
2025-03-22 07:17:06,528 - transformer_training - INFO - Main loop iteration: 2274
2025-03-22 07:17:06,788 - transformer_training - INFO - Main loop iteration: 2275
2025-03-22 07:17:07,066 - transformer_training - INFO - Main loop iteration: 2276
2025-03-22 07:17:07,317 - transformer_training - INFO - Main loop iteration: 2277
2025-03-22 07:17:07,578 - transformer_training - INFO - Main loop iteration: 2278
2025-03-22 07:17:07,838 - transformer_training - INFO - Main loop iteration: 2279
2025-03-22 07:17:08,115 - transformer_training - INFO - Main loop iteration: 2280
Iter 2280: loss 5.1164, lr 0.000500, 131035.14 tokens/sec
2025-03-22 07:17:08,366 - transformer_training - INFO - Main loop iteration: 2281
2025-03-22 07:17:08,627 - transformer_training - INFO - Main loop iteration: 2282
2025-03-22 07:17:08,887 - transformer_training - INFO - Main loop iteration: 2283
2025-03-22 07:17:09,165 - transformer_training - INFO - Main loop iteration: 2284
2025-03-22 07:17:09,416 - transformer_training - INFO - Main loop iteration: 2285
2025-03-22 07:17:09,677 - transformer_training - INFO - Main loop iteration: 2286
2025-03-22 07:17:09,936 - transformer_training - INFO - Main loop iteration: 2287
2025-03-22 07:17:10,214 - transformer_training - INFO - Main loop iteration: 2288
2025-03-22 07:17:10,465 - transformer_training - INFO - Main loop iteration: 2289
2025-03-22 07:17:10,726 - transformer_training - INFO - Main loop iteration: 2290
Iter 2290: loss 5.1371, lr 0.000500, 126564.41 tokens/sec
2025-03-22 07:17:10,986 - transformer_training - INFO - Main loop iteration: 2291
2025-03-22 07:17:11,267 - transformer_training - INFO - Main loop iteration: 2292
2025-03-22 07:17:11,518 - transformer_training - INFO - Main loop iteration: 2293
2025-03-22 07:17:11,779 - transformer_training - INFO - Main loop iteration: 2294
2025-03-22 07:17:12,039 - transformer_training - INFO - Main loop iteration: 2295
2025-03-22 07:17:12,321 - transformer_training - INFO - Main loop iteration: 2296
2025-03-22 07:17:12,572 - transformer_training - INFO - Main loop iteration: 2297
2025-03-22 07:17:12,833 - transformer_training - INFO - Main loop iteration: 2298
2025-03-22 07:17:13,093 - transformer_training - INFO - Main loop iteration: 2299
2025-03-22 07:17:13,371 - transformer_training - INFO - Main loop iteration: 2300
Iter 2300: loss 5.2344, lr 0.000500, 129570.40 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 2300: train loss 5.0367, val loss 4.9897
New best model saved with val loss: 4.9897
2025-03-22 07:17:33,370 - transformer_training - INFO - Main loop iteration: 2301
2025-03-22 07:17:33,623 - transformer_training - INFO - Main loop iteration: 2302
2025-03-22 07:17:33,885 - transformer_training - INFO - Main loop iteration: 2303
2025-03-22 07:17:34,161 - transformer_training - INFO - Main loop iteration: 2304
2025-03-22 07:17:34,411 - transformer_training - INFO - Main loop iteration: 2305
2025-03-22 07:17:34,672 - transformer_training - INFO - Main loop iteration: 2306
2025-03-22 07:17:34,931 - transformer_training - INFO - Main loop iteration: 2307
2025-03-22 07:17:35,210 - transformer_training - INFO - Main loop iteration: 2308
2025-03-22 07:17:35,462 - transformer_training - INFO - Main loop iteration: 2309
2025-03-22 07:17:35,723 - transformer_training - INFO - Main loop iteration: 2310
Iter 2310: loss 5.1352, lr 0.000500, 125494.97 tokens/sec
2025-03-22 07:17:35,986 - transformer_training - INFO - Main loop iteration: 2311
2025-03-22 07:17:36,269 - transformer_training - INFO - Main loop iteration: 2312
2025-03-22 07:17:36,519 - transformer_training - INFO - Main loop iteration: 2313
2025-03-22 07:17:36,780 - transformer_training - INFO - Main loop iteration: 2314
2025-03-22 07:17:37,040 - transformer_training - INFO - Main loop iteration: 2315
2025-03-22 07:17:37,318 - transformer_training - INFO - Main loop iteration: 2316
2025-03-22 07:17:37,568 - transformer_training - INFO - Main loop iteration: 2317
2025-03-22 07:17:37,828 - transformer_training - INFO - Main loop iteration: 2318
2025-03-22 07:17:38,089 - transformer_training - INFO - Main loop iteration: 2319
2025-03-22 07:17:38,367 - transformer_training - INFO - Main loop iteration: 2320
Iter 2320: loss 5.1436, lr 0.000500, 130764.97 tokens/sec
2025-03-22 07:17:38,619 - transformer_training - INFO - Main loop iteration: 2321
2025-03-22 07:17:38,880 - transformer_training - INFO - Main loop iteration: 2322
2025-03-22 07:17:39,139 - transformer_training - INFO - Main loop iteration: 2323
2025-03-22 07:17:39,417 - transformer_training - INFO - Main loop iteration: 2324
2025-03-22 07:17:39,666 - transformer_training - INFO - Main loop iteration: 2325
2025-03-22 07:17:39,926 - transformer_training - INFO - Main loop iteration: 2326
2025-03-22 07:17:40,186 - transformer_training - INFO - Main loop iteration: 2327
2025-03-22 07:17:40,464 - transformer_training - INFO - Main loop iteration: 2328
2025-03-22 07:17:40,713 - transformer_training - INFO - Main loop iteration: 2329
2025-03-22 07:17:40,973 - transformer_training - INFO - Main loop iteration: 2330
Iter 2330: loss 5.1022, lr 0.000500, 127026.35 tokens/sec
2025-03-22 07:17:41,233 - transformer_training - INFO - Main loop iteration: 2331
2025-03-22 07:17:41,510 - transformer_training - INFO - Main loop iteration: 2332
2025-03-22 07:17:41,759 - transformer_training - INFO - Main loop iteration: 2333
2025-03-22 07:17:42,020 - transformer_training - INFO - Main loop iteration: 2334
2025-03-22 07:17:42,282 - transformer_training - INFO - Main loop iteration: 2335
2025-03-22 07:17:42,560 - transformer_training - INFO - Main loop iteration: 2336
2025-03-22 07:17:42,813 - transformer_training - INFO - Main loop iteration: 2337
2025-03-22 07:17:43,074 - transformer_training - INFO - Main loop iteration: 2338
2025-03-22 07:17:43,337 - transformer_training - INFO - Main loop iteration: 2339
2025-03-22 07:17:43,616 - transformer_training - INFO - Main loop iteration: 2340
Iter 2340: loss 5.0764, lr 0.000500, 131673.75 tokens/sec
2025-03-22 07:17:43,866 - transformer_training - INFO - Main loop iteration: 2341
2025-03-22 07:17:44,127 - transformer_training - INFO - Main loop iteration: 2342
2025-03-22 07:17:44,387 - transformer_training - INFO - Main loop iteration: 2343
2025-03-22 07:17:44,670 - transformer_training - INFO - Main loop iteration: 2344
2025-03-22 07:17:44,919 - transformer_training - INFO - Main loop iteration: 2345
2025-03-22 07:17:45,179 - transformer_training - INFO - Main loop iteration: 2346
2025-03-22 07:17:45,437 - transformer_training - INFO - Main loop iteration: 2347
2025-03-22 07:17:45,715 - transformer_training - INFO - Main loop iteration: 2348
2025-03-22 07:17:45,964 - transformer_training - INFO - Main loop iteration: 2349
2025-03-22 07:17:46,224 - transformer_training - INFO - Main loop iteration: 2350
Iter 2350: loss 5.1710, lr 0.000500, 127529.88 tokens/sec
2025-03-22 07:17:46,483 - transformer_training - INFO - Main loop iteration: 2351
2025-03-22 07:17:46,760 - transformer_training - INFO - Main loop iteration: 2352
2025-03-22 07:17:47,009 - transformer_training - INFO - Main loop iteration: 2353
2025-03-22 07:17:47,269 - transformer_training - INFO - Main loop iteration: 2354
2025-03-22 07:17:47,527 - transformer_training - INFO - Main loop iteration: 2355
2025-03-22 07:17:47,806 - transformer_training - INFO - Main loop iteration: 2356
2025-03-22 07:17:48,055 - transformer_training - INFO - Main loop iteration: 2357
2025-03-22 07:17:48,314 - transformer_training - INFO - Main loop iteration: 2358
2025-03-22 07:17:48,573 - transformer_training - INFO - Main loop iteration: 2359
2025-03-22 07:17:48,855 - transformer_training - INFO - Main loop iteration: 2360
Iter 2360: loss 5.0876, lr 0.000500, 130632.48 tokens/sec
2025-03-22 07:17:49,107 - transformer_training - INFO - Main loop iteration: 2361
2025-03-22 07:17:49,368 - transformer_training - INFO - Main loop iteration: 2362
2025-03-22 07:17:49,626 - transformer_training - INFO - Main loop iteration: 2363
2025-03-22 07:17:49,906 - transformer_training - INFO - Main loop iteration: 2364
2025-03-22 07:17:50,155 - transformer_training - INFO - Main loop iteration: 2365
2025-03-22 07:17:50,417 - transformer_training - INFO - Main loop iteration: 2366
2025-03-22 07:17:50,676 - transformer_training - INFO - Main loop iteration: 2367
2025-03-22 07:17:50,956 - transformer_training - INFO - Main loop iteration: 2368
2025-03-22 07:17:51,205 - transformer_training - INFO - Main loop iteration: 2369
2025-03-22 07:17:51,467 - transformer_training - INFO - Main loop iteration: 2370
Iter 2370: loss 5.2128, lr 0.000499, 127250.87 tokens/sec
2025-03-22 07:17:51,727 - transformer_training - INFO - Main loop iteration: 2371
2025-03-22 07:17:52,005 - transformer_training - INFO - Main loop iteration: 2372
2025-03-22 07:17:52,257 - transformer_training - INFO - Main loop iteration: 2373
2025-03-22 07:17:52,518 - transformer_training - INFO - Main loop iteration: 2374
2025-03-22 07:17:52,776 - transformer_training - INFO - Main loop iteration: 2375
2025-03-22 07:17:53,061 - transformer_training - INFO - Main loop iteration: 2376
2025-03-22 07:17:53,312 - transformer_training - INFO - Main loop iteration: 2377
2025-03-22 07:17:53,572 - transformer_training - INFO - Main loop iteration: 2378
2025-03-22 07:17:53,833 - transformer_training - INFO - Main loop iteration: 2379
2025-03-22 07:17:54,112 - transformer_training - INFO - Main loop iteration: 2380
Iter 2380: loss 5.1319, lr 0.000499, 128861.06 tokens/sec
2025-03-22 07:17:54,367 - transformer_training - INFO - Main loop iteration: 2381
2025-03-22 07:17:54,627 - transformer_training - INFO - Main loop iteration: 2382
2025-03-22 07:17:54,886 - transformer_training - INFO - Main loop iteration: 2383
2025-03-22 07:17:55,165 - transformer_training - INFO - Main loop iteration: 2384
2025-03-22 07:17:55,415 - transformer_training - INFO - Main loop iteration: 2385
2025-03-22 07:17:55,676 - transformer_training - INFO - Main loop iteration: 2386
2025-03-22 07:17:55,935 - transformer_training - INFO - Main loop iteration: 2387
2025-03-22 07:17:56,214 - transformer_training - INFO - Main loop iteration: 2388
2025-03-22 07:17:56,467 - transformer_training - INFO - Main loop iteration: 2389
2025-03-22 07:17:56,728 - transformer_training - INFO - Main loop iteration: 2390
Iter 2390: loss 5.0358, lr 0.000499, 126404.13 tokens/sec
2025-03-22 07:17:56,990 - transformer_training - INFO - Main loop iteration: 2391
2025-03-22 07:17:57,274 - transformer_training - INFO - Main loop iteration: 2392
2025-03-22 07:17:57,531 - transformer_training - INFO - Main loop iteration: 2393
2025-03-22 07:17:57,792 - transformer_training - INFO - Main loop iteration: 2394
2025-03-22 07:17:58,053 - transformer_training - INFO - Main loop iteration: 2395
2025-03-22 07:17:58,335 - transformer_training - INFO - Main loop iteration: 2396
2025-03-22 07:17:58,587 - transformer_training - INFO - Main loop iteration: 2397
2025-03-22 07:17:58,846 - transformer_training - INFO - Main loop iteration: 2398
2025-03-22 07:17:59,106 - transformer_training - INFO - Main loop iteration: 2399
2025-03-22 07:17:59,385 - transformer_training - INFO - Main loop iteration: 2400
Iter 2400: loss 4.9620, lr 0.000499, 130353.09 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 2400: train loss 4.9650, val loss 4.9189
New best model saved with val loss: 4.9189
2025-03-22 07:18:20,098 - transformer_training - INFO - Main loop iteration: 2401
2025-03-22 07:18:20,389 - transformer_training - INFO - Main loop iteration: 2402
2025-03-22 07:18:20,661 - transformer_training - INFO - Main loop iteration: 2403
2025-03-22 07:18:20,940 - transformer_training - INFO - Main loop iteration: 2404
2025-03-22 07:18:21,190 - transformer_training - INFO - Main loop iteration: 2405
2025-03-22 07:18:21,452 - transformer_training - INFO - Main loop iteration: 2406
2025-03-22 07:18:21,712 - transformer_training - INFO - Main loop iteration: 2407
2025-03-22 07:18:21,990 - transformer_training - INFO - Main loop iteration: 2408
2025-03-22 07:18:22,240 - transformer_training - INFO - Main loop iteration: 2409
2025-03-22 07:18:22,502 - transformer_training - INFO - Main loop iteration: 2410
Iter 2410: loss 5.0337, lr 0.000499, 126627.38 tokens/sec
2025-03-22 07:18:22,761 - transformer_training - INFO - Main loop iteration: 2411
2025-03-22 07:18:23,039 - transformer_training - INFO - Main loop iteration: 2412
2025-03-22 07:18:23,290 - transformer_training - INFO - Main loop iteration: 2413
2025-03-22 07:18:23,551 - transformer_training - INFO - Main loop iteration: 2414
2025-03-22 07:18:23,811 - transformer_training - INFO - Main loop iteration: 2415
2025-03-22 07:18:24,088 - transformer_training - INFO - Main loop iteration: 2416
2025-03-22 07:18:24,339 - transformer_training - INFO - Main loop iteration: 2417
2025-03-22 07:18:24,600 - transformer_training - INFO - Main loop iteration: 2418
2025-03-22 07:18:24,860 - transformer_training - INFO - Main loop iteration: 2419
2025-03-22 07:18:25,137 - transformer_training - INFO - Main loop iteration: 2420
Iter 2420: loss 5.0181, lr 0.000499, 131322.10 tokens/sec
2025-03-22 07:18:25,388 - transformer_training - INFO - Main loop iteration: 2421
2025-03-22 07:18:25,650 - transformer_training - INFO - Main loop iteration: 2422
2025-03-22 07:18:25,909 - transformer_training - INFO - Main loop iteration: 2423
2025-03-22 07:18:26,188 - transformer_training - INFO - Main loop iteration: 2424
2025-03-22 07:18:26,439 - transformer_training - INFO - Main loop iteration: 2425
2025-03-22 07:18:26,700 - transformer_training - INFO - Main loop iteration: 2426
2025-03-22 07:18:26,970 - transformer_training - INFO - Main loop iteration: 2427
2025-03-22 07:18:27,247 - transformer_training - INFO - Main loop iteration: 2428
2025-03-22 07:18:27,498 - transformer_training - INFO - Main loop iteration: 2429
2025-03-22 07:18:27,759 - transformer_training - INFO - Main loop iteration: 2430
Iter 2430: loss 5.0313, lr 0.000499, 126855.06 tokens/sec
2025-03-22 07:18:28,019 - transformer_training - INFO - Main loop iteration: 2431
2025-03-22 07:18:28,304 - transformer_training - INFO - Main loop iteration: 2432
2025-03-22 07:18:28,555 - transformer_training - INFO - Main loop iteration: 2433
2025-03-22 07:18:28,817 - transformer_training - INFO - Main loop iteration: 2434
2025-03-22 07:18:29,078 - transformer_training - INFO - Main loop iteration: 2435
2025-03-22 07:18:29,357 - transformer_training - INFO - Main loop iteration: 2436
2025-03-22 07:18:29,607 - transformer_training - INFO - Main loop iteration: 2437
2025-03-22 07:18:29,869 - transformer_training - INFO - Main loop iteration: 2438
2025-03-22 07:18:30,130 - transformer_training - INFO - Main loop iteration: 2439
2025-03-22 07:18:30,408 - transformer_training - INFO - Main loop iteration: 2440
Iter 2440: loss 4.9372, lr 0.000499, 130379.06 tokens/sec
2025-03-22 07:18:30,660 - transformer_training - INFO - Main loop iteration: 2441
2025-03-22 07:18:30,921 - transformer_training - INFO - Main loop iteration: 2442
2025-03-22 07:18:31,180 - transformer_training - INFO - Main loop iteration: 2443
2025-03-22 07:18:31,458 - transformer_training - INFO - Main loop iteration: 2444
2025-03-22 07:18:31,711 - transformer_training - INFO - Main loop iteration: 2445
2025-03-22 07:18:31,972 - transformer_training - INFO - Main loop iteration: 2446
2025-03-22 07:18:32,233 - transformer_training - INFO - Main loop iteration: 2447
2025-03-22 07:18:32,511 - transformer_training - INFO - Main loop iteration: 2448
2025-03-22 07:18:32,762 - transformer_training - INFO - Main loop iteration: 2449
2025-03-22 07:18:33,023 - transformer_training - INFO - Main loop iteration: 2450
Iter 2450: loss 5.0234, lr 0.000499, 126309.22 tokens/sec
2025-03-22 07:18:33,284 - transformer_training - INFO - Main loop iteration: 2451
2025-03-22 07:18:33,562 - transformer_training - INFO - Main loop iteration: 2452
2025-03-22 07:18:33,816 - transformer_training - INFO - Main loop iteration: 2453
2025-03-22 07:18:34,077 - transformer_training - INFO - Main loop iteration: 2454
2025-03-22 07:18:34,337 - transformer_training - INFO - Main loop iteration: 2455
2025-03-22 07:18:34,618 - transformer_training - INFO - Main loop iteration: 2456
2025-03-22 07:18:34,869 - transformer_training - INFO - Main loop iteration: 2457
2025-03-22 07:18:35,130 - transformer_training - INFO - Main loop iteration: 2458
2025-03-22 07:18:35,390 - transformer_training - INFO - Main loop iteration: 2459
2025-03-22 07:18:35,668 - transformer_training - INFO - Main loop iteration: 2460
Iter 2460: loss 4.9574, lr 0.000499, 130865.95 tokens/sec
2025-03-22 07:18:35,919 - transformer_training - INFO - Main loop iteration: 2461
2025-03-22 07:18:36,181 - transformer_training - INFO - Main loop iteration: 2462
2025-03-22 07:18:36,441 - transformer_training - INFO - Main loop iteration: 2463
2025-03-22 07:18:36,719 - transformer_training - INFO - Main loop iteration: 2464
2025-03-22 07:18:36,970 - transformer_training - INFO - Main loop iteration: 2465
2025-03-22 07:18:37,231 - transformer_training - INFO - Main loop iteration: 2466
2025-03-22 07:18:37,492 - transformer_training - INFO - Main loop iteration: 2467
2025-03-22 07:18:37,770 - transformer_training - INFO - Main loop iteration: 2468
2025-03-22 07:18:38,021 - transformer_training - INFO - Main loop iteration: 2469
2025-03-22 07:18:38,283 - transformer_training - INFO - Main loop iteration: 2470
Iter 2470: loss 5.0107, lr 0.000499, 126454.83 tokens/sec
2025-03-22 07:18:38,543 - transformer_training - INFO - Main loop iteration: 2471
2025-03-22 07:18:38,821 - transformer_training - INFO - Main loop iteration: 2472
2025-03-22 07:18:39,076 - transformer_training - INFO - Main loop iteration: 2473
2025-03-22 07:18:39,336 - transformer_training - INFO - Main loop iteration: 2474
2025-03-22 07:18:39,596 - transformer_training - INFO - Main loop iteration: 2475
2025-03-22 07:18:39,875 - transformer_training - INFO - Main loop iteration: 2476
2025-03-22 07:18:40,125 - transformer_training - INFO - Main loop iteration: 2477
2025-03-22 07:18:40,387 - transformer_training - INFO - Main loop iteration: 2478
2025-03-22 07:18:40,651 - transformer_training - INFO - Main loop iteration: 2479
2025-03-22 07:18:40,930 - transformer_training - INFO - Main loop iteration: 2480
Iter 2480: loss 4.9777, lr 0.000499, 130623.79 tokens/sec
2025-03-22 07:18:41,182 - transformer_training - INFO - Main loop iteration: 2481
2025-03-22 07:18:41,443 - transformer_training - INFO - Main loop iteration: 2482
2025-03-22 07:18:41,703 - transformer_training - INFO - Main loop iteration: 2483
2025-03-22 07:18:41,982 - transformer_training - INFO - Main loop iteration: 2484
2025-03-22 07:18:42,233 - transformer_training - INFO - Main loop iteration: 2485
2025-03-22 07:18:42,494 - transformer_training - INFO - Main loop iteration: 2486
2025-03-22 07:18:42,754 - transformer_training - INFO - Main loop iteration: 2487
2025-03-22 07:18:43,033 - transformer_training - INFO - Main loop iteration: 2488
2025-03-22 07:18:43,285 - transformer_training - INFO - Main loop iteration: 2489
2025-03-22 07:18:43,546 - transformer_training - INFO - Main loop iteration: 2490
Iter 2490: loss 5.0958, lr 0.000499, 126222.22 tokens/sec
2025-03-22 07:18:43,807 - transformer_training - INFO - Main loop iteration: 2491
2025-03-22 07:18:44,084 - transformer_training - INFO - Main loop iteration: 2492
2025-03-22 07:18:44,335 - transformer_training - INFO - Main loop iteration: 2493
2025-03-22 07:18:44,597 - transformer_training - INFO - Main loop iteration: 2494
2025-03-22 07:18:44,858 - transformer_training - INFO - Main loop iteration: 2495
2025-03-22 07:18:45,137 - transformer_training - INFO - Main loop iteration: 2496
2025-03-22 07:18:45,389 - transformer_training - INFO - Main loop iteration: 2497
2025-03-22 07:18:45,650 - transformer_training - INFO - Main loop iteration: 2498
2025-03-22 07:18:45,910 - transformer_training - INFO - Main loop iteration: 2499
2025-03-22 07:18:46,188 - transformer_training - INFO - Main loop iteration: 2500
Iter 2500: loss 5.0715, lr 0.000499, 130989.93 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
terminate called without an active exception
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x76415c7b0280>
Traceback (most recent call last):
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1618, in __del__
    self._shutdown_workers()
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1582, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.10/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.10/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/usr/lib/python3.10/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 295903) is killed by signal: Aborted. 
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 2500: train loss 4.8945, val loss 4.8337
New best model saved with val loss: 4.8337
2025-03-22 07:19:08,278 - transformer_training - INFO - Main loop iteration: 2501
2025-03-22 07:19:08,558 - transformer_training - INFO - Main loop iteration: 2502
2025-03-22 07:19:08,820 - transformer_training - INFO - Main loop iteration: 2503
2025-03-22 07:19:09,097 - transformer_training - INFO - Main loop iteration: 2504
2025-03-22 07:19:09,348 - transformer_training - INFO - Main loop iteration: 2505
2025-03-22 07:19:09,610 - transformer_training - INFO - Main loop iteration: 2506
2025-03-22 07:19:09,870 - transformer_training - INFO - Main loop iteration: 2507
2025-03-22 07:19:10,149 - transformer_training - INFO - Main loop iteration: 2508
2025-03-22 07:19:10,403 - transformer_training - INFO - Main loop iteration: 2509
2025-03-22 07:19:10,665 - transformer_training - INFO - Main loop iteration: 2510
Iter 2510: loss 5.0128, lr 0.000499, 126388.20 tokens/sec
2025-03-22 07:19:10,926 - transformer_training - INFO - Main loop iteration: 2511
2025-03-22 07:19:11,210 - transformer_training - INFO - Main loop iteration: 2512
2025-03-22 07:19:11,459 - transformer_training - INFO - Main loop iteration: 2513
2025-03-22 07:19:11,721 - transformer_training - INFO - Main loop iteration: 2514
2025-03-22 07:19:11,987 - transformer_training - INFO - Main loop iteration: 2515
2025-03-22 07:19:12,267 - transformer_training - INFO - Main loop iteration: 2516
2025-03-22 07:19:12,516 - transformer_training - INFO - Main loop iteration: 2517
2025-03-22 07:19:12,781 - transformer_training - INFO - Main loop iteration: 2518
2025-03-22 07:19:13,046 - transformer_training - INFO - Main loop iteration: 2519
2025-03-22 07:19:13,330 - transformer_training - INFO - Main loop iteration: 2520
Iter 2520: loss 5.0046, lr 0.000499, 131324.86 tokens/sec
2025-03-22 07:19:13,581 - transformer_training - INFO - Main loop iteration: 2521
2025-03-22 07:19:13,842 - transformer_training - INFO - Main loop iteration: 2522
2025-03-22 07:19:14,102 - transformer_training - INFO - Main loop iteration: 2523
2025-03-22 07:19:14,382 - transformer_training - INFO - Main loop iteration: 2524
2025-03-22 07:19:14,635 - transformer_training - INFO - Main loop iteration: 2525
2025-03-22 07:19:14,896 - transformer_training - INFO - Main loop iteration: 2526
2025-03-22 07:19:15,157 - transformer_training - INFO - Main loop iteration: 2527
2025-03-22 07:19:15,437 - transformer_training - INFO - Main loop iteration: 2528
2025-03-22 07:19:15,686 - transformer_training - INFO - Main loop iteration: 2529
2025-03-22 07:19:15,948 - transformer_training - INFO - Main loop iteration: 2530
Iter 2530: loss 5.0171, lr 0.000499, 127087.55 tokens/sec
2025-03-22 07:19:16,208 - transformer_training - INFO - Main loop iteration: 2531
2025-03-22 07:19:16,488 - transformer_training - INFO - Main loop iteration: 2532
2025-03-22 07:19:16,737 - transformer_training - INFO - Main loop iteration: 2533
2025-03-22 07:19:17,000 - transformer_training - INFO - Main loop iteration: 2534
2025-03-22 07:19:17,260 - transformer_training - INFO - Main loop iteration: 2535
2025-03-22 07:19:17,544 - transformer_training - INFO - Main loop iteration: 2536
2025-03-22 07:19:17,795 - transformer_training - INFO - Main loop iteration: 2537
2025-03-22 07:19:18,058 - transformer_training - INFO - Main loop iteration: 2538
2025-03-22 07:19:18,319 - transformer_training - INFO - Main loop iteration: 2539
2025-03-22 07:19:18,599 - transformer_training - INFO - Main loop iteration: 2540
Iter 2540: loss 4.9975, lr 0.000499, 131290.36 tokens/sec
2025-03-22 07:19:18,849 - transformer_training - INFO - Main loop iteration: 2541
2025-03-22 07:19:19,111 - transformer_training - INFO - Main loop iteration: 2542
2025-03-22 07:19:19,372 - transformer_training - INFO - Main loop iteration: 2543
2025-03-22 07:19:19,651 - transformer_training - INFO - Main loop iteration: 2544
2025-03-22 07:19:19,902 - transformer_training - INFO - Main loop iteration: 2545
2025-03-22 07:19:20,163 - transformer_training - INFO - Main loop iteration: 2546
2025-03-22 07:19:20,424 - transformer_training - INFO - Main loop iteration: 2547
2025-03-22 07:19:20,703 - transformer_training - INFO - Main loop iteration: 2548
2025-03-22 07:19:20,953 - transformer_training - INFO - Main loop iteration: 2549
2025-03-22 07:19:21,215 - transformer_training - INFO - Main loop iteration: 2550
Iter 2550: loss 4.9223, lr 0.000499, 126863.84 tokens/sec
2025-03-22 07:19:21,476 - transformer_training - INFO - Main loop iteration: 2551
2025-03-22 07:19:21,756 - transformer_training - INFO - Main loop iteration: 2552
2025-03-22 07:19:22,006 - transformer_training - INFO - Main loop iteration: 2553
2025-03-22 07:19:22,269 - transformer_training - INFO - Main loop iteration: 2554
2025-03-22 07:19:22,528 - transformer_training - INFO - Main loop iteration: 2555
2025-03-22 07:19:22,808 - transformer_training - INFO - Main loop iteration: 2556
2025-03-22 07:19:23,059 - transformer_training - INFO - Main loop iteration: 2557
2025-03-22 07:19:23,321 - transformer_training - INFO - Main loop iteration: 2558
2025-03-22 07:19:23,581 - transformer_training - INFO - Main loop iteration: 2559
2025-03-22 07:19:23,861 - transformer_training - INFO - Main loop iteration: 2560
Iter 2560: loss 4.9255, lr 0.000499, 131404.59 tokens/sec
2025-03-22 07:19:24,111 - transformer_training - INFO - Main loop iteration: 2561
2025-03-22 07:19:24,372 - transformer_training - INFO - Main loop iteration: 2562
2025-03-22 07:19:24,632 - transformer_training - INFO - Main loop iteration: 2563
2025-03-22 07:19:24,911 - transformer_training - INFO - Main loop iteration: 2564
2025-03-22 07:19:25,161 - transformer_training - INFO - Main loop iteration: 2565
2025-03-22 07:19:25,423 - transformer_training - INFO - Main loop iteration: 2566
2025-03-22 07:19:25,683 - transformer_training - INFO - Main loop iteration: 2567
2025-03-22 07:19:26,099 - transformer_training - INFO - Main loop iteration: 2568
2025-03-22 07:19:26,351 - transformer_training - INFO - Main loop iteration: 2569
2025-03-22 07:19:26,613 - transformer_training - INFO - Main loop iteration: 2570
Iter 2570: loss 4.9152, lr 0.000499, 126248.07 tokens/sec
2025-03-22 07:19:26,873 - transformer_training - INFO - Main loop iteration: 2571
2025-03-22 07:19:27,157 - transformer_training - INFO - Main loop iteration: 2572
2025-03-22 07:19:27,408 - transformer_training - INFO - Main loop iteration: 2573
2025-03-22 07:19:27,670 - transformer_training - INFO - Main loop iteration: 2574
2025-03-22 07:19:27,930 - transformer_training - INFO - Main loop iteration: 2575
2025-03-22 07:19:28,208 - transformer_training - INFO - Main loop iteration: 2576
2025-03-22 07:19:28,458 - transformer_training - INFO - Main loop iteration: 2577
2025-03-22 07:19:28,720 - transformer_training - INFO - Main loop iteration: 2578
2025-03-22 07:19:28,980 - transformer_training - INFO - Main loop iteration: 2579
2025-03-22 07:19:29,258 - transformer_training - INFO - Main loop iteration: 2580
Iter 2580: loss 4.8765, lr 0.000499, 130748.67 tokens/sec
2025-03-22 07:19:29,509 - transformer_training - INFO - Main loop iteration: 2581
2025-03-22 07:19:29,774 - transformer_training - INFO - Main loop iteration: 2582
2025-03-22 07:19:30,034 - transformer_training - INFO - Main loop iteration: 2583
2025-03-22 07:19:30,312 - transformer_training - INFO - Main loop iteration: 2584
2025-03-22 07:19:30,563 - transformer_training - INFO - Main loop iteration: 2585
2025-03-22 07:19:30,824 - transformer_training - INFO - Main loop iteration: 2586
2025-03-22 07:19:31,084 - transformer_training - INFO - Main loop iteration: 2587
2025-03-22 07:19:31,362 - transformer_training - INFO - Main loop iteration: 2588
2025-03-22 07:19:31,615 - transformer_training - INFO - Main loop iteration: 2589
2025-03-22 07:19:31,878 - transformer_training - INFO - Main loop iteration: 2590
Iter 2590: loss 4.8839, lr 0.000499, 124077.20 tokens/sec
2025-03-22 07:19:32,143 - transformer_training - INFO - Main loop iteration: 2591
2025-03-22 07:19:32,426 - transformer_training - INFO - Main loop iteration: 2592
2025-03-22 07:19:32,678 - transformer_training - INFO - Main loop iteration: 2593
2025-03-22 07:19:32,940 - transformer_training - INFO - Main loop iteration: 2594
2025-03-22 07:19:33,200 - transformer_training - INFO - Main loop iteration: 2595
2025-03-22 07:19:33,477 - transformer_training - INFO - Main loop iteration: 2596
2025-03-22 07:19:33,729 - transformer_training - INFO - Main loop iteration: 2597
2025-03-22 07:19:33,991 - transformer_training - INFO - Main loop iteration: 2598
2025-03-22 07:19:34,250 - transformer_training - INFO - Main loop iteration: 2599
2025-03-22 07:19:34,528 - transformer_training - INFO - Main loop iteration: 2600
Iter 2600: loss 4.9950, lr 0.000499, 130635.09 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 2600: train loss 4.8413, val loss 4.7809
New best model saved with val loss: 4.7809
2025-03-22 07:19:55,040 - transformer_training - INFO - Main loop iteration: 2601
2025-03-22 07:19:55,296 - transformer_training - INFO - Main loop iteration: 2602
2025-03-22 07:19:55,557 - transformer_training - INFO - Main loop iteration: 2603
2025-03-22 07:19:55,834 - transformer_training - INFO - Main loop iteration: 2604
2025-03-22 07:19:56,085 - transformer_training - INFO - Main loop iteration: 2605
2025-03-22 07:19:56,346 - transformer_training - INFO - Main loop iteration: 2606
2025-03-22 07:19:56,612 - transformer_training - INFO - Main loop iteration: 2607
2025-03-22 07:19:56,890 - transformer_training - INFO - Main loop iteration: 2608
2025-03-22 07:19:57,141 - transformer_training - INFO - Main loop iteration: 2609
2025-03-22 07:19:57,402 - transformer_training - INFO - Main loop iteration: 2610
Iter 2610: loss 4.8869, lr 0.000499, 126056.20 tokens/sec
2025-03-22 07:19:57,663 - transformer_training - INFO - Main loop iteration: 2611
2025-03-22 07:19:57,941 - transformer_training - INFO - Main loop iteration: 2612
2025-03-22 07:19:58,192 - transformer_training - INFO - Main loop iteration: 2613
2025-03-22 07:19:58,453 - transformer_training - INFO - Main loop iteration: 2614
2025-03-22 07:19:58,713 - transformer_training - INFO - Main loop iteration: 2615
2025-03-22 07:19:58,991 - transformer_training - INFO - Main loop iteration: 2616
2025-03-22 07:19:59,242 - transformer_training - INFO - Main loop iteration: 2617
2025-03-22 07:19:59,503 - transformer_training - INFO - Main loop iteration: 2618
2025-03-22 07:19:59,764 - transformer_training - INFO - Main loop iteration: 2619
2025-03-22 07:20:00,048 - transformer_training - INFO - Main loop iteration: 2620
Iter 2620: loss 4.8963, lr 0.000499, 131414.89 tokens/sec
2025-03-22 07:20:00,298 - transformer_training - INFO - Main loop iteration: 2621
2025-03-22 07:20:00,559 - transformer_training - INFO - Main loop iteration: 2622
2025-03-22 07:20:00,819 - transformer_training - INFO - Main loop iteration: 2623
2025-03-22 07:20:01,098 - transformer_training - INFO - Main loop iteration: 2624
2025-03-22 07:20:01,349 - transformer_training - INFO - Main loop iteration: 2625
2025-03-22 07:20:01,610 - transformer_training - INFO - Main loop iteration: 2626
2025-03-22 07:20:01,870 - transformer_training - INFO - Main loop iteration: 2627
2025-03-22 07:20:02,150 - transformer_training - INFO - Main loop iteration: 2628
2025-03-22 07:20:02,399 - transformer_training - INFO - Main loop iteration: 2629
2025-03-22 07:20:02,661 - transformer_training - INFO - Main loop iteration: 2630
Iter 2630: loss 4.9239, lr 0.000499, 127094.13 tokens/sec
2025-03-22 07:20:02,921 - transformer_training - INFO - Main loop iteration: 2631
2025-03-22 07:20:03,200 - transformer_training - INFO - Main loop iteration: 2632
2025-03-22 07:20:03,450 - transformer_training - INFO - Main loop iteration: 2633
2025-03-22 07:20:03,726 - transformer_training - INFO - Main loop iteration: 2634
2025-03-22 07:20:03,992 - transformer_training - INFO - Main loop iteration: 2635
2025-03-22 07:20:04,278 - transformer_training - INFO - Main loop iteration: 2636
2025-03-22 07:20:04,528 - transformer_training - INFO - Main loop iteration: 2637
2025-03-22 07:20:04,789 - transformer_training - INFO - Main loop iteration: 2638
2025-03-22 07:20:05,049 - transformer_training - INFO - Main loop iteration: 2639
2025-03-22 07:20:05,329 - transformer_training - INFO - Main loop iteration: 2640
Iter 2640: loss 4.9177, lr 0.000498, 131485.17 tokens/sec
2025-03-22 07:20:05,579 - transformer_training - INFO - Main loop iteration: 2641
2025-03-22 07:20:05,840 - transformer_training - INFO - Main loop iteration: 2642
2025-03-22 07:20:06,099 - transformer_training - INFO - Main loop iteration: 2643
2025-03-22 07:20:06,379 - transformer_training - INFO - Main loop iteration: 2644
2025-03-22 07:20:06,629 - transformer_training - INFO - Main loop iteration: 2645
2025-03-22 07:20:06,890 - transformer_training - INFO - Main loop iteration: 2646
2025-03-22 07:20:07,150 - transformer_training - INFO - Main loop iteration: 2647
2025-03-22 07:20:07,430 - transformer_training - INFO - Main loop iteration: 2648
2025-03-22 07:20:07,680 - transformer_training - INFO - Main loop iteration: 2649
2025-03-22 07:20:07,941 - transformer_training - INFO - Main loop iteration: 2650
Iter 2650: loss 4.9021, lr 0.000498, 126858.92 tokens/sec
2025-03-22 07:20:08,202 - transformer_training - INFO - Main loop iteration: 2651
2025-03-22 07:20:08,481 - transformer_training - INFO - Main loop iteration: 2652
2025-03-22 07:20:08,733 - transformer_training - INFO - Main loop iteration: 2653
2025-03-22 07:20:08,995 - transformer_training - INFO - Main loop iteration: 2654
2025-03-22 07:20:09,255 - transformer_training - INFO - Main loop iteration: 2655
2025-03-22 07:20:09,535 - transformer_training - INFO - Main loop iteration: 2656
2025-03-22 07:20:09,784 - transformer_training - INFO - Main loop iteration: 2657
2025-03-22 07:20:10,046 - transformer_training - INFO - Main loop iteration: 2658
2025-03-22 07:20:10,306 - transformer_training - INFO - Main loop iteration: 2659
2025-03-22 07:20:10,586 - transformer_training - INFO - Main loop iteration: 2660
Iter 2660: loss 4.8675, lr 0.000498, 131485.05 tokens/sec
2025-03-22 07:20:10,836 - transformer_training - INFO - Main loop iteration: 2661
2025-03-22 07:20:11,098 - transformer_training - INFO - Main loop iteration: 2662
2025-03-22 07:20:11,358 - transformer_training - INFO - Main loop iteration: 2663
2025-03-22 07:20:11,638 - transformer_training - INFO - Main loop iteration: 2664
2025-03-22 07:20:11,895 - transformer_training - INFO - Main loop iteration: 2665
2025-03-22 07:20:12,156 - transformer_training - INFO - Main loop iteration: 2666
2025-03-22 07:20:12,422 - transformer_training - INFO - Main loop iteration: 2667
2025-03-22 07:20:12,704 - transformer_training - INFO - Main loop iteration: 2668
2025-03-22 07:20:12,954 - transformer_training - INFO - Main loop iteration: 2669
2025-03-22 07:20:13,215 - transformer_training - INFO - Main loop iteration: 2670
Iter 2670: loss 4.8112, lr 0.000498, 126908.70 tokens/sec
2025-03-22 07:20:13,476 - transformer_training - INFO - Main loop iteration: 2671
2025-03-22 07:20:13,754 - transformer_training - INFO - Main loop iteration: 2672
2025-03-22 07:20:14,004 - transformer_training - INFO - Main loop iteration: 2673
2025-03-22 07:20:14,266 - transformer_training - INFO - Main loop iteration: 2674
2025-03-22 07:20:14,525 - transformer_training - INFO - Main loop iteration: 2675
2025-03-22 07:20:14,804 - transformer_training - INFO - Main loop iteration: 2676
2025-03-22 07:20:15,054 - transformer_training - INFO - Main loop iteration: 2677
2025-03-22 07:20:15,315 - transformer_training - INFO - Main loop iteration: 2678
2025-03-22 07:20:15,576 - transformer_training - INFO - Main loop iteration: 2679
2025-03-22 07:20:15,854 - transformer_training - INFO - Main loop iteration: 2680
Iter 2680: loss 5.0240, lr 0.000498, 130194.54 tokens/sec
2025-03-22 07:20:16,107 - transformer_training - INFO - Main loop iteration: 2681
2025-03-22 07:20:16,368 - transformer_training - INFO - Main loop iteration: 2682
2025-03-22 07:20:16,628 - transformer_training - INFO - Main loop iteration: 2683
2025-03-22 07:20:16,907 - transformer_training - INFO - Main loop iteration: 2684
2025-03-22 07:20:17,157 - transformer_training - INFO - Main loop iteration: 2685
2025-03-22 07:20:17,418 - transformer_training - INFO - Main loop iteration: 2686
2025-03-22 07:20:17,677 - transformer_training - INFO - Main loop iteration: 2687
2025-03-22 07:20:17,962 - transformer_training - INFO - Main loop iteration: 2688
2025-03-22 07:20:18,211 - transformer_training - INFO - Main loop iteration: 2689
2025-03-22 07:20:18,474 - transformer_training - INFO - Main loop iteration: 2690
Iter 2690: loss 4.9514, lr 0.000498, 126364.73 tokens/sec
2025-03-22 07:20:18,736 - transformer_training - INFO - Main loop iteration: 2691
2025-03-22 07:20:19,019 - transformer_training - INFO - Main loop iteration: 2692
2025-03-22 07:20:19,268 - transformer_training - INFO - Main loop iteration: 2693
2025-03-22 07:20:19,529 - transformer_training - INFO - Main loop iteration: 2694
2025-03-22 07:20:19,789 - transformer_training - INFO - Main loop iteration: 2695
2025-03-22 07:20:20,069 - transformer_training - INFO - Main loop iteration: 2696
2025-03-22 07:20:20,318 - transformer_training - INFO - Main loop iteration: 2697
2025-03-22 07:20:20,580 - transformer_training - INFO - Main loop iteration: 2698
2025-03-22 07:20:20,842 - transformer_training - INFO - Main loop iteration: 2699
2025-03-22 07:20:21,120 - transformer_training - INFO - Main loop iteration: 2700
Iter 2700: loss 4.8091, lr 0.000498, 131553.01 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 2700: train loss 4.7746, val loss 4.7136
New best model saved with val loss: 4.7136
2025-03-22 07:20:41,736 - transformer_training - INFO - Main loop iteration: 2701
2025-03-22 07:20:41,993 - transformer_training - INFO - Main loop iteration: 2702
2025-03-22 07:20:42,255 - transformer_training - INFO - Main loop iteration: 2703
2025-03-22 07:20:42,532 - transformer_training - INFO - Main loop iteration: 2704
2025-03-22 07:20:42,783 - transformer_training - INFO - Main loop iteration: 2705
2025-03-22 07:20:43,045 - transformer_training - INFO - Main loop iteration: 2706
2025-03-22 07:20:43,305 - transformer_training - INFO - Main loop iteration: 2707
2025-03-22 07:20:43,584 - transformer_training - INFO - Main loop iteration: 2708
2025-03-22 07:20:43,835 - transformer_training - INFO - Main loop iteration: 2709
2025-03-22 07:20:44,096 - transformer_training - INFO - Main loop iteration: 2710
Iter 2710: loss 4.8359, lr 0.000498, 126105.36 tokens/sec
2025-03-22 07:20:44,357 - transformer_training - INFO - Main loop iteration: 2711
2025-03-22 07:20:44,635 - transformer_training - INFO - Main loop iteration: 2712
2025-03-22 07:20:44,887 - transformer_training - INFO - Main loop iteration: 2713
2025-03-22 07:20:45,149 - transformer_training - INFO - Main loop iteration: 2714
2025-03-22 07:20:45,410 - transformer_training - INFO - Main loop iteration: 2715
2025-03-22 07:20:45,688 - transformer_training - INFO - Main loop iteration: 2716
2025-03-22 07:20:45,939 - transformer_training - INFO - Main loop iteration: 2717
2025-03-22 07:20:46,200 - transformer_training - INFO - Main loop iteration: 2718
2025-03-22 07:20:46,466 - transformer_training - INFO - Main loop iteration: 2719
2025-03-22 07:20:46,744 - transformer_training - INFO - Main loop iteration: 2720
Iter 2720: loss 4.8439, lr 0.000498, 130652.60 tokens/sec
2025-03-22 07:20:46,996 - transformer_training - INFO - Main loop iteration: 2721
2025-03-22 07:20:47,258 - transformer_training - INFO - Main loop iteration: 2722
2025-03-22 07:20:47,518 - transformer_training - INFO - Main loop iteration: 2723
2025-03-22 07:20:47,802 - transformer_training - INFO - Main loop iteration: 2724
2025-03-22 07:20:48,053 - transformer_training - INFO - Main loop iteration: 2725
2025-03-22 07:20:48,315 - transformer_training - INFO - Main loop iteration: 2726
2025-03-22 07:20:48,713 - transformer_training - INFO - Main loop iteration: 2727
2025-03-22 07:20:48,991 - transformer_training - INFO - Main loop iteration: 2728
2025-03-22 07:20:49,242 - transformer_training - INFO - Main loop iteration: 2729
2025-03-22 07:20:49,504 - transformer_training - INFO - Main loop iteration: 2730
Iter 2730: loss 4.9207, lr 0.000498, 126430.87 tokens/sec
2025-03-22 07:20:49,764 - transformer_training - INFO - Main loop iteration: 2731
2025-03-22 07:20:50,048 - transformer_training - INFO - Main loop iteration: 2732
2025-03-22 07:20:50,303 - transformer_training - INFO - Main loop iteration: 2733
2025-03-22 07:20:50,564 - transformer_training - INFO - Main loop iteration: 2734
2025-03-22 07:20:50,824 - transformer_training - INFO - Main loop iteration: 2735
2025-03-22 07:20:51,102 - transformer_training - INFO - Main loop iteration: 2736
2025-03-22 07:20:51,353 - transformer_training - INFO - Main loop iteration: 2737
2025-03-22 07:20:51,615 - transformer_training - INFO - Main loop iteration: 2738
2025-03-22 07:20:51,874 - transformer_training - INFO - Main loop iteration: 2739
2025-03-22 07:20:52,153 - transformer_training - INFO - Main loop iteration: 2740
Iter 2740: loss 4.9389, lr 0.000498, 130595.74 tokens/sec
2025-03-22 07:20:52,405 - transformer_training - INFO - Main loop iteration: 2741
2025-03-22 07:20:52,666 - transformer_training - INFO - Main loop iteration: 2742
2025-03-22 07:20:52,926 - transformer_training - INFO - Main loop iteration: 2743
2025-03-22 07:20:53,205 - transformer_training - INFO - Main loop iteration: 2744
2025-03-22 07:20:53,456 - transformer_training - INFO - Main loop iteration: 2745
2025-03-22 07:20:53,717 - transformer_training - INFO - Main loop iteration: 2746
2025-03-22 07:20:53,977 - transformer_training - INFO - Main loop iteration: 2747
2025-03-22 07:20:54,256 - transformer_training - INFO - Main loop iteration: 2748
2025-03-22 07:20:54,507 - transformer_training - INFO - Main loop iteration: 2749
2025-03-22 07:20:54,768 - transformer_training - INFO - Main loop iteration: 2750
Iter 2750: loss 4.8794, lr 0.000498, 126487.19 tokens/sec
2025-03-22 07:20:55,028 - transformer_training - INFO - Main loop iteration: 2751
2025-03-22 07:20:55,307 - transformer_training - INFO - Main loop iteration: 2752
2025-03-22 07:20:55,558 - transformer_training - INFO - Main loop iteration: 2753
2025-03-22 07:20:55,820 - transformer_training - INFO - Main loop iteration: 2754
2025-03-22 07:20:56,080 - transformer_training - INFO - Main loop iteration: 2755
2025-03-22 07:20:56,358 - transformer_training - INFO - Main loop iteration: 2756
2025-03-22 07:20:56,613 - transformer_training - INFO - Main loop iteration: 2757
2025-03-22 07:20:56,875 - transformer_training - INFO - Main loop iteration: 2758
2025-03-22 07:20:57,135 - transformer_training - INFO - Main loop iteration: 2759
2025-03-22 07:20:57,413 - transformer_training - INFO - Main loop iteration: 2760
Iter 2760: loss 4.8445, lr 0.000498, 130509.55 tokens/sec
2025-03-22 07:20:57,665 - transformer_training - INFO - Main loop iteration: 2761
2025-03-22 07:20:57,926 - transformer_training - INFO - Main loop iteration: 2762
2025-03-22 07:20:58,186 - transformer_training - INFO - Main loop iteration: 2763
2025-03-22 07:20:58,464 - transformer_training - INFO - Main loop iteration: 2764
2025-03-22 07:20:58,715 - transformer_training - INFO - Main loop iteration: 2765
2025-03-22 07:20:58,977 - transformer_training - INFO - Main loop iteration: 2766
2025-03-22 07:20:59,237 - transformer_training - INFO - Main loop iteration: 2767
2025-03-22 07:20:59,516 - transformer_training - INFO - Main loop iteration: 2768
2025-03-22 07:20:59,768 - transformer_training - INFO - Main loop iteration: 2769
2025-03-22 07:21:00,029 - transformer_training - INFO - Main loop iteration: 2770
Iter 2770: loss 4.8091, lr 0.000498, 125991.61 tokens/sec
2025-03-22 07:21:00,290 - transformer_training - INFO - Main loop iteration: 2771
2025-03-22 07:21:00,568 - transformer_training - INFO - Main loop iteration: 2772
2025-03-22 07:21:00,820 - transformer_training - INFO - Main loop iteration: 2773
2025-03-22 07:21:01,081 - transformer_training - INFO - Main loop iteration: 2774
2025-03-22 07:21:01,341 - transformer_training - INFO - Main loop iteration: 2775
2025-03-22 07:21:01,620 - transformer_training - INFO - Main loop iteration: 2776
2025-03-22 07:21:01,870 - transformer_training - INFO - Main loop iteration: 2777
2025-03-22 07:21:02,132 - transformer_training - INFO - Main loop iteration: 2778
2025-03-22 07:21:02,392 - transformer_training - INFO - Main loop iteration: 2779
2025-03-22 07:21:02,670 - transformer_training - INFO - Main loop iteration: 2780
Iter 2780: loss 4.8989, lr 0.000498, 130776.67 tokens/sec
2025-03-22 07:21:02,921 - transformer_training - INFO - Main loop iteration: 2781
2025-03-22 07:21:03,183 - transformer_training - INFO - Main loop iteration: 2782
2025-03-22 07:21:03,442 - transformer_training - INFO - Main loop iteration: 2783
2025-03-22 07:21:03,721 - transformer_training - INFO - Main loop iteration: 2784
2025-03-22 07:21:03,972 - transformer_training - INFO - Main loop iteration: 2785
2025-03-22 07:21:04,234 - transformer_training - INFO - Main loop iteration: 2786
2025-03-22 07:21:04,494 - transformer_training - INFO - Main loop iteration: 2787
2025-03-22 07:21:04,772 - transformer_training - INFO - Main loop iteration: 2788
2025-03-22 07:21:05,023 - transformer_training - INFO - Main loop iteration: 2789
2025-03-22 07:21:05,285 - transformer_training - INFO - Main loop iteration: 2790
Iter 2790: loss 4.9080, lr 0.000498, 126371.35 tokens/sec
2025-03-22 07:21:05,545 - transformer_training - INFO - Main loop iteration: 2791
2025-03-22 07:21:05,823 - transformer_training - INFO - Main loop iteration: 2792
2025-03-22 07:21:06,077 - transformer_training - INFO - Main loop iteration: 2793
2025-03-22 07:21:06,339 - transformer_training - INFO - Main loop iteration: 2794
2025-03-22 07:21:06,599 - transformer_training - INFO - Main loop iteration: 2795
2025-03-22 07:21:06,877 - transformer_training - INFO - Main loop iteration: 2796
2025-03-22 07:21:07,129 - transformer_training - INFO - Main loop iteration: 2797
2025-03-22 07:21:07,391 - transformer_training - INFO - Main loop iteration: 2798
2025-03-22 07:21:07,651 - transformer_training - INFO - Main loop iteration: 2799
2025-03-22 07:21:07,929 - transformer_training - INFO - Main loop iteration: 2800
Iter 2800: loss 4.8815, lr 0.000498, 128732.64 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 2800: train loss 4.7264, val loss 4.6526
New best model saved with val loss: 4.6526
2025-03-22 07:21:28,695 - transformer_training - INFO - Main loop iteration: 2801
2025-03-22 07:21:28,956 - transformer_training - INFO - Main loop iteration: 2802
2025-03-22 07:21:29,218 - transformer_training - INFO - Main loop iteration: 2803
2025-03-22 07:21:29,495 - transformer_training - INFO - Main loop iteration: 2804
2025-03-22 07:21:29,746 - transformer_training - INFO - Main loop iteration: 2805
2025-03-22 07:21:30,008 - transformer_training - INFO - Main loop iteration: 2806
2025-03-22 07:21:30,268 - transformer_training - INFO - Main loop iteration: 2807
2025-03-22 07:21:30,552 - transformer_training - INFO - Main loop iteration: 2808
2025-03-22 07:21:30,803 - transformer_training - INFO - Main loop iteration: 2809
2025-03-22 07:21:31,064 - transformer_training - INFO - Main loop iteration: 2810
Iter 2810: loss 4.8385, lr 0.000498, 126348.00 tokens/sec
2025-03-22 07:21:31,325 - transformer_training - INFO - Main loop iteration: 2811
2025-03-22 07:21:31,602 - transformer_training - INFO - Main loop iteration: 2812
2025-03-22 07:21:31,853 - transformer_training - INFO - Main loop iteration: 2813
2025-03-22 07:21:32,114 - transformer_training - INFO - Main loop iteration: 2814
2025-03-22 07:21:32,374 - transformer_training - INFO - Main loop iteration: 2815
2025-03-22 07:21:32,653 - transformer_training - INFO - Main loop iteration: 2816
2025-03-22 07:21:32,903 - transformer_training - INFO - Main loop iteration: 2817
2025-03-22 07:21:33,165 - transformer_training - INFO - Main loop iteration: 2818
2025-03-22 07:21:33,424 - transformer_training - INFO - Main loop iteration: 2819
2025-03-22 07:21:33,701 - transformer_training - INFO - Main loop iteration: 2820
Iter 2820: loss 4.9196, lr 0.000497, 129633.22 tokens/sec
2025-03-22 07:21:33,955 - transformer_training - INFO - Main loop iteration: 2821
2025-03-22 07:21:34,220 - transformer_training - INFO - Main loop iteration: 2822
2025-03-22 07:21:34,479 - transformer_training - INFO - Main loop iteration: 2823
2025-03-22 07:21:34,756 - transformer_training - INFO - Main loop iteration: 2824
2025-03-22 07:21:35,008 - transformer_training - INFO - Main loop iteration: 2825
2025-03-22 07:21:35,269 - transformer_training - INFO - Main loop iteration: 2826
2025-03-22 07:21:35,527 - transformer_training - INFO - Main loop iteration: 2827
2025-03-22 07:21:35,804 - transformer_training - INFO - Main loop iteration: 2828
2025-03-22 07:21:36,054 - transformer_training - INFO - Main loop iteration: 2829
2025-03-22 07:21:36,315 - transformer_training - INFO - Main loop iteration: 2830
Iter 2830: loss 4.8775, lr 0.000497, 127147.86 tokens/sec
2025-03-22 07:21:36,573 - transformer_training - INFO - Main loop iteration: 2831
2025-03-22 07:21:36,850 - transformer_training - INFO - Main loop iteration: 2832
2025-03-22 07:21:37,100 - transformer_training - INFO - Main loop iteration: 2833
2025-03-22 07:21:37,361 - transformer_training - INFO - Main loop iteration: 2834
2025-03-22 07:21:37,619 - transformer_training - INFO - Main loop iteration: 2835
2025-03-22 07:21:37,896 - transformer_training - INFO - Main loop iteration: 2836
2025-03-22 07:21:38,145 - transformer_training - INFO - Main loop iteration: 2837
2025-03-22 07:21:38,406 - transformer_training - INFO - Main loop iteration: 2838
2025-03-22 07:21:38,664 - transformer_training - INFO - Main loop iteration: 2839
2025-03-22 07:21:38,942 - transformer_training - INFO - Main loop iteration: 2840
Iter 2840: loss 4.8656, lr 0.000497, 131830.24 tokens/sec
2025-03-22 07:21:39,191 - transformer_training - INFO - Main loop iteration: 2841
2025-03-22 07:21:39,451 - transformer_training - INFO - Main loop iteration: 2842
2025-03-22 07:21:39,710 - transformer_training - INFO - Main loop iteration: 2843
2025-03-22 07:21:39,987 - transformer_training - INFO - Main loop iteration: 2844
2025-03-22 07:21:40,240 - transformer_training - INFO - Main loop iteration: 2845
2025-03-22 07:21:40,518 - transformer_training - INFO - Main loop iteration: 2846
2025-03-22 07:21:40,776 - transformer_training - INFO - Main loop iteration: 2847
2025-03-22 07:21:41,053 - transformer_training - INFO - Main loop iteration: 2848
2025-03-22 07:21:41,303 - transformer_training - INFO - Main loop iteration: 2849
2025-03-22 07:21:41,563 - transformer_training - INFO - Main loop iteration: 2850
Iter 2850: loss 4.7268, lr 0.000497, 127178.57 tokens/sec
2025-03-22 07:21:41,822 - transformer_training - INFO - Main loop iteration: 2851
2025-03-22 07:21:42,099 - transformer_training - INFO - Main loop iteration: 2852
2025-03-22 07:21:42,348 - transformer_training - INFO - Main loop iteration: 2853
2025-03-22 07:21:42,609 - transformer_training - INFO - Main loop iteration: 2854
2025-03-22 07:21:42,868 - transformer_training - INFO - Main loop iteration: 2855
2025-03-22 07:21:43,144 - transformer_training - INFO - Main loop iteration: 2856
2025-03-22 07:21:43,394 - transformer_training - INFO - Main loop iteration: 2857
2025-03-22 07:21:43,654 - transformer_training - INFO - Main loop iteration: 2858
2025-03-22 07:21:43,912 - transformer_training - INFO - Main loop iteration: 2859
2025-03-22 07:21:44,189 - transformer_training - INFO - Main loop iteration: 2860
Iter 2860: loss 4.8960, lr 0.000497, 131900.33 tokens/sec
2025-03-22 07:21:44,438 - transformer_training - INFO - Main loop iteration: 2861
2025-03-22 07:21:44,698 - transformer_training - INFO - Main loop iteration: 2862
2025-03-22 07:21:44,956 - transformer_training - INFO - Main loop iteration: 2863
2025-03-22 07:21:45,233 - transformer_training - INFO - Main loop iteration: 2864
2025-03-22 07:21:45,482 - transformer_training - INFO - Main loop iteration: 2865
2025-03-22 07:21:45,742 - transformer_training - INFO - Main loop iteration: 2866
2025-03-22 07:21:46,000 - transformer_training - INFO - Main loop iteration: 2867
2025-03-22 07:21:46,277 - transformer_training - INFO - Main loop iteration: 2868
2025-03-22 07:21:46,527 - transformer_training - INFO - Main loop iteration: 2869
2025-03-22 07:21:46,787 - transformer_training - INFO - Main loop iteration: 2870
Iter 2870: loss 4.7854, lr 0.000497, 127236.73 tokens/sec
2025-03-22 07:21:47,046 - transformer_training - INFO - Main loop iteration: 2871
2025-03-22 07:21:47,323 - transformer_training - INFO - Main loop iteration: 2872
2025-03-22 07:21:47,572 - transformer_training - INFO - Main loop iteration: 2873
2025-03-22 07:21:47,832 - transformer_training - INFO - Main loop iteration: 2874
2025-03-22 07:21:48,090 - transformer_training - INFO - Main loop iteration: 2875
2025-03-22 07:21:48,367 - transformer_training - INFO - Main loop iteration: 2876
2025-03-22 07:21:48,620 - transformer_training - INFO - Main loop iteration: 2877
2025-03-22 07:21:48,880 - transformer_training - INFO - Main loop iteration: 2878
2025-03-22 07:21:49,139 - transformer_training - INFO - Main loop iteration: 2879
2025-03-22 07:21:49,416 - transformer_training - INFO - Main loop iteration: 2880
Iter 2880: loss 4.8432, lr 0.000497, 131525.82 tokens/sec
2025-03-22 07:21:49,666 - transformer_training - INFO - Main loop iteration: 2881
2025-03-22 07:21:49,926 - transformer_training - INFO - Main loop iteration: 2882
2025-03-22 07:21:50,185 - transformer_training - INFO - Main loop iteration: 2883
2025-03-22 07:21:50,463 - transformer_training - INFO - Main loop iteration: 2884
2025-03-22 07:21:50,712 - transformer_training - INFO - Main loop iteration: 2885
2025-03-22 07:21:50,973 - transformer_training - INFO - Main loop iteration: 2886
2025-03-22 07:21:51,231 - transformer_training - INFO - Main loop iteration: 2887
2025-03-22 07:21:51,508 - transformer_training - INFO - Main loop iteration: 2888
2025-03-22 07:21:51,757 - transformer_training - INFO - Main loop iteration: 2889
2025-03-22 07:21:52,018 - transformer_training - INFO - Main loop iteration: 2890
Iter 2890: loss 4.7893, lr 0.000497, 127034.45 tokens/sec
2025-03-22 07:21:52,276 - transformer_training - INFO - Main loop iteration: 2891
2025-03-22 07:21:52,554 - transformer_training - INFO - Main loop iteration: 2892
2025-03-22 07:21:52,808 - transformer_training - INFO - Main loop iteration: 2893
2025-03-22 07:21:53,069 - transformer_training - INFO - Main loop iteration: 2894
2025-03-22 07:21:53,329 - transformer_training - INFO - Main loop iteration: 2895
2025-03-22 07:21:53,606 - transformer_training - INFO - Main loop iteration: 2896
2025-03-22 07:21:53,857 - transformer_training - INFO - Main loop iteration: 2897
2025-03-22 07:21:54,118 - transformer_training - INFO - Main loop iteration: 2898
2025-03-22 07:21:54,378 - transformer_training - INFO - Main loop iteration: 2899
2025-03-22 07:21:54,656 - transformer_training - INFO - Main loop iteration: 2900
Iter 2900: loss 4.7549, lr 0.000497, 131159.56 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
terminate called without an active exception
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPTYour base folder is: /workspace/GPT

Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 2900: train loss 4.6811, val loss 4.5999
New best model saved with val loss: 4.5999
2025-03-22 07:22:15,273 - transformer_training - INFO - Main loop iteration: 2901
2025-03-22 07:22:15,526 - transformer_training - INFO - Main loop iteration: 2902
2025-03-22 07:22:15,785 - transformer_training - INFO - Main loop iteration: 2903
2025-03-22 07:22:16,063 - transformer_training - INFO - Main loop iteration: 2904
2025-03-22 07:22:16,314 - transformer_training - INFO - Main loop iteration: 2905
2025-03-22 07:22:16,575 - transformer_training - INFO - Main loop iteration: 2906
2025-03-22 07:22:16,835 - transformer_training - INFO - Main loop iteration: 2907
2025-03-22 07:22:17,113 - transformer_training - INFO - Main loop iteration: 2908
2025-03-22 07:22:17,363 - transformer_training - INFO - Main loop iteration: 2909
2025-03-22 07:22:17,624 - transformer_training - INFO - Main loop iteration: 2910
Iter 2910: loss 4.8069, lr 0.000497, 126391.46 tokens/sec
2025-03-22 07:22:17,885 - transformer_training - INFO - Main loop iteration: 2911
2025-03-22 07:22:18,163 - transformer_training - INFO - Main loop iteration: 2912
2025-03-22 07:22:18,413 - transformer_training - INFO - Main loop iteration: 2913
2025-03-22 07:22:18,675 - transformer_training - INFO - Main loop iteration: 2914
2025-03-22 07:22:18,935 - transformer_training - INFO - Main loop iteration: 2915
2025-03-22 07:22:19,213 - transformer_training - INFO - Main loop iteration: 2916
2025-03-22 07:22:19,464 - transformer_training - INFO - Main loop iteration: 2917
2025-03-22 07:22:19,725 - transformer_training - INFO - Main loop iteration: 2918
2025-03-22 07:22:19,985 - transformer_training - INFO - Main loop iteration: 2919
2025-03-22 07:22:20,263 - transformer_training - INFO - Main loop iteration: 2920
Iter 2920: loss 4.7160, lr 0.000497, 131066.38 tokens/sec
2025-03-22 07:22:20,514 - transformer_training - INFO - Main loop iteration: 2921
2025-03-22 07:22:20,775 - transformer_training - INFO - Main loop iteration: 2922
2025-03-22 07:22:21,035 - transformer_training - INFO - Main loop iteration: 2923
2025-03-22 07:22:21,313 - transformer_training - INFO - Main loop iteration: 2924
2025-03-22 07:22:21,564 - transformer_training - INFO - Main loop iteration: 2925
2025-03-22 07:22:21,824 - transformer_training - INFO - Main loop iteration: 2926
2025-03-22 07:22:22,084 - transformer_training - INFO - Main loop iteration: 2927
2025-03-22 07:22:22,362 - transformer_training - INFO - Main loop iteration: 2928
2025-03-22 07:22:22,612 - transformer_training - INFO - Main loop iteration: 2929
2025-03-22 07:22:22,874 - transformer_training - INFO - Main loop iteration: 2930
Iter 2930: loss 4.6491, lr 0.000497, 126560.10 tokens/sec
2025-03-22 07:22:23,134 - transformer_training - INFO - Main loop iteration: 2931
2025-03-22 07:22:23,411 - transformer_training - INFO - Main loop iteration: 2932
2025-03-22 07:22:23,662 - transformer_training - INFO - Main loop iteration: 2933
2025-03-22 07:22:23,923 - transformer_training - INFO - Main loop iteration: 2934
2025-03-22 07:22:24,182 - transformer_training - INFO - Main loop iteration: 2935
2025-03-22 07:22:24,460 - transformer_training - INFO - Main loop iteration: 2936
2025-03-22 07:22:24,713 - transformer_training - INFO - Main loop iteration: 2937
2025-03-22 07:22:24,976 - transformer_training - INFO - Main loop iteration: 2938
2025-03-22 07:22:25,237 - transformer_training - INFO - Main loop iteration: 2939
2025-03-22 07:22:25,515 - transformer_training - INFO - Main loop iteration: 2940
Iter 2940: loss 4.8334, lr 0.000497, 130908.58 tokens/sec
2025-03-22 07:22:25,766 - transformer_training - INFO - Main loop iteration: 2941
2025-03-22 07:22:26,027 - transformer_training - INFO - Main loop iteration: 2942
2025-03-22 07:22:26,287 - transformer_training - INFO - Main loop iteration: 2943
2025-03-22 07:22:26,565 - transformer_training - INFO - Main loop iteration: 2944
2025-03-22 07:22:26,816 - transformer_training - INFO - Main loop iteration: 2945
2025-03-22 07:22:27,078 - transformer_training - INFO - Main loop iteration: 2946
2025-03-22 07:22:27,340 - transformer_training - INFO - Main loop iteration: 2947
2025-03-22 07:22:27,619 - transformer_training - INFO - Main loop iteration: 2948
2025-03-22 07:22:27,871 - transformer_training - INFO - Main loop iteration: 2949
2025-03-22 07:22:28,132 - transformer_training - INFO - Main loop iteration: 2950
Iter 2950: loss 4.7636, lr 0.000497, 126392.73 tokens/sec
2025-03-22 07:22:28,393 - transformer_training - INFO - Main loop iteration: 2951
2025-03-22 07:22:28,671 - transformer_training - INFO - Main loop iteration: 2952
2025-03-22 07:22:28,921 - transformer_training - INFO - Main loop iteration: 2953
2025-03-22 07:22:29,182 - transformer_training - INFO - Main loop iteration: 2954
2025-03-22 07:22:29,442 - transformer_training - INFO - Main loop iteration: 2955
2025-03-22 07:22:29,720 - transformer_training - INFO - Main loop iteration: 2956
2025-03-22 07:22:29,971 - transformer_training - INFO - Main loop iteration: 2957
2025-03-22 07:22:30,232 - transformer_training - INFO - Main loop iteration: 2958
2025-03-22 07:22:30,492 - transformer_training - INFO - Main loop iteration: 2959
2025-03-22 07:22:30,770 - transformer_training - INFO - Main loop iteration: 2960
Iter 2960: loss 4.8837, lr 0.000497, 131104.51 tokens/sec
2025-03-22 07:22:31,020 - transformer_training - INFO - Main loop iteration: 2961
2025-03-22 07:22:31,282 - transformer_training - INFO - Main loop iteration: 2962
2025-03-22 07:22:31,541 - transformer_training - INFO - Main loop iteration: 2963
2025-03-22 07:22:31,819 - transformer_training - INFO - Main loop iteration: 2964
2025-03-22 07:22:32,070 - transformer_training - INFO - Main loop iteration: 2965
2025-03-22 07:22:32,332 - transformer_training - INFO - Main loop iteration: 2966
2025-03-22 07:22:32,592 - transformer_training - INFO - Main loop iteration: 2967
2025-03-22 07:22:32,870 - transformer_training - INFO - Main loop iteration: 2968
2025-03-22 07:22:33,121 - transformer_training - INFO - Main loop iteration: 2969
2025-03-22 07:22:33,382 - transformer_training - INFO - Main loop iteration: 2970
Iter 2970: loss 4.7339, lr 0.000496, 126404.47 tokens/sec
2025-03-22 07:22:33,643 - transformer_training - INFO - Main loop iteration: 2971
2025-03-22 07:22:33,920 - transformer_training - INFO - Main loop iteration: 2972
2025-03-22 07:22:34,171 - transformer_training - INFO - Main loop iteration: 2973
2025-03-22 07:22:34,432 - transformer_training - INFO - Main loop iteration: 2974
2025-03-22 07:22:34,692 - transformer_training - INFO - Main loop iteration: 2975
2025-03-22 07:22:34,970 - transformer_training - INFO - Main loop iteration: 2976
2025-03-22 07:22:35,221 - transformer_training - INFO - Main loop iteration: 2977
2025-03-22 07:22:35,482 - transformer_training - INFO - Main loop iteration: 2978
2025-03-22 07:22:35,742 - transformer_training - INFO - Main loop iteration: 2979
2025-03-22 07:22:36,020 - transformer_training - INFO - Main loop iteration: 2980
Iter 2980: loss 4.6779, lr 0.000496, 130930.28 tokens/sec
2025-03-22 07:22:36,271 - transformer_training - INFO - Main loop iteration: 2981
2025-03-22 07:22:36,532 - transformer_training - INFO - Main loop iteration: 2982
2025-03-22 07:22:36,792 - transformer_training - INFO - Main loop iteration: 2983
2025-03-22 07:22:37,070 - transformer_training - INFO - Main loop iteration: 2984
2025-03-22 07:22:37,321 - transformer_training - INFO - Main loop iteration: 2985
2025-03-22 07:22:37,583 - transformer_training - INFO - Main loop iteration: 2986
2025-03-22 07:22:37,842 - transformer_training - INFO - Main loop iteration: 2987
2025-03-22 07:22:38,124 - transformer_training - INFO - Main loop iteration: 2988
2025-03-22 07:22:38,375 - transformer_training - INFO - Main loop iteration: 2989
2025-03-22 07:22:38,636 - transformer_training - INFO - Main loop iteration: 2990
Iter 2990: loss 4.7852, lr 0.000496, 126452.39 tokens/sec
2025-03-22 07:22:38,897 - transformer_training - INFO - Main loop iteration: 2991
2025-03-22 07:22:39,179 - transformer_training - INFO - Main loop iteration: 2992
2025-03-22 07:22:39,432 - transformer_training - INFO - Main loop iteration: 2993
2025-03-22 07:22:39,693 - transformer_training - INFO - Main loop iteration: 2994
2025-03-22 07:22:39,953 - transformer_training - INFO - Main loop iteration: 2995
2025-03-22 07:22:40,232 - transformer_training - INFO - Main loop iteration: 2996
2025-03-22 07:22:40,483 - transformer_training - INFO - Main loop iteration: 2997
2025-03-22 07:22:40,744 - transformer_training - INFO - Main loop iteration: 2998
2025-03-22 07:22:41,004 - transformer_training - INFO - Main loop iteration: 2999
2025-03-22 07:22:41,282 - transformer_training - INFO - Main loop iteration: 3000
Iter 3000: loss 4.7554, lr 0.000496, 130783.88 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 3000: train loss 4.6326, val loss 4.5590
New best model saved with val loss: 4.5590
2025-03-22 07:23:03,932 - transformer_training - INFO - Main loop iteration: 3001
2025-03-22 07:23:04,206 - transformer_training - INFO - Main loop iteration: 3002
2025-03-22 07:23:04,472 - transformer_training - INFO - Main loop iteration: 3003
2025-03-22 07:23:04,748 - transformer_training - INFO - Main loop iteration: 3004
2025-03-22 07:23:04,998 - transformer_training - INFO - Main loop iteration: 3005
2025-03-22 07:23:05,260 - transformer_training - INFO - Main loop iteration: 3006
2025-03-22 07:23:05,519 - transformer_training - INFO - Main loop iteration: 3007
2025-03-22 07:23:05,798 - transformer_training - INFO - Main loop iteration: 3008
2025-03-22 07:23:06,049 - transformer_training - INFO - Main loop iteration: 3009
2025-03-22 07:23:06,310 - transformer_training - INFO - Main loop iteration: 3010
Iter 3010: loss 4.7277, lr 0.000496, 126450.06 tokens/sec
2025-03-22 07:23:06,570 - transformer_training - INFO - Main loop iteration: 3011
2025-03-22 07:23:06,854 - transformer_training - INFO - Main loop iteration: 3012
2025-03-22 07:23:07,105 - transformer_training - INFO - Main loop iteration: 3013
2025-03-22 07:23:07,366 - transformer_training - INFO - Main loop iteration: 3014
2025-03-22 07:23:07,625 - transformer_training - INFO - Main loop iteration: 3015
2025-03-22 07:23:07,904 - transformer_training - INFO - Main loop iteration: 3016
2025-03-22 07:23:08,153 - transformer_training - INFO - Main loop iteration: 3017
2025-03-22 07:23:08,415 - transformer_training - INFO - Main loop iteration: 3018
2025-03-22 07:23:08,674 - transformer_training - INFO - Main loop iteration: 3019
2025-03-22 07:23:08,953 - transformer_training - INFO - Main loop iteration: 3020
Iter 3020: loss 4.6973, lr 0.000496, 131207.77 tokens/sec
2025-03-22 07:23:09,204 - transformer_training - INFO - Main loop iteration: 3021
2025-03-22 07:23:09,465 - transformer_training - INFO - Main loop iteration: 3022
2025-03-22 07:23:09,724 - transformer_training - INFO - Main loop iteration: 3023
2025-03-22 07:23:10,002 - transformer_training - INFO - Main loop iteration: 3024
2025-03-22 07:23:10,252 - transformer_training - INFO - Main loop iteration: 3025
2025-03-22 07:23:10,513 - transformer_training - INFO - Main loop iteration: 3026
2025-03-22 07:23:10,773 - transformer_training - INFO - Main loop iteration: 3027
2025-03-22 07:23:11,051 - transformer_training - INFO - Main loop iteration: 3028
2025-03-22 07:23:11,301 - transformer_training - INFO - Main loop iteration: 3029
2025-03-22 07:23:11,562 - transformer_training - INFO - Main loop iteration: 3030
Iter 3030: loss 4.7093, lr 0.000496, 126609.18 tokens/sec
2025-03-22 07:23:11,822 - transformer_training - INFO - Main loop iteration: 3031
2025-03-22 07:23:12,100 - transformer_training - INFO - Main loop iteration: 3032
2025-03-22 07:23:12,350 - transformer_training - INFO - Main loop iteration: 3033
2025-03-22 07:23:12,612 - transformer_training - INFO - Main loop iteration: 3034
2025-03-22 07:23:12,874 - transformer_training - INFO - Main loop iteration: 3035
2025-03-22 07:23:13,152 - transformer_training - INFO - Main loop iteration: 3036
2025-03-22 07:23:13,402 - transformer_training - INFO - Main loop iteration: 3037
2025-03-22 07:23:13,663 - transformer_training - INFO - Main loop iteration: 3038
2025-03-22 07:23:13,924 - transformer_training - INFO - Main loop iteration: 3039
2025-03-22 07:23:14,201 - transformer_training - INFO - Main loop iteration: 3040
Iter 3040: loss 4.7137, lr 0.000496, 130962.72 tokens/sec
2025-03-22 07:23:14,452 - transformer_training - INFO - Main loop iteration: 3041
2025-03-22 07:23:14,713 - transformer_training - INFO - Main loop iteration: 3042
2025-03-22 07:23:14,973 - transformer_training - INFO - Main loop iteration: 3043
2025-03-22 07:23:15,251 - transformer_training - INFO - Main loop iteration: 3044
2025-03-22 07:23:15,500 - transformer_training - INFO - Main loop iteration: 3045
2025-03-22 07:23:15,761 - transformer_training - INFO - Main loop iteration: 3046
2025-03-22 07:23:16,021 - transformer_training - INFO - Main loop iteration: 3047
2025-03-22 07:23:16,301 - transformer_training - INFO - Main loop iteration: 3048
2025-03-22 07:23:16,551 - transformer_training - INFO - Main loop iteration: 3049
2025-03-22 07:23:16,812 - transformer_training - INFO - Main loop iteration: 3050
Iter 3050: loss 4.6199, lr 0.000496, 126572.46 tokens/sec
2025-03-22 07:23:17,074 - transformer_training - INFO - Main loop iteration: 3051
2025-03-22 07:23:17,373 - transformer_training - INFO - Main loop iteration: 3052
2025-03-22 07:23:17,622 - transformer_training - INFO - Main loop iteration: 3053
2025-03-22 07:23:17,884 - transformer_training - INFO - Main loop iteration: 3054
2025-03-22 07:23:18,144 - transformer_training - INFO - Main loop iteration: 3055
2025-03-22 07:23:18,424 - transformer_training - INFO - Main loop iteration: 3056
2025-03-22 07:23:18,673 - transformer_training - INFO - Main loop iteration: 3057
2025-03-22 07:23:18,934 - transformer_training - INFO - Main loop iteration: 3058
2025-03-22 07:23:19,194 - transformer_training - INFO - Main loop iteration: 3059
2025-03-22 07:23:19,474 - transformer_training - INFO - Main loop iteration: 3060
Iter 3060: loss 4.7267, lr 0.000496, 129747.15 tokens/sec
2025-03-22 07:23:19,727 - transformer_training - INFO - Main loop iteration: 3061
2025-03-22 07:23:19,988 - transformer_training - INFO - Main loop iteration: 3062
2025-03-22 07:23:20,247 - transformer_training - INFO - Main loop iteration: 3063
2025-03-22 07:23:20,526 - transformer_training - INFO - Main loop iteration: 3064
2025-03-22 07:23:20,776 - transformer_training - INFO - Main loop iteration: 3065
2025-03-22 07:23:21,038 - transformer_training - INFO - Main loop iteration: 3066
2025-03-22 07:23:21,297 - transformer_training - INFO - Main loop iteration: 3067
2025-03-22 07:23:21,576 - transformer_training - INFO - Main loop iteration: 3068
2025-03-22 07:23:21,826 - transformer_training - INFO - Main loop iteration: 3069
2025-03-22 07:23:22,088 - transformer_training - INFO - Main loop iteration: 3070
Iter 3070: loss 4.7443, lr 0.000496, 127231.43 tokens/sec
2025-03-22 07:23:22,347 - transformer_training - INFO - Main loop iteration: 3071
2025-03-22 07:23:22,627 - transformer_training - INFO - Main loop iteration: 3072
2025-03-22 07:23:23,000 - transformer_training - INFO - Main loop iteration: 3073
2025-03-22 07:23:23,262 - transformer_training - INFO - Main loop iteration: 3074
2025-03-22 07:23:23,528 - transformer_training - INFO - Main loop iteration: 3075
2025-03-22 07:23:23,806 - transformer_training - INFO - Main loop iteration: 3076
2025-03-22 07:23:24,057 - transformer_training - INFO - Main loop iteration: 3077
2025-03-22 07:23:24,318 - transformer_training - INFO - Main loop iteration: 3078
2025-03-22 07:23:24,584 - transformer_training - INFO - Main loop iteration: 3079
2025-03-22 07:23:24,862 - transformer_training - INFO - Main loop iteration: 3080
Iter 3080: loss 4.7100, lr 0.000496, 129178.14 tokens/sec
2025-03-22 07:23:25,116 - transformer_training - INFO - Main loop iteration: 3081
2025-03-22 07:23:25,378 - transformer_training - INFO - Main loop iteration: 3082
2025-03-22 07:23:25,638 - transformer_training - INFO - Main loop iteration: 3083
2025-03-22 07:23:25,915 - transformer_training - INFO - Main loop iteration: 3084
2025-03-22 07:23:26,165 - transformer_training - INFO - Main loop iteration: 3085
2025-03-22 07:23:26,427 - transformer_training - INFO - Main loop iteration: 3086
2025-03-22 07:23:26,687 - transformer_training - INFO - Main loop iteration: 3087
2025-03-22 07:23:26,968 - transformer_training - INFO - Main loop iteration: 3088
2025-03-22 07:23:27,219 - transformer_training - INFO - Main loop iteration: 3089
2025-03-22 07:23:27,480 - transformer_training - INFO - Main loop iteration: 3090
Iter 3090: loss 4.6887, lr 0.000496, 126359.85 tokens/sec
2025-03-22 07:23:27,740 - transformer_training - INFO - Main loop iteration: 3091
2025-03-22 07:23:28,023 - transformer_training - INFO - Main loop iteration: 3092
2025-03-22 07:23:28,273 - transformer_training - INFO - Main loop iteration: 3093
2025-03-22 07:23:28,534 - transformer_training - INFO - Main loop iteration: 3094
2025-03-22 07:23:28,794 - transformer_training - INFO - Main loop iteration: 3095
2025-03-22 07:23:29,072 - transformer_training - INFO - Main loop iteration: 3096
2025-03-22 07:23:29,323 - transformer_training - INFO - Main loop iteration: 3097
2025-03-22 07:23:29,584 - transformer_training - INFO - Main loop iteration: 3098
2025-03-22 07:23:29,844 - transformer_training - INFO - Main loop iteration: 3099
2025-03-22 07:23:30,121 - transformer_training - INFO - Main loop iteration: 3100
Iter 3100: loss 4.6713, lr 0.000495, 130721.69 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 3100: train loss 4.5884, val loss 4.5087
New best model saved with val loss: 4.5087
2025-03-22 07:23:50,436 - transformer_training - INFO - Main loop iteration: 3101
2025-03-22 07:23:50,713 - transformer_training - INFO - Main loop iteration: 3102
2025-03-22 07:23:50,975 - transformer_training - INFO - Main loop iteration: 3103
2025-03-22 07:23:51,252 - transformer_training - INFO - Main loop iteration: 3104
2025-03-22 07:23:51,502 - transformer_training - INFO - Main loop iteration: 3105
2025-03-22 07:23:51,763 - transformer_training - INFO - Main loop iteration: 3106
2025-03-22 07:23:52,023 - transformer_training - INFO - Main loop iteration: 3107
2025-03-22 07:23:52,303 - transformer_training - INFO - Main loop iteration: 3108
2025-03-22 07:23:52,553 - transformer_training - INFO - Main loop iteration: 3109
2025-03-22 07:23:52,816 - transformer_training - INFO - Main loop iteration: 3110
Iter 3110: loss 4.5492, lr 0.000495, 127238.03 tokens/sec
2025-03-22 07:23:53,076 - transformer_training - INFO - Main loop iteration: 3111
2025-03-22 07:23:53,356 - transformer_training - INFO - Main loop iteration: 3112
2025-03-22 07:23:53,605 - transformer_training - INFO - Main loop iteration: 3113
2025-03-22 07:23:53,867 - transformer_training - INFO - Main loop iteration: 3114
2025-03-22 07:23:54,126 - transformer_training - INFO - Main loop iteration: 3115
2025-03-22 07:23:54,410 - transformer_training - INFO - Main loop iteration: 3116
2025-03-22 07:23:54,659 - transformer_training - INFO - Main loop iteration: 3117
2025-03-22 07:23:54,920 - transformer_training - INFO - Main loop iteration: 3118
2025-03-22 07:23:55,182 - transformer_training - INFO - Main loop iteration: 3119
2025-03-22 07:23:55,461 - transformer_training - INFO - Main loop iteration: 3120
Iter 3120: loss 4.7665, lr 0.000495, 131714.26 tokens/sec
2025-03-22 07:23:55,710 - transformer_training - INFO - Main loop iteration: 3121
2025-03-22 07:23:55,971 - transformer_training - INFO - Main loop iteration: 3122
2025-03-22 07:23:56,230 - transformer_training - INFO - Main loop iteration: 3123
2025-03-22 07:23:56,509 - transformer_training - INFO - Main loop iteration: 3124
2025-03-22 07:23:56,758 - transformer_training - INFO - Main loop iteration: 3125
2025-03-22 07:23:57,020 - transformer_training - INFO - Main loop iteration: 3126
2025-03-22 07:23:57,279 - transformer_training - INFO - Main loop iteration: 3127
2025-03-22 07:23:57,558 - transformer_training - INFO - Main loop iteration: 3128
2025-03-22 07:23:57,810 - transformer_training - INFO - Main loop iteration: 3129
2025-03-22 07:23:58,071 - transformer_training - INFO - Main loop iteration: 3130
Iter 3130: loss 4.6788, lr 0.000495, 127500.07 tokens/sec
2025-03-22 07:23:58,330 - transformer_training - INFO - Main loop iteration: 3131
2025-03-22 07:23:58,615 - transformer_training - INFO - Main loop iteration: 3132
2025-03-22 07:23:58,869 - transformer_training - INFO - Main loop iteration: 3133
2025-03-22 07:23:59,127 - transformer_training - INFO - Main loop iteration: 3134
2025-03-22 07:23:59,388 - transformer_training - INFO - Main loop iteration: 3135
2025-03-22 07:23:59,665 - transformer_training - INFO - Main loop iteration: 3136
2025-03-22 07:23:59,914 - transformer_training - INFO - Main loop iteration: 3137
2025-03-22 07:24:00,175 - transformer_training - INFO - Main loop iteration: 3138
2025-03-22 07:24:00,434 - transformer_training - INFO - Main loop iteration: 3139
2025-03-22 07:24:00,713 - transformer_training - INFO - Main loop iteration: 3140
Iter 3140: loss 4.5554, lr 0.000495, 131726.88 tokens/sec
2025-03-22 07:24:00,962 - transformer_training - INFO - Main loop iteration: 3141
2025-03-22 07:24:01,223 - transformer_training - INFO - Main loop iteration: 3142
2025-03-22 07:24:01,483 - transformer_training - INFO - Main loop iteration: 3143
2025-03-22 07:24:01,762 - transformer_training - INFO - Main loop iteration: 3144
2025-03-22 07:24:02,011 - transformer_training - INFO - Main loop iteration: 3145
2025-03-22 07:24:02,273 - transformer_training - INFO - Main loop iteration: 3146
2025-03-22 07:24:02,532 - transformer_training - INFO - Main loop iteration: 3147
2025-03-22 07:24:02,812 - transformer_training - INFO - Main loop iteration: 3148
2025-03-22 07:24:03,061 - transformer_training - INFO - Main loop iteration: 3149
2025-03-22 07:24:03,323 - transformer_training - INFO - Main loop iteration: 3150
Iter 3150: loss 4.6934, lr 0.000495, 127280.80 tokens/sec
2025-03-22 07:24:03,583 - transformer_training - INFO - Main loop iteration: 3151
2025-03-22 07:24:03,862 - transformer_training - INFO - Main loop iteration: 3152
2025-03-22 07:24:04,111 - transformer_training - INFO - Main loop iteration: 3153
2025-03-22 07:24:04,373 - transformer_training - INFO - Main loop iteration: 3154
2025-03-22 07:24:04,632 - transformer_training - INFO - Main loop iteration: 3155
2025-03-22 07:24:04,911 - transformer_training - INFO - Main loop iteration: 3156
2025-03-22 07:24:05,161 - transformer_training - INFO - Main loop iteration: 3157
2025-03-22 07:24:05,423 - transformer_training - INFO - Main loop iteration: 3158
2025-03-22 07:24:05,682 - transformer_training - INFO - Main loop iteration: 3159
2025-03-22 07:24:05,967 - transformer_training - INFO - Main loop iteration: 3160
Iter 3160: loss 4.6927, lr 0.000495, 131711.61 tokens/sec
2025-03-22 07:24:06,217 - transformer_training - INFO - Main loop iteration: 3161
2025-03-22 07:24:06,478 - transformer_training - INFO - Main loop iteration: 3162
2025-03-22 07:24:06,737 - transformer_training - INFO - Main loop iteration: 3163
2025-03-22 07:24:07,016 - transformer_training - INFO - Main loop iteration: 3164
2025-03-22 07:24:07,266 - transformer_training - INFO - Main loop iteration: 3165
2025-03-22 07:24:07,527 - transformer_training - INFO - Main loop iteration: 3166
2025-03-22 07:24:07,787 - transformer_training - INFO - Main loop iteration: 3167
2025-03-22 07:24:08,070 - transformer_training - INFO - Main loop iteration: 3168
2025-03-22 07:24:08,320 - transformer_training - INFO - Main loop iteration: 3169
2025-03-22 07:24:08,581 - transformer_training - INFO - Main loop iteration: 3170
Iter 3170: loss 4.5888, lr 0.000495, 127075.80 tokens/sec
2025-03-22 07:24:08,841 - transformer_training - INFO - Main loop iteration: 3171
2025-03-22 07:24:09,120 - transformer_training - INFO - Main loop iteration: 3172
2025-03-22 07:24:09,370 - transformer_training - INFO - Main loop iteration: 3173
2025-03-22 07:24:09,631 - transformer_training - INFO - Main loop iteration: 3174
2025-03-22 07:24:09,890 - transformer_training - INFO - Main loop iteration: 3175
2025-03-22 07:24:10,169 - transformer_training - INFO - Main loop iteration: 3176
2025-03-22 07:24:10,419 - transformer_training - INFO - Main loop iteration: 3177
2025-03-22 07:24:10,680 - transformer_training - INFO - Main loop iteration: 3178
2025-03-22 07:24:10,940 - transformer_training - INFO - Main loop iteration: 3179
2025-03-22 07:24:11,219 - transformer_training - INFO - Main loop iteration: 3180
Iter 3180: loss 4.5944, lr 0.000495, 129979.19 tokens/sec
2025-03-22 07:24:11,472 - transformer_training - INFO - Main loop iteration: 3181
2025-03-22 07:24:11,733 - transformer_training - INFO - Main loop iteration: 3182
2025-03-22 07:24:11,994 - transformer_training - INFO - Main loop iteration: 3183
2025-03-22 07:24:12,272 - transformer_training - INFO - Main loop iteration: 3184
2025-03-22 07:24:12,521 - transformer_training - INFO - Main loop iteration: 3185
2025-03-22 07:24:12,782 - transformer_training - INFO - Main loop iteration: 3186
2025-03-22 07:24:13,044 - transformer_training - INFO - Main loop iteration: 3187
2025-03-22 07:24:13,322 - transformer_training - INFO - Main loop iteration: 3188
2025-03-22 07:24:13,572 - transformer_training - INFO - Main loop iteration: 3189
2025-03-22 07:24:13,833 - transformer_training - INFO - Main loop iteration: 3190
Iter 3190: loss 4.6532, lr 0.000495, 127304.97 tokens/sec
2025-03-22 07:24:14,092 - transformer_training - INFO - Main loop iteration: 3191
2025-03-22 07:24:14,372 - transformer_training - INFO - Main loop iteration: 3192
2025-03-22 07:24:14,622 - transformer_training - INFO - Main loop iteration: 3193
2025-03-22 07:24:14,883 - transformer_training - INFO - Main loop iteration: 3194
2025-03-22 07:24:15,142 - transformer_training - INFO - Main loop iteration: 3195
2025-03-22 07:24:15,421 - transformer_training - INFO - Main loop iteration: 3196
2025-03-22 07:24:15,671 - transformer_training - INFO - Main loop iteration: 3197
2025-03-22 07:24:15,931 - transformer_training - INFO - Main loop iteration: 3198
2025-03-22 07:24:16,195 - transformer_training - INFO - Main loop iteration: 3199
2025-03-22 07:24:16,474 - transformer_training - INFO - Main loop iteration: 3200
Iter 3200: loss 4.6214, lr 0.000495, 131472.72 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 3200: train loss 4.5510, val loss 4.4645
New best model saved with val loss: 4.4645
2025-03-22 07:24:36,825 - transformer_training - INFO - Main loop iteration: 3201
2025-03-22 07:24:37,077 - transformer_training - INFO - Main loop iteration: 3202
2025-03-22 07:24:37,336 - transformer_training - INFO - Main loop iteration: 3203
2025-03-22 07:24:37,620 - transformer_training - INFO - Main loop iteration: 3204
2025-03-22 07:24:37,875 - transformer_training - INFO - Main loop iteration: 3205
2025-03-22 07:24:38,136 - transformer_training - INFO - Main loop iteration: 3206
2025-03-22 07:24:38,395 - transformer_training - INFO - Main loop iteration: 3207
2025-03-22 07:24:38,674 - transformer_training - INFO - Main loop iteration: 3208
2025-03-22 07:24:38,924 - transformer_training - INFO - Main loop iteration: 3209
2025-03-22 07:24:39,185 - transformer_training - INFO - Main loop iteration: 3210
Iter 3210: loss 4.6088, lr 0.000495, 126705.95 tokens/sec
2025-03-22 07:24:39,445 - transformer_training - INFO - Main loop iteration: 3211
2025-03-22 07:24:39,722 - transformer_training - INFO - Main loop iteration: 3212
2025-03-22 07:24:39,973 - transformer_training - INFO - Main loop iteration: 3213
2025-03-22 07:24:40,234 - transformer_training - INFO - Main loop iteration: 3214
2025-03-22 07:24:40,493 - transformer_training - INFO - Main loop iteration: 3215
2025-03-22 07:24:40,770 - transformer_training - INFO - Main loop iteration: 3216
2025-03-22 07:24:41,020 - transformer_training - INFO - Main loop iteration: 3217
2025-03-22 07:24:41,282 - transformer_training - INFO - Main loop iteration: 3218
2025-03-22 07:24:41,541 - transformer_training - INFO - Main loop iteration: 3219
2025-03-22 07:24:41,824 - transformer_training - INFO - Main loop iteration: 3220
Iter 3220: loss 4.5734, lr 0.000494, 120425.59 tokens/sec
2025-03-22 07:24:42,097 - transformer_training - INFO - Main loop iteration: 3221
2025-03-22 07:24:42,373 - transformer_training - INFO - Main loop iteration: 3222
2025-03-22 07:24:42,651 - transformer_training - INFO - Main loop iteration: 3223
2025-03-22 07:24:42,947 - transformer_training - INFO - Main loop iteration: 3224
2025-03-22 07:24:43,214 - transformer_training - INFO - Main loop iteration: 3225
2025-03-22 07:24:43,490 - transformer_training - INFO - Main loop iteration: 3226
2025-03-22 07:24:43,765 - transformer_training - INFO - Main loop iteration: 3227
2025-03-22 07:24:44,061 - transformer_training - INFO - Main loop iteration: 3228
2025-03-22 07:24:44,329 - transformer_training - INFO - Main loop iteration: 3229
2025-03-22 07:24:44,606 - transformer_training - INFO - Main loop iteration: 3230
Iter 3230: loss 4.5817, lr 0.000494, 119306.61 tokens/sec
2025-03-22 07:24:44,882 - transformer_training - INFO - Main loop iteration: 3231
2025-03-22 07:24:45,183 - transformer_training - INFO - Main loop iteration: 3232
2025-03-22 07:24:45,451 - transformer_training - INFO - Main loop iteration: 3233
2025-03-22 07:24:45,727 - transformer_training - INFO - Main loop iteration: 3234
2025-03-22 07:24:46,002 - transformer_training - INFO - Main loop iteration: 3235
2025-03-22 07:24:46,300 - transformer_training - INFO - Main loop iteration: 3236
2025-03-22 07:24:46,568 - transformer_training - INFO - Main loop iteration: 3237
2025-03-22 07:24:46,845 - transformer_training - INFO - Main loop iteration: 3238
2025-03-22 07:24:47,120 - transformer_training - INFO - Main loop iteration: 3239
2025-03-22 07:24:47,417 - transformer_training - INFO - Main loop iteration: 3240
Iter 3240: loss 4.5527, lr 0.000494, 122571.52 tokens/sec
2025-03-22 07:24:47,685 - transformer_training - INFO - Main loop iteration: 3241
2025-03-22 07:24:47,962 - transformer_training - INFO - Main loop iteration: 3242
2025-03-22 07:24:48,238 - transformer_training - INFO - Main loop iteration: 3243
2025-03-22 07:24:48,534 - transformer_training - INFO - Main loop iteration: 3244
2025-03-22 07:24:48,802 - transformer_training - INFO - Main loop iteration: 3245
2025-03-22 07:24:49,363 - transformer_training - INFO - Main loop iteration: 3246
2025-03-22 07:24:49,640 - transformer_training - INFO - Main loop iteration: 3247
2025-03-22 07:24:49,935 - transformer_training - INFO - Main loop iteration: 3248
2025-03-22 07:24:50,206 - transformer_training - INFO - Main loop iteration: 3249
2025-03-22 07:24:50,481 - transformer_training - INFO - Main loop iteration: 3250
Iter 3250: loss 4.5338, lr 0.000494, 119688.44 tokens/sec
2025-03-22 07:24:50,758 - transformer_training - INFO - Main loop iteration: 3251
2025-03-22 07:24:51,047 - transformer_training - INFO - Main loop iteration: 3252
2025-03-22 07:24:51,310 - transformer_training - INFO - Main loop iteration: 3253
2025-03-22 07:24:51,587 - transformer_training - INFO - Main loop iteration: 3254
2025-03-22 07:24:51,861 - transformer_training - INFO - Main loop iteration: 3255
2025-03-22 07:24:52,157 - transformer_training - INFO - Main loop iteration: 3256
2025-03-22 07:24:52,425 - transformer_training - INFO - Main loop iteration: 3257
2025-03-22 07:24:52,692 - transformer_training - INFO - Main loop iteration: 3258
2025-03-22 07:24:52,952 - transformer_training - INFO - Main loop iteration: 3259
2025-03-22 07:24:53,231 - transformer_training - INFO - Main loop iteration: 3260
Iter 3260: loss 4.6006, lr 0.000494, 128957.18 tokens/sec
2025-03-22 07:24:53,486 - transformer_training - INFO - Main loop iteration: 3261
2025-03-22 07:24:53,747 - transformer_training - INFO - Main loop iteration: 3262
2025-03-22 07:24:54,009 - transformer_training - INFO - Main loop iteration: 3263
2025-03-22 07:24:54,287 - transformer_training - INFO - Main loop iteration: 3264
2025-03-22 07:24:54,537 - transformer_training - INFO - Main loop iteration: 3265
2025-03-22 07:24:54,799 - transformer_training - INFO - Main loop iteration: 3266
2025-03-22 07:24:55,059 - transformer_training - INFO - Main loop iteration: 3267
2025-03-22 07:24:55,337 - transformer_training - INFO - Main loop iteration: 3268
2025-03-22 07:24:55,588 - transformer_training - INFO - Main loop iteration: 3269
2025-03-22 07:24:55,849 - transformer_training - INFO - Main loop iteration: 3270
Iter 3270: loss 4.5526, lr 0.000494, 126648.85 tokens/sec
2025-03-22 07:24:56,109 - transformer_training - INFO - Main loop iteration: 3271
2025-03-22 07:24:56,391 - transformer_training - INFO - Main loop iteration: 3272
2025-03-22 07:24:56,641 - transformer_training - INFO - Main loop iteration: 3273
2025-03-22 07:24:56,903 - transformer_training - INFO - Main loop iteration: 3274
2025-03-22 07:24:57,163 - transformer_training - INFO - Main loop iteration: 3275
2025-03-22 07:24:57,441 - transformer_training - INFO - Main loop iteration: 3276
2025-03-22 07:24:57,693 - transformer_training - INFO - Main loop iteration: 3277
2025-03-22 07:24:57,954 - transformer_training - INFO - Main loop iteration: 3278
2025-03-22 07:24:58,214 - transformer_training - INFO - Main loop iteration: 3279
2025-03-22 07:24:58,492 - transformer_training - INFO - Main loop iteration: 3280
Iter 3280: loss 4.6529, lr 0.000494, 130829.33 tokens/sec
2025-03-22 07:24:58,743 - transformer_training - INFO - Main loop iteration: 3281
2025-03-22 07:24:59,009 - transformer_training - INFO - Main loop iteration: 3282
2025-03-22 07:24:59,269 - transformer_training - INFO - Main loop iteration: 3283
2025-03-22 07:24:59,548 - transformer_training - INFO - Main loop iteration: 3284
2025-03-22 07:24:59,799 - transformer_training - INFO - Main loop iteration: 3285
2025-03-22 07:25:00,060 - transformer_training - INFO - Main loop iteration: 3286
2025-03-22 07:25:00,325 - transformer_training - INFO - Main loop iteration: 3287
2025-03-22 07:25:00,603 - transformer_training - INFO - Main loop iteration: 3288
2025-03-22 07:25:00,856 - transformer_training - INFO - Main loop iteration: 3289
2025-03-22 07:25:01,117 - transformer_training - INFO - Main loop iteration: 3290
Iter 3290: loss 4.5441, lr 0.000494, 126305.15 tokens/sec
2025-03-22 07:25:01,378 - transformer_training - INFO - Main loop iteration: 3291
2025-03-22 07:25:01,655 - transformer_training - INFO - Main loop iteration: 3292
2025-03-22 07:25:01,906 - transformer_training - INFO - Main loop iteration: 3293
2025-03-22 07:25:02,168 - transformer_training - INFO - Main loop iteration: 3294
2025-03-22 07:25:02,432 - transformer_training - INFO - Main loop iteration: 3295
2025-03-22 07:25:02,709 - transformer_training - INFO - Main loop iteration: 3296
2025-03-22 07:25:02,960 - transformer_training - INFO - Main loop iteration: 3297
2025-03-22 07:25:03,221 - transformer_training - INFO - Main loop iteration: 3298
2025-03-22 07:25:03,480 - transformer_training - INFO - Main loop iteration: 3299
2025-03-22 07:25:03,758 - transformer_training - INFO - Main loop iteration: 3300
Iter 3300: loss 4.7622, lr 0.000494, 130888.38 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 3300: train loss 4.5164, val loss 4.4298
New best model saved with val loss: 4.4298
2025-03-22 07:25:23,929 - transformer_training - INFO - Main loop iteration: 3301
2025-03-22 07:25:24,213 - transformer_training - INFO - Main loop iteration: 3302
2025-03-22 07:25:24,471 - transformer_training - INFO - Main loop iteration: 3303
2025-03-22 07:25:24,749 - transformer_training - INFO - Main loop iteration: 3304
2025-03-22 07:25:24,998 - transformer_training - INFO - Main loop iteration: 3305
2025-03-22 07:25:25,258 - transformer_training - INFO - Main loop iteration: 3306
2025-03-22 07:25:25,517 - transformer_training - INFO - Main loop iteration: 3307
2025-03-22 07:25:25,795 - transformer_training - INFO - Main loop iteration: 3308
2025-03-22 07:25:26,043 - transformer_training - INFO - Main loop iteration: 3309
2025-03-22 07:25:26,308 - transformer_training - INFO - Main loop iteration: 3310
Iter 3310: loss 4.5443, lr 0.000494, 127760.82 tokens/sec
2025-03-22 07:25:26,567 - transformer_training - INFO - Main loop iteration: 3311
2025-03-22 07:25:26,844 - transformer_training - INFO - Main loop iteration: 3312
2025-03-22 07:25:27,091 - transformer_training - INFO - Main loop iteration: 3313
2025-03-22 07:25:27,351 - transformer_training - INFO - Main loop iteration: 3314
2025-03-22 07:25:27,610 - transformer_training - INFO - Main loop iteration: 3315
2025-03-22 07:25:27,888 - transformer_training - INFO - Main loop iteration: 3316
2025-03-22 07:25:28,136 - transformer_training - INFO - Main loop iteration: 3317
2025-03-22 07:25:28,396 - transformer_training - INFO - Main loop iteration: 3318
2025-03-22 07:25:28,655 - transformer_training - INFO - Main loop iteration: 3319
2025-03-22 07:25:28,931 - transformer_training - INFO - Main loop iteration: 3320
Iter 3320: loss 4.6530, lr 0.000493, 131855.40 tokens/sec
2025-03-22 07:25:29,180 - transformer_training - INFO - Main loop iteration: 3321
2025-03-22 07:25:29,442 - transformer_training - INFO - Main loop iteration: 3322
2025-03-22 07:25:29,700 - transformer_training - INFO - Main loop iteration: 3323
2025-03-22 07:25:29,977 - transformer_training - INFO - Main loop iteration: 3324
2025-03-22 07:25:30,226 - transformer_training - INFO - Main loop iteration: 3325
2025-03-22 07:25:30,485 - transformer_training - INFO - Main loop iteration: 3326
2025-03-22 07:25:30,744 - transformer_training - INFO - Main loop iteration: 3327
2025-03-22 07:25:31,021 - transformer_training - INFO - Main loop iteration: 3328
2025-03-22 07:25:31,270 - transformer_training - INFO - Main loop iteration: 3329
2025-03-22 07:25:31,530 - transformer_training - INFO - Main loop iteration: 3330
Iter 3330: loss 4.5278, lr 0.000493, 127147.86 tokens/sec
2025-03-22 07:25:31,789 - transformer_training - INFO - Main loop iteration: 3331
2025-03-22 07:25:32,065 - transformer_training - INFO - Main loop iteration: 3332
2025-03-22 07:25:32,314 - transformer_training - INFO - Main loop iteration: 3333
2025-03-22 07:25:32,575 - transformer_training - INFO - Main loop iteration: 3334
2025-03-22 07:25:32,833 - transformer_training - INFO - Main loop iteration: 3335
2025-03-22 07:25:33,110 - transformer_training - INFO - Main loop iteration: 3336
2025-03-22 07:25:33,357 - transformer_training - INFO - Main loop iteration: 3337
2025-03-22 07:25:33,618 - transformer_training - INFO - Main loop iteration: 3338
2025-03-22 07:25:33,876 - transformer_training - INFO - Main loop iteration: 3339
2025-03-22 07:25:34,152 - transformer_training - INFO - Main loop iteration: 3340
Iter 3340: loss 4.6133, lr 0.000493, 132943.98 tokens/sec
2025-03-22 07:25:34,399 - transformer_training - INFO - Main loop iteration: 3341
2025-03-22 07:25:34,642 - transformer_training - INFO - Main loop iteration: 3342
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
2025-03-22 07:25:38,256 - transformer_training - INFO - Main loop iteration: 3343
2025-03-22 07:25:38,533 - transformer_training - INFO - Main loop iteration: 3344
2025-03-22 07:25:38,782 - transformer_training - INFO - Main loop iteration: 3345
2025-03-22 07:25:39,042 - transformer_training - INFO - Main loop iteration: 3346
2025-03-22 07:25:39,300 - transformer_training - INFO - Main loop iteration: 3347
2025-03-22 07:25:39,578 - transformer_training - INFO - Main loop iteration: 3348
2025-03-22 07:25:39,828 - transformer_training - INFO - Main loop iteration: 3349
2025-03-22 07:25:40,088 - transformer_training - INFO - Main loop iteration: 3350
Iter 3350: loss 4.5024, lr 0.000493, 127198.11 tokens/sec
2025-03-22 07:25:40,346 - transformer_training - INFO - Main loop iteration: 3351
2025-03-22 07:25:40,623 - transformer_training - INFO - Main loop iteration: 3352
2025-03-22 07:25:40,872 - transformer_training - INFO - Main loop iteration: 3353
2025-03-22 07:25:41,132 - transformer_training - INFO - Main loop iteration: 3354
2025-03-22 07:25:41,391 - transformer_training - INFO - Main loop iteration: 3355
2025-03-22 07:25:41,667 - transformer_training - INFO - Main loop iteration: 3356
2025-03-22 07:25:41,917 - transformer_training - INFO - Main loop iteration: 3357
2025-03-22 07:25:42,177 - transformer_training - INFO - Main loop iteration: 3358
2025-03-22 07:25:42,436 - transformer_training - INFO - Main loop iteration: 3359
2025-03-22 07:25:42,712 - transformer_training - INFO - Main loop iteration: 3360
Iter 3360: loss 4.5326, lr 0.000493, 131663.53 tokens/sec
2025-03-22 07:25:42,962 - transformer_training - INFO - Main loop iteration: 3361
2025-03-22 07:25:43,222 - transformer_training - INFO - Main loop iteration: 3362
2025-03-22 07:25:43,480 - transformer_training - INFO - Main loop iteration: 3363
2025-03-22 07:25:43,757 - transformer_training - INFO - Main loop iteration: 3364
2025-03-22 07:25:44,007 - transformer_training - INFO - Main loop iteration: 3365
2025-03-22 07:25:44,267 - transformer_training - INFO - Main loop iteration: 3366
2025-03-22 07:25:44,526 - transformer_training - INFO - Main loop iteration: 3367
2025-03-22 07:25:44,803 - transformer_training - INFO - Main loop iteration: 3368
2025-03-22 07:25:45,053 - transformer_training - INFO - Main loop iteration: 3369
2025-03-22 07:25:45,313 - transformer_training - INFO - Main loop iteration: 3370
Iter 3370: loss 4.5544, lr 0.000493, 127072.98 tokens/sec
2025-03-22 07:25:45,572 - transformer_training - INFO - Main loop iteration: 3371
2025-03-22 07:25:45,849 - transformer_training - INFO - Main loop iteration: 3372
2025-03-22 07:25:46,099 - transformer_training - INFO - Main loop iteration: 3373
2025-03-22 07:25:46,360 - transformer_training - INFO - Main loop iteration: 3374
2025-03-22 07:25:46,618 - transformer_training - INFO - Main loop iteration: 3375
2025-03-22 07:25:46,895 - transformer_training - INFO - Main loop iteration: 3376
2025-03-22 07:25:47,144 - transformer_training - INFO - Main loop iteration: 3377
2025-03-22 07:25:47,404 - transformer_training - INFO - Main loop iteration: 3378
2025-03-22 07:25:47,663 - transformer_training - INFO - Main loop iteration: 3379
2025-03-22 07:25:47,940 - transformer_training - INFO - Main loop iteration: 3380
Iter 3380: loss 4.5286, lr 0.000493, 131510.97 tokens/sec
2025-03-22 07:25:48,190 - transformer_training - INFO - Main loop iteration: 3381
2025-03-22 07:25:48,449 - transformer_training - INFO - Main loop iteration: 3382
2025-03-22 07:25:48,708 - transformer_training - INFO - Main loop iteration: 3383
2025-03-22 07:25:48,985 - transformer_training - INFO - Main loop iteration: 3384
2025-03-22 07:25:49,234 - transformer_training - INFO - Main loop iteration: 3385
2025-03-22 07:25:49,494 - transformer_training - INFO - Main loop iteration: 3386
2025-03-22 07:25:49,753 - transformer_training - INFO - Main loop iteration: 3387
2025-03-22 07:25:50,029 - transformer_training - INFO - Main loop iteration: 3388
2025-03-22 07:25:50,282 - transformer_training - INFO - Main loop iteration: 3389
2025-03-22 07:25:50,542 - transformer_training - INFO - Main loop iteration: 3390
Iter 3390: loss 4.6361, lr 0.000493, 127075.21 tokens/sec
2025-03-22 07:25:50,801 - transformer_training - INFO - Main loop iteration: 3391
2025-03-22 07:25:51,077 - transformer_training - INFO - Main loop iteration: 3392
2025-03-22 07:25:51,327 - transformer_training - INFO - Main loop iteration: 3393
2025-03-22 07:25:51,587 - transformer_training - INFO - Main loop iteration: 3394
2025-03-22 07:25:51,851 - transformer_training - INFO - Main loop iteration: 3395
2025-03-22 07:25:52,136 - transformer_training - INFO - Main loop iteration: 3396
2025-03-22 07:25:52,387 - transformer_training - INFO - Main loop iteration: 3397
2025-03-22 07:25:52,648 - transformer_training - INFO - Main loop iteration: 3398
2025-03-22 07:25:52,908 - transformer_training - INFO - Main loop iteration: 3399
2025-03-22 07:25:53,191 - transformer_training - INFO - Main loop iteration: 3400
Iter 3400: loss 4.4721, lr 0.000493, 130892.00 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 3400: train loss 4.2655, val loss 4.3896
New best model saved with val loss: 4.3896
2025-03-22 07:26:13,309 - transformer_training - INFO - Main loop iteration: 3401
2025-03-22 07:26:13,569 - transformer_training - INFO - Main loop iteration: 3402
2025-03-22 07:26:13,830 - transformer_training - INFO - Main loop iteration: 3403
2025-03-22 07:26:14,108 - transformer_training - INFO - Main loop iteration: 3404
2025-03-22 07:26:14,485 - transformer_training - INFO - Main loop iteration: 3405
2025-03-22 07:26:14,748 - transformer_training - INFO - Main loop iteration: 3406
2025-03-22 07:26:15,007 - transformer_training - INFO - Main loop iteration: 3407
2025-03-22 07:26:15,285 - transformer_training - INFO - Main loop iteration: 3408
2025-03-22 07:26:15,537 - transformer_training - INFO - Main loop iteration: 3409
2025-03-22 07:26:15,800 - transformer_training - INFO - Main loop iteration: 3410
Iter 3410: loss 4.5473, lr 0.000493, 126437.15 tokens/sec
2025-03-22 07:26:16,060 - transformer_training - INFO - Main loop iteration: 3411
2025-03-22 07:26:16,338 - transformer_training - INFO - Main loop iteration: 3412
2025-03-22 07:26:16,589 - transformer_training - INFO - Main loop iteration: 3413
2025-03-22 07:26:16,851 - transformer_training - INFO - Main loop iteration: 3414
2025-03-22 07:26:17,111 - transformer_training - INFO - Main loop iteration: 3415
2025-03-22 07:26:17,389 - transformer_training - INFO - Main loop iteration: 3416
2025-03-22 07:26:17,641 - transformer_training - INFO - Main loop iteration: 3417
2025-03-22 07:26:17,902 - transformer_training - INFO - Main loop iteration: 3418
2025-03-22 07:26:18,168 - transformer_training - INFO - Main loop iteration: 3419
2025-03-22 07:26:18,447 - transformer_training - INFO - Main loop iteration: 3420
Iter 3420: loss 4.6495, lr 0.000492, 130768.21 tokens/sec
2025-03-22 07:26:18,698 - transformer_training - INFO - Main loop iteration: 3421
2025-03-22 07:26:18,959 - transformer_training - INFO - Main loop iteration: 3422
2025-03-22 07:26:19,219 - transformer_training - INFO - Main loop iteration: 3423
2025-03-22 07:26:19,498 - transformer_training - INFO - Main loop iteration: 3424
2025-03-22 07:26:19,749 - transformer_training - INFO - Main loop iteration: 3425
2025-03-22 07:26:20,010 - transformer_training - INFO - Main loop iteration: 3426
2025-03-22 07:26:20,276 - transformer_training - INFO - Main loop iteration: 3427
2025-03-22 07:26:20,555 - transformer_training - INFO - Main loop iteration: 3428
2025-03-22 07:26:20,807 - transformer_training - INFO - Main loop iteration: 3429
2025-03-22 07:26:21,068 - transformer_training - INFO - Main loop iteration: 3430
Iter 3430: loss 4.6137, lr 0.000492, 126360.55 tokens/sec
2025-03-22 07:26:21,328 - transformer_training - INFO - Main loop iteration: 3431
2025-03-22 07:26:21,607 - transformer_training - INFO - Main loop iteration: 3432
2025-03-22 07:26:21,858 - transformer_training - INFO - Main loop iteration: 3433
2025-03-22 07:26:22,120 - transformer_training - INFO - Main loop iteration: 3434
2025-03-22 07:26:22,380 - transformer_training - INFO - Main loop iteration: 3435
2025-03-22 07:26:22,658 - transformer_training - INFO - Main loop iteration: 3436
2025-03-22 07:26:22,910 - transformer_training - INFO - Main loop iteration: 3437
2025-03-22 07:26:23,171 - transformer_training - INFO - Main loop iteration: 3438
2025-03-22 07:26:23,431 - transformer_training - INFO - Main loop iteration: 3439
2025-03-22 07:26:23,709 - transformer_training - INFO - Main loop iteration: 3440
Iter 3440: loss 4.6892, lr 0.000492, 130804.92 tokens/sec
2025-03-22 07:26:23,960 - transformer_training - INFO - Main loop iteration: 3441
2025-03-22 07:26:24,222 - transformer_training - INFO - Main loop iteration: 3442
2025-03-22 07:26:24,482 - transformer_training - INFO - Main loop iteration: 3443
2025-03-22 07:26:24,760 - transformer_training - INFO - Main loop iteration: 3444
2025-03-22 07:26:25,011 - transformer_training - INFO - Main loop iteration: 3445
2025-03-22 07:26:25,272 - transformer_training - INFO - Main loop iteration: 3446
2025-03-22 07:26:25,532 - transformer_training - INFO - Main loop iteration: 3447
2025-03-22 07:26:25,810 - transformer_training - INFO - Main loop iteration: 3448
2025-03-22 07:26:26,062 - transformer_training - INFO - Main loop iteration: 3449
2025-03-22 07:26:26,323 - transformer_training - INFO - Main loop iteration: 3450
Iter 3450: loss 4.4931, lr 0.000492, 126168.34 tokens/sec
2025-03-22 07:26:26,584 - transformer_training - INFO - Main loop iteration: 3451
2025-03-22 07:26:26,862 - transformer_training - INFO - Main loop iteration: 3452
2025-03-22 07:26:27,113 - transformer_training - INFO - Main loop iteration: 3453
2025-03-22 07:26:27,375 - transformer_training - INFO - Main loop iteration: 3454
2025-03-22 07:26:27,635 - transformer_training - INFO - Main loop iteration: 3455
2025-03-22 07:26:27,913 - transformer_training - INFO - Main loop iteration: 3456
2025-03-22 07:26:28,164 - transformer_training - INFO - Main loop iteration: 3457
2025-03-22 07:26:28,426 - transformer_training - INFO - Main loop iteration: 3458
2025-03-22 07:26:28,686 - transformer_training - INFO - Main loop iteration: 3459
2025-03-22 07:26:28,965 - transformer_training - INFO - Main loop iteration: 3460
Iter 3460: loss 4.5227, lr 0.000492, 130726.54 tokens/sec
2025-03-22 07:26:29,217 - transformer_training - INFO - Main loop iteration: 3461
2025-03-22 07:26:29,478 - transformer_training - INFO - Main loop iteration: 3462
2025-03-22 07:26:29,738 - transformer_training - INFO - Main loop iteration: 3463
2025-03-22 07:26:30,017 - transformer_training - INFO - Main loop iteration: 3464
2025-03-22 07:26:30,271 - transformer_training - INFO - Main loop iteration: 3465
2025-03-22 07:26:30,534 - transformer_training - INFO - Main loop iteration: 3466
2025-03-22 07:26:30,797 - transformer_training - INFO - Main loop iteration: 3467
2025-03-22 07:26:31,075 - transformer_training - INFO - Main loop iteration: 3468
2025-03-22 07:26:31,327 - transformer_training - INFO - Main loop iteration: 3469
2025-03-22 07:26:31,589 - transformer_training - INFO - Main loop iteration: 3470
Iter 3470: loss 4.5368, lr 0.000492, 126125.96 tokens/sec
2025-03-22 07:26:31,850 - transformer_training - INFO - Main loop iteration: 3471
2025-03-22 07:26:32,133 - transformer_training - INFO - Main loop iteration: 3472
2025-03-22 07:26:32,385 - transformer_training - INFO - Main loop iteration: 3473
2025-03-22 07:26:32,646 - transformer_training - INFO - Main loop iteration: 3474
2025-03-22 07:26:32,907 - transformer_training - INFO - Main loop iteration: 3475
2025-03-22 07:26:33,185 - transformer_training - INFO - Main loop iteration: 3476
2025-03-22 07:26:33,439 - transformer_training - INFO - Main loop iteration: 3477
2025-03-22 07:26:33,701 - transformer_training - INFO - Main loop iteration: 3478
2025-03-22 07:26:33,962 - transformer_training - INFO - Main loop iteration: 3479
2025-03-22 07:26:34,240 - transformer_training - INFO - Main loop iteration: 3480
Iter 3480: loss 4.5359, lr 0.000492, 130723.06 tokens/sec
2025-03-22 07:26:34,491 - transformer_training - INFO - Main loop iteration: 3481
2025-03-22 07:26:34,752 - transformer_training - INFO - Main loop iteration: 3482
2025-03-22 07:26:35,013 - transformer_training - INFO - Main loop iteration: 3483
2025-03-22 07:26:35,293 - transformer_training - INFO - Main loop iteration: 3484
2025-03-22 07:26:35,546 - transformer_training - INFO - Main loop iteration: 3485
2025-03-22 07:26:35,807 - transformer_training - INFO - Main loop iteration: 3486
2025-03-22 07:26:36,066 - transformer_training - INFO - Main loop iteration: 3487
2025-03-22 07:26:36,344 - transformer_training - INFO - Main loop iteration: 3488
2025-03-22 07:26:36,593 - transformer_training - INFO - Main loop iteration: 3489
2025-03-22 07:26:36,854 - transformer_training - INFO - Main loop iteration: 3490
Iter 3490: loss 4.5556, lr 0.000492, 126908.47 tokens/sec
2025-03-22 07:26:37,113 - transformer_training - INFO - Main loop iteration: 3491
2025-03-22 07:26:37,390 - transformer_training - INFO - Main loop iteration: 3492
2025-03-22 07:26:37,640 - transformer_training - INFO - Main loop iteration: 3493
2025-03-22 07:26:37,901 - transformer_training - INFO - Main loop iteration: 3494
2025-03-22 07:26:38,160 - transformer_training - INFO - Main loop iteration: 3495
2025-03-22 07:26:38,437 - transformer_training - INFO - Main loop iteration: 3496
2025-03-22 07:26:38,691 - transformer_training - INFO - Main loop iteration: 3497
2025-03-22 07:26:38,951 - transformer_training - INFO - Main loop iteration: 3498
2025-03-22 07:26:39,210 - transformer_training - INFO - Main loop iteration: 3499
2025-03-22 07:26:39,487 - transformer_training - INFO - Main loop iteration: 3500
Iter 3500: loss 4.5135, lr 0.000492, 128861.06 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 3500: train loss 4.2844, val loss 4.3536
New best model saved with val loss: 4.3536
2025-03-22 07:27:04,199 - transformer_training - INFO - Main loop iteration: 3501
2025-03-22 07:27:04,475 - transformer_training - INFO - Main loop iteration: 3502
2025-03-22 07:27:04,735 - transformer_training - INFO - Main loop iteration: 3503
2025-03-22 07:27:05,011 - transformer_training - INFO - Main loop iteration: 3504
2025-03-22 07:27:05,260 - transformer_training - INFO - Main loop iteration: 3505
2025-03-22 07:27:05,530 - transformer_training - INFO - Main loop iteration: 3506
2025-03-22 07:27:05,789 - transformer_training - INFO - Main loop iteration: 3507
2025-03-22 07:27:06,066 - transformer_training - INFO - Main loop iteration: 3508
2025-03-22 07:27:06,316 - transformer_training - INFO - Main loop iteration: 3509
2025-03-22 07:27:06,576 - transformer_training - INFO - Main loop iteration: 3510
Iter 3510: loss 4.4965, lr 0.000491, 126875.55 tokens/sec
2025-03-22 07:27:06,835 - transformer_training - INFO - Main loop iteration: 3511
2025-03-22 07:27:07,112 - transformer_training - INFO - Main loop iteration: 3512
2025-03-22 07:27:07,361 - transformer_training - INFO - Main loop iteration: 3513
2025-03-22 07:27:07,621 - transformer_training - INFO - Main loop iteration: 3514
2025-03-22 07:27:07,880 - transformer_training - INFO - Main loop iteration: 3515
2025-03-22 07:27:08,156 - transformer_training - INFO - Main loop iteration: 3516
2025-03-22 07:27:08,406 - transformer_training - INFO - Main loop iteration: 3517
2025-03-22 07:27:08,666 - transformer_training - INFO - Main loop iteration: 3518
2025-03-22 07:27:08,925 - transformer_training - INFO - Main loop iteration: 3519
2025-03-22 07:27:09,201 - transformer_training - INFO - Main loop iteration: 3520
Iter 3520: loss 4.4644, lr 0.000491, 131547.47 tokens/sec
2025-03-22 07:27:09,451 - transformer_training - INFO - Main loop iteration: 3521
2025-03-22 07:27:09,711 - transformer_training - INFO - Main loop iteration: 3522
2025-03-22 07:27:09,970 - transformer_training - INFO - Main loop iteration: 3523
2025-03-22 07:27:10,247 - transformer_training - INFO - Main loop iteration: 3524
2025-03-22 07:27:10,497 - transformer_training - INFO - Main loop iteration: 3525
2025-03-22 07:27:10,757 - transformer_training - INFO - Main loop iteration: 3526
2025-03-22 07:27:11,016 - transformer_training - INFO - Main loop iteration: 3527
2025-03-22 07:27:11,293 - transformer_training - INFO - Main loop iteration: 3528
2025-03-22 07:27:11,542 - transformer_training - INFO - Main loop iteration: 3529
2025-03-22 07:27:11,802 - transformer_training - INFO - Main loop iteration: 3530
Iter 3530: loss 4.4719, lr 0.000491, 126868.29 tokens/sec
2025-03-22 07:27:12,061 - transformer_training - INFO - Main loop iteration: 3531
2025-03-22 07:27:12,338 - transformer_training - INFO - Main loop iteration: 3532
2025-03-22 07:27:12,588 - transformer_training - INFO - Main loop iteration: 3533
2025-03-22 07:27:12,848 - transformer_training - INFO - Main loop iteration: 3534
2025-03-22 07:27:13,107 - transformer_training - INFO - Main loop iteration: 3535
2025-03-22 07:27:13,384 - transformer_training - INFO - Main loop iteration: 3536
2025-03-22 07:27:13,633 - transformer_training - INFO - Main loop iteration: 3537
2025-03-22 07:27:13,893 - transformer_training - INFO - Main loop iteration: 3538
2025-03-22 07:27:14,152 - transformer_training - INFO - Main loop iteration: 3539
2025-03-22 07:27:14,431 - transformer_training - INFO - Main loop iteration: 3540
Iter 3540: loss 4.5283, lr 0.000491, 129372.94 tokens/sec
2025-03-22 07:27:14,685 - transformer_training - INFO - Main loop iteration: 3541
2025-03-22 07:27:14,946 - transformer_training - INFO - Main loop iteration: 3542
2025-03-22 07:27:15,206 - transformer_training - INFO - Main loop iteration: 3543
2025-03-22 07:27:15,483 - transformer_training - INFO - Main loop iteration: 3544
2025-03-22 07:27:15,734 - transformer_training - INFO - Main loop iteration: 3545
2025-03-22 07:27:15,996 - transformer_training - INFO - Main loop iteration: 3546
2025-03-22 07:27:16,256 - transformer_training - INFO - Main loop iteration: 3547
2025-03-22 07:27:16,534 - transformer_training - INFO - Main loop iteration: 3548
2025-03-22 07:27:16,784 - transformer_training - INFO - Main loop iteration: 3549
2025-03-22 07:27:17,046 - transformer_training - INFO - Main loop iteration: 3550
Iter 3550: loss 4.4646, lr 0.000491, 126403.66 tokens/sec
2025-03-22 07:27:17,306 - transformer_training - INFO - Main loop iteration: 3551
2025-03-22 07:27:17,588 - transformer_training - INFO - Main loop iteration: 3552
2025-03-22 07:27:17,839 - transformer_training - INFO - Main loop iteration: 3553
2025-03-22 07:27:18,100 - transformer_training - INFO - Main loop iteration: 3554
2025-03-22 07:27:18,360 - transformer_training - INFO - Main loop iteration: 3555
2025-03-22 07:27:18,638 - transformer_training - INFO - Main loop iteration: 3556
2025-03-22 07:27:18,892 - transformer_training - INFO - Main loop iteration: 3557
2025-03-22 07:27:19,153 - transformer_training - INFO - Main loop iteration: 3558
2025-03-22 07:27:19,413 - transformer_training - INFO - Main loop iteration: 3559
2025-03-22 07:27:19,696 - transformer_training - INFO - Main loop iteration: 3560
Iter 3560: loss 4.4976, lr 0.000491, 130709.01 tokens/sec
2025-03-22 07:27:19,947 - transformer_training - INFO - Main loop iteration: 3561
2025-03-22 07:27:20,209 - transformer_training - INFO - Main loop iteration: 3562
2025-03-22 07:27:20,469 - transformer_training - INFO - Main loop iteration: 3563
2025-03-22 07:27:20,752 - transformer_training - INFO - Main loop iteration: 3564
2025-03-22 07:27:21,002 - transformer_training - INFO - Main loop iteration: 3565
2025-03-22 07:27:21,264 - transformer_training - INFO - Main loop iteration: 3566
2025-03-22 07:27:21,524 - transformer_training - INFO - Main loop iteration: 3567
2025-03-22 07:27:21,923 - transformer_training - INFO - Main loop iteration: 3568
2025-03-22 07:27:22,174 - transformer_training - INFO - Main loop iteration: 3569
2025-03-22 07:27:22,435 - transformer_training - INFO - Main loop iteration: 3570
Iter 3570: loss 4.4388, lr 0.000491, 125624.47 tokens/sec
2025-03-22 07:27:22,697 - transformer_training - INFO - Main loop iteration: 3571
2025-03-22 07:27:22,975 - transformer_training - INFO - Main loop iteration: 3572
2025-03-22 07:27:23,226 - transformer_training - INFO - Main loop iteration: 3573
2025-03-22 07:27:23,487 - transformer_training - INFO - Main loop iteration: 3574
2025-03-22 07:27:23,747 - transformer_training - INFO - Main loop iteration: 3575
2025-03-22 07:27:24,024 - transformer_training - INFO - Main loop iteration: 3576
2025-03-22 07:27:24,275 - transformer_training - INFO - Main loop iteration: 3577
2025-03-22 07:27:24,536 - transformer_training - INFO - Main loop iteration: 3578
2025-03-22 07:27:24,796 - transformer_training - INFO - Main loop iteration: 3579
2025-03-22 07:27:25,073 - transformer_training - INFO - Main loop iteration: 3580
Iter 3580: loss 4.4167, lr 0.000491, 131001.91 tokens/sec
2025-03-22 07:27:25,324 - transformer_training - INFO - Main loop iteration: 3581
2025-03-22 07:27:25,586 - transformer_training - INFO - Main loop iteration: 3582
2025-03-22 07:27:25,845 - transformer_training - INFO - Main loop iteration: 3583
2025-03-22 07:27:26,123 - transformer_training - INFO - Main loop iteration: 3584
2025-03-22 07:27:26,374 - transformer_training - INFO - Main loop iteration: 3585
2025-03-22 07:27:26,636 - transformer_training - INFO - Main loop iteration: 3586
2025-03-22 07:27:26,896 - transformer_training - INFO - Main loop iteration: 3587
2025-03-22 07:27:27,174 - transformer_training - INFO - Main loop iteration: 3588
2025-03-22 07:27:27,424 - transformer_training - INFO - Main loop iteration: 3589
2025-03-22 07:27:27,686 - transformer_training - INFO - Main loop iteration: 3590
Iter 3590: loss 4.6020, lr 0.000491, 124314.23 tokens/sec
2025-03-22 07:27:27,950 - transformer_training - INFO - Main loop iteration: 3591
2025-03-22 07:27:28,228 - transformer_training - INFO - Main loop iteration: 3592
2025-03-22 07:27:28,479 - transformer_training - INFO - Main loop iteration: 3593
2025-03-22 07:27:28,740 - transformer_training - INFO - Main loop iteration: 3594
2025-03-22 07:27:29,000 - transformer_training - INFO - Main loop iteration: 3595
2025-03-22 07:27:29,282 - transformer_training - INFO - Main loop iteration: 3596
2025-03-22 07:27:29,533 - transformer_training - INFO - Main loop iteration: 3597
2025-03-22 07:27:29,795 - transformer_training - INFO - Main loop iteration: 3598
2025-03-22 07:27:30,054 - transformer_training - INFO - Main loop iteration: 3599
2025-03-22 07:27:30,333 - transformer_training - INFO - Main loop iteration: 3600
Iter 3600: loss 4.4457, lr 0.000490, 130713.36 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 3600: train loss 4.2838, val loss 4.3214
New best model saved with val loss: 4.3214
2025-03-22 07:27:50,438 - transformer_training - INFO - Main loop iteration: 3601
2025-03-22 07:27:50,716 - transformer_training - INFO - Main loop iteration: 3602
2025-03-22 07:27:50,977 - transformer_training - INFO - Main loop iteration: 3603
2025-03-22 07:27:51,255 - transformer_training - INFO - Main loop iteration: 3604
2025-03-22 07:27:51,509 - transformer_training - INFO - Main loop iteration: 3605
2025-03-22 07:27:51,771 - transformer_training - INFO - Main loop iteration: 3606
2025-03-22 07:27:52,037 - transformer_training - INFO - Main loop iteration: 3607
2025-03-22 07:27:52,314 - transformer_training - INFO - Main loop iteration: 3608
2025-03-22 07:27:52,565 - transformer_training - INFO - Main loop iteration: 3609
2025-03-22 07:27:52,827 - transformer_training - INFO - Main loop iteration: 3610
Iter 3610: loss 4.3835, lr 0.000490, 126603.82 tokens/sec
2025-03-22 07:27:53,087 - transformer_training - INFO - Main loop iteration: 3611
2025-03-22 07:27:53,364 - transformer_training - INFO - Main loop iteration: 3612
2025-03-22 07:27:53,615 - transformer_training - INFO - Main loop iteration: 3613
2025-03-22 07:27:53,876 - transformer_training - INFO - Main loop iteration: 3614
2025-03-22 07:27:54,136 - transformer_training - INFO - Main loop iteration: 3615
2025-03-22 07:27:54,414 - transformer_training - INFO - Main loop iteration: 3616
2025-03-22 07:27:54,665 - transformer_training - INFO - Main loop iteration: 3617
2025-03-22 07:27:54,926 - transformer_training - INFO - Main loop iteration: 3618
2025-03-22 07:27:55,186 - transformer_training - INFO - Main loop iteration: 3619
2025-03-22 07:27:55,464 - transformer_training - INFO - Main loop iteration: 3620
Iter 3620: loss 4.4882, lr 0.000490, 130962.22 tokens/sec
2025-03-22 07:27:55,715 - transformer_training - INFO - Main loop iteration: 3621
2025-03-22 07:27:55,976 - transformer_training - INFO - Main loop iteration: 3622
2025-03-22 07:27:56,235 - transformer_training - INFO - Main loop iteration: 3623
2025-03-22 07:27:56,513 - transformer_training - INFO - Main loop iteration: 3624
2025-03-22 07:27:56,763 - transformer_training - INFO - Main loop iteration: 3625
2025-03-22 07:27:57,025 - transformer_training - INFO - Main loop iteration: 3626
2025-03-22 07:27:57,289 - transformer_training - INFO - Main loop iteration: 3627
2025-03-22 07:27:57,566 - transformer_training - INFO - Main loop iteration: 3628
2025-03-22 07:27:57,820 - transformer_training - INFO - Main loop iteration: 3629
2025-03-22 07:27:58,081 - transformer_training - INFO - Main loop iteration: 3630
Iter 3630: loss 4.4309, lr 0.000490, 126511.99 tokens/sec
2025-03-22 07:27:58,341 - transformer_training - INFO - Main loop iteration: 3631
2025-03-22 07:27:58,619 - transformer_training - INFO - Main loop iteration: 3632
2025-03-22 07:27:58,870 - transformer_training - INFO - Main loop iteration: 3633
2025-03-22 07:27:59,131 - transformer_training - INFO - Main loop iteration: 3634
2025-03-22 07:27:59,391 - transformer_training - INFO - Main loop iteration: 3635
2025-03-22 07:27:59,669 - transformer_training - INFO - Main loop iteration: 3636
2025-03-22 07:27:59,920 - transformer_training - INFO - Main loop iteration: 3637
2025-03-22 07:28:00,181 - transformer_training - INFO - Main loop iteration: 3638
2025-03-22 07:28:00,440 - transformer_training - INFO - Main loop iteration: 3639
2025-03-22 07:28:00,718 - transformer_training - INFO - Main loop iteration: 3640
Iter 3640: loss 4.5080, lr 0.000490, 130901.10 tokens/sec
2025-03-22 07:28:00,969 - transformer_training - INFO - Main loop iteration: 3641
2025-03-22 07:28:01,231 - transformer_training - INFO - Main loop iteration: 3642
2025-03-22 07:28:01,490 - transformer_training - INFO - Main loop iteration: 3643
2025-03-22 07:28:01,768 - transformer_training - INFO - Main loop iteration: 3644
2025-03-22 07:28:02,019 - transformer_training - INFO - Main loop iteration: 3645
2025-03-22 07:28:02,280 - transformer_training - INFO - Main loop iteration: 3646
2025-03-22 07:28:02,540 - transformer_training - INFO - Main loop iteration: 3647
2025-03-22 07:28:02,818 - transformer_training - INFO - Main loop iteration: 3648
2025-03-22 07:28:03,069 - transformer_training - INFO - Main loop iteration: 3649
2025-03-22 07:28:03,330 - transformer_training - INFO - Main loop iteration: 3650
Iter 3650: loss 4.4394, lr 0.000490, 126507.33 tokens/sec
2025-03-22 07:28:03,590 - transformer_training - INFO - Main loop iteration: 3651
2025-03-22 07:28:03,868 - transformer_training - INFO - Main loop iteration: 3652
2025-03-22 07:28:04,118 - transformer_training - INFO - Main loop iteration: 3653
2025-03-22 07:28:04,380 - transformer_training - INFO - Main loop iteration: 3654
2025-03-22 07:28:04,640 - transformer_training - INFO - Main loop iteration: 3655
2025-03-22 07:28:04,917 - transformer_training - INFO - Main loop iteration: 3656
2025-03-22 07:28:05,168 - transformer_training - INFO - Main loop iteration: 3657
2025-03-22 07:28:05,429 - transformer_training - INFO - Main loop iteration: 3658
2025-03-22 07:28:05,689 - transformer_training - INFO - Main loop iteration: 3659
2025-03-22 07:28:05,967 - transformer_training - INFO - Main loop iteration: 3660
Iter 3660: loss 4.5065, lr 0.000490, 130831.44 tokens/sec
2025-03-22 07:28:06,218 - transformer_training - INFO - Main loop iteration: 3661
2025-03-22 07:28:06,479 - transformer_training - INFO - Main loop iteration: 3662
2025-03-22 07:28:06,739 - transformer_training - INFO - Main loop iteration: 3663
2025-03-22 07:28:07,016 - transformer_training - INFO - Main loop iteration: 3664
2025-03-22 07:28:07,267 - transformer_training - INFO - Main loop iteration: 3665
2025-03-22 07:28:07,529 - transformer_training - INFO - Main loop iteration: 3666
2025-03-22 07:28:07,788 - transformer_training - INFO - Main loop iteration: 3667
2025-03-22 07:28:08,066 - transformer_training - INFO - Main loop iteration: 3668
2025-03-22 07:28:08,316 - transformer_training - INFO - Main loop iteration: 3669
2025-03-22 07:28:08,578 - transformer_training - INFO - Main loop iteration: 3670
Iter 3670: loss 4.4026, lr 0.000490, 126413.66 tokens/sec
2025-03-22 07:28:08,838 - transformer_training - INFO - Main loop iteration: 3671
2025-03-22 07:28:09,115 - transformer_training - INFO - Main loop iteration: 3672
2025-03-22 07:28:09,366 - transformer_training - INFO - Main loop iteration: 3673
2025-03-22 07:28:09,628 - transformer_training - INFO - Main loop iteration: 3674
2025-03-22 07:28:09,889 - transformer_training - INFO - Main loop iteration: 3675
2025-03-22 07:28:10,190 - transformer_training - INFO - Main loop iteration: 3676
2025-03-22 07:28:10,441 - transformer_training - INFO - Main loop iteration: 3677
2025-03-22 07:28:10,702 - transformer_training - INFO - Main loop iteration: 3678
2025-03-22 07:28:10,962 - transformer_training - INFO - Main loop iteration: 3679
2025-03-22 07:28:11,240 - transformer_training - INFO - Main loop iteration: 3680
Iter 3680: loss 4.4890, lr 0.000489, 131036.88 tokens/sec
2025-03-22 07:28:11,491 - transformer_training - INFO - Main loop iteration: 3681
2025-03-22 07:28:11,752 - transformer_training - INFO - Main loop iteration: 3682
2025-03-22 07:28:12,011 - transformer_training - INFO - Main loop iteration: 3683
2025-03-22 07:28:12,289 - transformer_training - INFO - Main loop iteration: 3684
2025-03-22 07:28:12,540 - transformer_training - INFO - Main loop iteration: 3685
2025-03-22 07:28:12,801 - transformer_training - INFO - Main loop iteration: 3686
2025-03-22 07:28:13,061 - transformer_training - INFO - Main loop iteration: 3687
2025-03-22 07:28:13,344 - transformer_training - INFO - Main loop iteration: 3688
2025-03-22 07:28:13,594 - transformer_training - INFO - Main loop iteration: 3689
2025-03-22 07:28:13,856 - transformer_training - INFO - Main loop iteration: 3690
Iter 3690: loss 4.5096, lr 0.000489, 126314.56 tokens/sec
2025-03-22 07:28:14,116 - transformer_training - INFO - Main loop iteration: 3691
2025-03-22 07:28:14,405 - transformer_training - INFO - Main loop iteration: 3692
2025-03-22 07:28:14,656 - transformer_training - INFO - Main loop iteration: 3693
2025-03-22 07:28:14,917 - transformer_training - INFO - Main loop iteration: 3694
2025-03-22 07:28:15,177 - transformer_training - INFO - Main loop iteration: 3695
2025-03-22 07:28:15,455 - transformer_training - INFO - Main loop iteration: 3696
2025-03-22 07:28:15,705 - transformer_training - INFO - Main loop iteration: 3697
2025-03-22 07:28:15,966 - transformer_training - INFO - Main loop iteration: 3698
2025-03-22 07:28:16,231 - transformer_training - INFO - Main loop iteration: 3699
2025-03-22 07:28:16,509 - transformer_training - INFO - Main loop iteration: 3700
Iter 3700: loss 4.4453, lr 0.000489, 130932.40 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPTYour base folder is: /workspace/GPT

Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPTYour base folder is: /workspace/GPT

Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 3700: train loss 4.2795, val loss 4.2975
New best model saved with val loss: 4.2975
2025-03-22 07:28:37,165 - transformer_training - INFO - Main loop iteration: 3701
2025-03-22 07:28:37,442 - transformer_training - INFO - Main loop iteration: 3702
2025-03-22 07:28:37,704 - transformer_training - INFO - Main loop iteration: 3703
2025-03-22 07:28:37,981 - transformer_training - INFO - Main loop iteration: 3704
2025-03-22 07:28:38,235 - transformer_training - INFO - Main loop iteration: 3705
2025-03-22 07:28:38,497 - transformer_training - INFO - Main loop iteration: 3706
2025-03-22 07:28:38,757 - transformer_training - INFO - Main loop iteration: 3707
2025-03-22 07:28:39,036 - transformer_training - INFO - Main loop iteration: 3708
2025-03-22 07:28:39,288 - transformer_training - INFO - Main loop iteration: 3709
2025-03-22 07:28:39,549 - transformer_training - INFO - Main loop iteration: 3710
Iter 3710: loss 4.4690, lr 0.000489, 126487.89 tokens/sec
2025-03-22 07:28:39,809 - transformer_training - INFO - Main loop iteration: 3711
2025-03-22 07:28:40,087 - transformer_training - INFO - Main loop iteration: 3712
2025-03-22 07:28:40,338 - transformer_training - INFO - Main loop iteration: 3713
2025-03-22 07:28:40,600 - transformer_training - INFO - Main loop iteration: 3714
2025-03-22 07:28:40,865 - transformer_training - INFO - Main loop iteration: 3715
2025-03-22 07:28:41,143 - transformer_training - INFO - Main loop iteration: 3716
2025-03-22 07:28:41,394 - transformer_training - INFO - Main loop iteration: 3717
2025-03-22 07:28:41,656 - transformer_training - INFO - Main loop iteration: 3718
2025-03-22 07:28:41,916 - transformer_training - INFO - Main loop iteration: 3719
2025-03-22 07:28:42,196 - transformer_training - INFO - Main loop iteration: 3720
Iter 3720: loss 4.5402, lr 0.000489, 124840.55 tokens/sec
2025-03-22 07:28:42,460 - transformer_training - INFO - Main loop iteration: 3721
2025-03-22 07:28:42,721 - transformer_training - INFO - Main loop iteration: 3722
2025-03-22 07:28:42,982 - transformer_training - INFO - Main loop iteration: 3723
2025-03-22 07:28:43,260 - transformer_training - INFO - Main loop iteration: 3724
2025-03-22 07:28:43,511 - transformer_training - INFO - Main loop iteration: 3725
2025-03-22 07:28:43,773 - transformer_training - INFO - Main loop iteration: 3726
2025-03-22 07:28:44,034 - transformer_training - INFO - Main loop iteration: 3727
2025-03-22 07:28:44,312 - transformer_training - INFO - Main loop iteration: 3728
2025-03-22 07:28:44,563 - transformer_training - INFO - Main loop iteration: 3729
2025-03-22 07:28:44,825 - transformer_training - INFO - Main loop iteration: 3730
Iter 3730: loss 4.4234, lr 0.000489, 126226.16 tokens/sec
2025-03-22 07:28:45,086 - transformer_training - INFO - Main loop iteration: 3731
2025-03-22 07:28:45,363 - transformer_training - INFO - Main loop iteration: 3732
2025-03-22 07:28:45,741 - transformer_training - INFO - Main loop iteration: 3733
2025-03-22 07:28:46,005 - transformer_training - INFO - Main loop iteration: 3734
2025-03-22 07:28:46,265 - transformer_training - INFO - Main loop iteration: 3735
2025-03-22 07:28:46,543 - transformer_training - INFO - Main loop iteration: 3736
2025-03-22 07:28:46,795 - transformer_training - INFO - Main loop iteration: 3737
2025-03-22 07:28:47,057 - transformer_training - INFO - Main loop iteration: 3738
2025-03-22 07:28:47,317 - transformer_training - INFO - Main loop iteration: 3739
2025-03-22 07:28:47,595 - transformer_training - INFO - Main loop iteration: 3740
Iter 3740: loss 4.3327, lr 0.000489, 130616.22 tokens/sec
2025-03-22 07:28:47,847 - transformer_training - INFO - Main loop iteration: 3741
2025-03-22 07:28:48,108 - transformer_training - INFO - Main loop iteration: 3742
2025-03-22 07:28:48,368 - transformer_training - INFO - Main loop iteration: 3743
2025-03-22 07:28:48,651 - transformer_training - INFO - Main loop iteration: 3744
2025-03-22 07:28:48,902 - transformer_training - INFO - Main loop iteration: 3745
2025-03-22 07:28:49,164 - transformer_training - INFO - Main loop iteration: 3746
2025-03-22 07:28:49,424 - transformer_training - INFO - Main loop iteration: 3747
2025-03-22 07:28:49,702 - transformer_training - INFO - Main loop iteration: 3748
2025-03-22 07:28:49,953 - transformer_training - INFO - Main loop iteration: 3749
2025-03-22 07:28:50,215 - transformer_training - INFO - Main loop iteration: 3750
Iter 3750: loss 4.4992, lr 0.000489, 126442.62 tokens/sec
2025-03-22 07:28:50,475 - transformer_training - INFO - Main loop iteration: 3751
2025-03-22 07:28:50,753 - transformer_training - INFO - Main loop iteration: 3752
2025-03-22 07:28:51,004 - transformer_training - INFO - Main loop iteration: 3753
2025-03-22 07:28:51,267 - transformer_training - INFO - Main loop iteration: 3754
2025-03-22 07:28:51,527 - transformer_training - INFO - Main loop iteration: 3755
2025-03-22 07:28:51,805 - transformer_training - INFO - Main loop iteration: 3756
2025-03-22 07:28:52,056 - transformer_training - INFO - Main loop iteration: 3757
2025-03-22 07:28:52,318 - transformer_training - INFO - Main loop iteration: 3758
2025-03-22 07:28:52,579 - transformer_training - INFO - Main loop iteration: 3759
2025-03-22 07:28:52,856 - transformer_training - INFO - Main loop iteration: 3760
Iter 3760: loss 4.4690, lr 0.000488, 130377.45 tokens/sec
2025-03-22 07:28:53,109 - transformer_training - INFO - Main loop iteration: 3761
2025-03-22 07:28:53,371 - transformer_training - INFO - Main loop iteration: 3762
2025-03-22 07:28:53,631 - transformer_training - INFO - Main loop iteration: 3763
2025-03-22 07:28:53,909 - transformer_training - INFO - Main loop iteration: 3764
2025-03-22 07:28:54,161 - transformer_training - INFO - Main loop iteration: 3765
2025-03-22 07:28:54,422 - transformer_training - INFO - Main loop iteration: 3766
2025-03-22 07:28:54,686 - transformer_training - INFO - Main loop iteration: 3767
2025-03-22 07:28:54,969 - transformer_training - INFO - Main loop iteration: 3768
2025-03-22 07:28:55,220 - transformer_training - INFO - Main loop iteration: 3769
2025-03-22 07:28:55,482 - transformer_training - INFO - Main loop iteration: 3770
Iter 3770: loss 4.4467, lr 0.000488, 124181.80 tokens/sec
2025-03-22 07:28:55,747 - transformer_training - INFO - Main loop iteration: 3771
2025-03-22 07:28:56,024 - transformer_training - INFO - Main loop iteration: 3772
2025-03-22 07:28:56,275 - transformer_training - INFO - Main loop iteration: 3773
2025-03-22 07:28:56,537 - transformer_training - INFO - Main loop iteration: 3774
2025-03-22 07:28:56,797 - transformer_training - INFO - Main loop iteration: 3775
2025-03-22 07:28:57,075 - transformer_training - INFO - Main loop iteration: 3776
2025-03-22 07:28:57,327 - transformer_training - INFO - Main loop iteration: 3777
2025-03-22 07:28:57,588 - transformer_training - INFO - Main loop iteration: 3778
2025-03-22 07:28:57,853 - transformer_training - INFO - Main loop iteration: 3779
2025-03-22 07:28:58,131 - transformer_training - INFO - Main loop iteration: 3780
Iter 3780: loss 4.4306, lr 0.000488, 129137.48 tokens/sec
2025-03-22 07:28:58,385 - transformer_training - INFO - Main loop iteration: 3781
2025-03-22 07:28:58,647 - transformer_training - INFO - Main loop iteration: 3782
2025-03-22 07:28:58,907 - transformer_training - INFO - Main loop iteration: 3783
2025-03-22 07:28:59,185 - transformer_training - INFO - Main loop iteration: 3784
2025-03-22 07:28:59,436 - transformer_training - INFO - Main loop iteration: 3785
2025-03-22 07:28:59,698 - transformer_training - INFO - Main loop iteration: 3786
2025-03-22 07:28:59,962 - transformer_training - INFO - Main loop iteration: 3787
2025-03-22 07:29:00,240 - transformer_training - INFO - Main loop iteration: 3788
2025-03-22 07:29:00,492 - transformer_training - INFO - Main loop iteration: 3789
2025-03-22 07:29:00,753 - transformer_training - INFO - Main loop iteration: 3790
Iter 3790: loss 4.4961, lr 0.000488, 126447.97 tokens/sec
2025-03-22 07:29:01,014 - transformer_training - INFO - Main loop iteration: 3791
2025-03-22 07:29:01,296 - transformer_training - INFO - Main loop iteration: 3792
2025-03-22 07:29:01,548 - transformer_training - INFO - Main loop iteration: 3793
2025-03-22 07:29:01,810 - transformer_training - INFO - Main loop iteration: 3794
2025-03-22 07:29:02,075 - transformer_training - INFO - Main loop iteration: 3795
2025-03-22 07:29:02,353 - transformer_training - INFO - Main loop iteration: 3796
2025-03-22 07:29:02,605 - transformer_training - INFO - Main loop iteration: 3797
2025-03-22 07:29:02,866 - transformer_training - INFO - Main loop iteration: 3798
2025-03-22 07:29:03,126 - transformer_training - INFO - Main loop iteration: 3799
2025-03-22 07:29:03,405 - transformer_training - INFO - Main loop iteration: 3800
Iter 3800: loss 4.4047, lr 0.000488, 130508.31 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 3800: train loss 4.2700, val loss 4.2566
New best model saved with val loss: 4.2566
2025-03-22 07:29:23,626 - transformer_training - INFO - Main loop iteration: 3801
2025-03-22 07:29:23,904 - transformer_training - INFO - Main loop iteration: 3802
2025-03-22 07:29:24,164 - transformer_training - INFO - Main loop iteration: 3803
2025-03-22 07:29:24,441 - transformer_training - INFO - Main loop iteration: 3804
2025-03-22 07:29:24,693 - transformer_training - INFO - Main loop iteration: 3805
2025-03-22 07:29:24,954 - transformer_training - INFO - Main loop iteration: 3806
2025-03-22 07:29:25,215 - transformer_training - INFO - Main loop iteration: 3807
2025-03-22 07:29:25,493 - transformer_training - INFO - Main loop iteration: 3808
2025-03-22 07:29:25,744 - transformer_training - INFO - Main loop iteration: 3809
2025-03-22 07:29:26,006 - transformer_training - INFO - Main loop iteration: 3810
Iter 3810: loss 4.5167, lr 0.000488, 126356.36 tokens/sec
2025-03-22 07:29:26,266 - transformer_training - INFO - Main loop iteration: 3811
2025-03-22 07:29:26,544 - transformer_training - INFO - Main loop iteration: 3812
2025-03-22 07:29:26,795 - transformer_training - INFO - Main loop iteration: 3813
2025-03-22 07:29:27,057 - transformer_training - INFO - Main loop iteration: 3814
2025-03-22 07:29:27,318 - transformer_training - INFO - Main loop iteration: 3815
2025-03-22 07:29:27,595 - transformer_training - INFO - Main loop iteration: 3816
2025-03-22 07:29:27,847 - transformer_training - INFO - Main loop iteration: 3817
2025-03-22 07:29:28,109 - transformer_training - INFO - Main loop iteration: 3818
2025-03-22 07:29:28,369 - transformer_training - INFO - Main loop iteration: 3819
2025-03-22 07:29:28,647 - transformer_training - INFO - Main loop iteration: 3820
Iter 3820: loss 4.3907, lr 0.000488, 130067.51 tokens/sec
2025-03-22 07:29:28,900 - transformer_training - INFO - Main loop iteration: 3821
2025-03-22 07:29:29,161 - transformer_training - INFO - Main loop iteration: 3822
2025-03-22 07:29:29,422 - transformer_training - INFO - Main loop iteration: 3823
2025-03-22 07:29:29,701 - transformer_training - INFO - Main loop iteration: 3824
2025-03-22 07:29:29,953 - transformer_training - INFO - Main loop iteration: 3825
2025-03-22 07:29:30,215 - transformer_training - INFO - Main loop iteration: 3826
2025-03-22 07:29:30,476 - transformer_training - INFO - Main loop iteration: 3827
2025-03-22 07:29:30,754 - transformer_training - INFO - Main loop iteration: 3828
2025-03-22 07:29:31,005 - transformer_training - INFO - Main loop iteration: 3829
2025-03-22 07:29:31,267 - transformer_training - INFO - Main loop iteration: 3830
Iter 3830: loss 4.4975, lr 0.000487, 126401.57 tokens/sec
2025-03-22 07:29:31,527 - transformer_training - INFO - Main loop iteration: 3831
2025-03-22 07:29:31,805 - transformer_training - INFO - Main loop iteration: 3832
2025-03-22 07:29:32,056 - transformer_training - INFO - Main loop iteration: 3833
2025-03-22 07:29:32,318 - transformer_training - INFO - Main loop iteration: 3834
2025-03-22 07:29:32,579 - transformer_training - INFO - Main loop iteration: 3835
2025-03-22 07:29:32,857 - transformer_training - INFO - Main loop iteration: 3836
2025-03-22 07:29:33,109 - transformer_training - INFO - Main loop iteration: 3837
2025-03-22 07:29:33,370 - transformer_training - INFO - Main loop iteration: 3838
2025-03-22 07:29:33,631 - transformer_training - INFO - Main loop iteration: 3839
2025-03-22 07:29:33,909 - transformer_training - INFO - Main loop iteration: 3840
Iter 3840: loss 4.4035, lr 0.000487, 130554.43 tokens/sec
2025-03-22 07:29:34,161 - transformer_training - INFO - Main loop iteration: 3841
2025-03-22 07:29:34,422 - transformer_training - INFO - Main loop iteration: 3842
2025-03-22 07:29:34,682 - transformer_training - INFO - Main loop iteration: 3843
2025-03-22 07:29:34,960 - transformer_training - INFO - Main loop iteration: 3844
2025-03-22 07:29:35,211 - transformer_training - INFO - Main loop iteration: 3845
2025-03-22 07:29:35,473 - transformer_training - INFO - Main loop iteration: 3846
2025-03-22 07:29:35,738 - transformer_training - INFO - Main loop iteration: 3847
2025-03-22 07:29:36,016 - transformer_training - INFO - Main loop iteration: 3848
2025-03-22 07:29:36,267 - transformer_training - INFO - Main loop iteration: 3849
2025-03-22 07:29:36,528 - transformer_training - INFO - Main loop iteration: 3850
Iter 3850: loss 4.3769, lr 0.000487, 126214.80 tokens/sec
2025-03-22 07:29:36,789 - transformer_training - INFO - Main loop iteration: 3851
2025-03-22 07:29:37,067 - transformer_training - INFO - Main loop iteration: 3852
2025-03-22 07:29:37,318 - transformer_training - INFO - Main loop iteration: 3853
2025-03-22 07:29:37,580 - transformer_training - INFO - Main loop iteration: 3854
2025-03-22 07:29:37,840 - transformer_training - INFO - Main loop iteration: 3855
2025-03-22 07:29:38,119 - transformer_training - INFO - Main loop iteration: 3856
2025-03-22 07:29:38,370 - transformer_training - INFO - Main loop iteration: 3857
2025-03-22 07:29:38,631 - transformer_training - INFO - Main loop iteration: 3858
2025-03-22 07:29:38,896 - transformer_training - INFO - Main loop iteration: 3859
2025-03-22 07:29:39,178 - transformer_training - INFO - Main loop iteration: 3860
Iter 3860: loss 4.4159, lr 0.000487, 130861.34 tokens/sec
2025-03-22 07:29:39,429 - transformer_training - INFO - Main loop iteration: 3861
2025-03-22 07:29:39,690 - transformer_training - INFO - Main loop iteration: 3862
2025-03-22 07:29:39,950 - transformer_training - INFO - Main loop iteration: 3863
2025-03-22 07:29:40,228 - transformer_training - INFO - Main loop iteration: 3864
2025-03-22 07:29:40,479 - transformer_training - INFO - Main loop iteration: 3865
2025-03-22 07:29:40,741 - transformer_training - INFO - Main loop iteration: 3866
2025-03-22 07:29:41,001 - transformer_training - INFO - Main loop iteration: 3867
2025-03-22 07:29:41,279 - transformer_training - INFO - Main loop iteration: 3868
2025-03-22 07:29:41,530 - transformer_training - INFO - Main loop iteration: 3869
2025-03-22 07:29:41,791 - transformer_training - INFO - Main loop iteration: 3870
Iter 3870: loss 4.3749, lr 0.000487, 124328.06 tokens/sec
2025-03-22 07:29:42,056 - transformer_training - INFO - Main loop iteration: 3871
2025-03-22 07:29:42,334 - transformer_training - INFO - Main loop iteration: 3872
2025-03-22 07:29:42,586 - transformer_training - INFO - Main loop iteration: 3873
2025-03-22 07:29:42,847 - transformer_training - INFO - Main loop iteration: 3874
2025-03-22 07:29:43,107 - transformer_training - INFO - Main loop iteration: 3875
2025-03-22 07:29:43,386 - transformer_training - INFO - Main loop iteration: 3876
2025-03-22 07:29:43,637 - transformer_training - INFO - Main loop iteration: 3877
2025-03-22 07:29:43,899 - transformer_training - INFO - Main loop iteration: 3878
2025-03-22 07:29:44,159 - transformer_training - INFO - Main loop iteration: 3879
2025-03-22 07:29:44,437 - transformer_training - INFO - Main loop iteration: 3880
Iter 3880: loss 4.4487, lr 0.000487, 130791.23 tokens/sec
2025-03-22 07:29:44,688 - transformer_training - INFO - Main loop iteration: 3881
2025-03-22 07:29:44,950 - transformer_training - INFO - Main loop iteration: 3882
2025-03-22 07:29:45,210 - transformer_training - INFO - Main loop iteration: 3883
2025-03-22 07:29:45,488 - transformer_training - INFO - Main loop iteration: 3884
2025-03-22 07:29:45,739 - transformer_training - INFO - Main loop iteration: 3885
2025-03-22 07:29:46,001 - transformer_training - INFO - Main loop iteration: 3886
2025-03-22 07:29:46,261 - transformer_training - INFO - Main loop iteration: 3887
2025-03-22 07:29:46,543 - transformer_training - INFO - Main loop iteration: 3888
2025-03-22 07:29:46,796 - transformer_training - INFO - Main loop iteration: 3889
2025-03-22 07:29:47,058 - transformer_training - INFO - Main loop iteration: 3890
Iter 3890: loss 4.4073, lr 0.000487, 124144.00 tokens/sec
2025-03-22 07:29:47,323 - transformer_training - INFO - Main loop iteration: 3891
2025-03-22 07:29:47,605 - transformer_training - INFO - Main loop iteration: 3892
2025-03-22 07:29:47,859 - transformer_training - INFO - Main loop iteration: 3893
2025-03-22 07:29:48,120 - transformer_training - INFO - Main loop iteration: 3894
2025-03-22 07:29:48,380 - transformer_training - INFO - Main loop iteration: 3895
2025-03-22 07:29:48,658 - transformer_training - INFO - Main loop iteration: 3896
2025-03-22 07:29:48,909 - transformer_training - INFO - Main loop iteration: 3897
2025-03-22 07:29:49,171 - transformer_training - INFO - Main loop iteration: 3898
2025-03-22 07:29:49,431 - transformer_training - INFO - Main loop iteration: 3899
2025-03-22 07:29:49,709 - transformer_training - INFO - Main loop iteration: 3900
Iter 3900: loss 4.4222, lr 0.000486, 130630.62 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 3900: train loss 4.2549, val loss 4.2315
New best model saved with val loss: 4.2315
2025-03-22 07:30:10,057 - transformer_training - INFO - Main loop iteration: 3901
2025-03-22 07:30:10,336 - transformer_training - INFO - Main loop iteration: 3902
2025-03-22 07:30:10,597 - transformer_training - INFO - Main loop iteration: 3903
2025-03-22 07:30:10,882 - transformer_training - INFO - Main loop iteration: 3904
2025-03-22 07:30:11,133 - transformer_training - INFO - Main loop iteration: 3905
2025-03-22 07:30:11,395 - transformer_training - INFO - Main loop iteration: 3906
2025-03-22 07:30:11,780 - transformer_training - INFO - Main loop iteration: 3907
2025-03-22 07:30:12,058 - transformer_training - INFO - Main loop iteration: 3908
2025-03-22 07:30:12,309 - transformer_training - INFO - Main loop iteration: 3909
2025-03-22 07:30:12,571 - transformer_training - INFO - Main loop iteration: 3910
Iter 3910: loss 4.4512, lr 0.000486, 126726.51 tokens/sec
2025-03-22 07:30:12,831 - transformer_training - INFO - Main loop iteration: 3911
2025-03-22 07:30:13,109 - transformer_training - INFO - Main loop iteration: 3912
2025-03-22 07:30:13,360 - transformer_training - INFO - Main loop iteration: 3913
2025-03-22 07:30:13,622 - transformer_training - INFO - Main loop iteration: 3914
2025-03-22 07:30:13,882 - transformer_training - INFO - Main loop iteration: 3915
2025-03-22 07:30:14,160 - transformer_training - INFO - Main loop iteration: 3916
2025-03-22 07:30:14,411 - transformer_training - INFO - Main loop iteration: 3917
2025-03-22 07:30:14,673 - transformer_training - INFO - Main loop iteration: 3918
2025-03-22 07:30:14,933 - transformer_training - INFO - Main loop iteration: 3919
2025-03-22 07:30:15,212 - transformer_training - INFO - Main loop iteration: 3920
Iter 3920: loss 4.3975, lr 0.000486, 130432.76 tokens/sec
2025-03-22 07:30:15,464 - transformer_training - INFO - Main loop iteration: 3921
2025-03-22 07:30:15,725 - transformer_training - INFO - Main loop iteration: 3922
2025-03-22 07:30:15,985 - transformer_training - INFO - Main loop iteration: 3923
2025-03-22 07:30:16,263 - transformer_training - INFO - Main loop iteration: 3924
2025-03-22 07:30:16,515 - transformer_training - INFO - Main loop iteration: 3925
2025-03-22 07:30:16,777 - transformer_training - INFO - Main loop iteration: 3926
2025-03-22 07:30:17,037 - transformer_training - INFO - Main loop iteration: 3927
2025-03-22 07:30:17,316 - transformer_training - INFO - Main loop iteration: 3928
2025-03-22 07:30:17,568 - transformer_training - INFO - Main loop iteration: 3929
2025-03-22 07:30:17,830 - transformer_training - INFO - Main loop iteration: 3930
Iter 3930: loss 4.2764, lr 0.000486, 126221.52 tokens/sec
2025-03-22 07:30:18,091 - transformer_training - INFO - Main loop iteration: 3931
2025-03-22 07:30:18,368 - transformer_training - INFO - Main loop iteration: 3932
2025-03-22 07:30:18,619 - transformer_training - INFO - Main loop iteration: 3933
2025-03-22 07:30:18,881 - transformer_training - INFO - Main loop iteration: 3934
2025-03-22 07:30:19,162 - transformer_training - INFO - Main loop iteration: 3935
2025-03-22 07:30:19,439 - transformer_training - INFO - Main loop iteration: 3936
2025-03-22 07:30:19,690 - transformer_training - INFO - Main loop iteration: 3937
2025-03-22 07:30:19,952 - transformer_training - INFO - Main loop iteration: 3938
2025-03-22 07:30:20,212 - transformer_training - INFO - Main loop iteration: 3939
2025-03-22 07:30:20,491 - transformer_training - INFO - Main loop iteration: 3940
Iter 3940: loss 4.3936, lr 0.000486, 130775.80 tokens/sec
2025-03-22 07:30:20,742 - transformer_training - INFO - Main loop iteration: 3941
2025-03-22 07:30:21,004 - transformer_training - INFO - Main loop iteration: 3942
2025-03-22 07:30:21,268 - transformer_training - INFO - Main loop iteration: 3943
2025-03-22 07:30:21,546 - transformer_training - INFO - Main loop iteration: 3944
2025-03-22 07:30:21,797 - transformer_training - INFO - Main loop iteration: 3945
2025-03-22 07:30:22,059 - transformer_training - INFO - Main loop iteration: 3946
2025-03-22 07:30:22,323 - transformer_training - INFO - Main loop iteration: 3947
2025-03-22 07:30:22,601 - transformer_training - INFO - Main loop iteration: 3948
2025-03-22 07:30:22,853 - transformer_training - INFO - Main loop iteration: 3949
2025-03-22 07:30:23,114 - transformer_training - INFO - Main loop iteration: 3950
Iter 3950: loss 4.4112, lr 0.000486, 126254.91 tokens/sec
2025-03-22 07:30:23,375 - transformer_training - INFO - Main loop iteration: 3951
2025-03-22 07:30:23,652 - transformer_training - INFO - Main loop iteration: 3952
2025-03-22 07:30:23,903 - transformer_training - INFO - Main loop iteration: 3953
2025-03-22 07:30:24,165 - transformer_training - INFO - Main loop iteration: 3954
2025-03-22 07:30:24,430 - transformer_training - INFO - Main loop iteration: 3955
2025-03-22 07:30:24,708 - transformer_training - INFO - Main loop iteration: 3956
2025-03-22 07:30:24,959 - transformer_training - INFO - Main loop iteration: 3957
2025-03-22 07:30:25,220 - transformer_training - INFO - Main loop iteration: 3958
2025-03-22 07:30:25,481 - transformer_training - INFO - Main loop iteration: 3959
2025-03-22 07:30:25,759 - transformer_training - INFO - Main loop iteration: 3960
Iter 3960: loss 4.4258, lr 0.000486, 131100.13 tokens/sec
2025-03-22 07:30:26,009 - transformer_training - INFO - Main loop iteration: 3961
2025-03-22 07:30:26,272 - transformer_training - INFO - Main loop iteration: 3962
2025-03-22 07:30:26,532 - transformer_training - INFO - Main loop iteration: 3963
2025-03-22 07:30:26,810 - transformer_training - INFO - Main loop iteration: 3964
2025-03-22 07:30:27,062 - transformer_training - INFO - Main loop iteration: 3965
2025-03-22 07:30:27,323 - transformer_training - INFO - Main loop iteration: 3966
2025-03-22 07:30:27,584 - transformer_training - INFO - Main loop iteration: 3967
2025-03-22 07:30:27,862 - transformer_training - INFO - Main loop iteration: 3968
2025-03-22 07:30:28,113 - transformer_training - INFO - Main loop iteration: 3969
2025-03-22 07:30:28,375 - transformer_training - INFO - Main loop iteration: 3970
Iter 3970: loss 4.2871, lr 0.000485, 123410.40 tokens/sec
2025-03-22 07:30:28,641 - transformer_training - INFO - Main loop iteration: 3971
2025-03-22 07:30:28,919 - transformer_training - INFO - Main loop iteration: 3972
2025-03-22 07:30:29,171 - transformer_training - INFO - Main loop iteration: 3973
2025-03-22 07:30:29,432 - transformer_training - INFO - Main loop iteration: 3974
2025-03-22 07:30:29,697 - transformer_training - INFO - Main loop iteration: 3975
2025-03-22 07:30:29,975 - transformer_training - INFO - Main loop iteration: 3976
2025-03-22 07:30:30,226 - transformer_training - INFO - Main loop iteration: 3977
2025-03-22 07:30:30,488 - transformer_training - INFO - Main loop iteration: 3978
2025-03-22 07:30:30,748 - transformer_training - INFO - Main loop iteration: 3979
2025-03-22 07:30:31,025 - transformer_training - INFO - Main loop iteration: 3980
Iter 3980: loss 4.4689, lr 0.000485, 130588.17 tokens/sec
2025-03-22 07:30:31,277 - transformer_training - INFO - Main loop iteration: 3981
2025-03-22 07:30:31,538 - transformer_training - INFO - Main loop iteration: 3982
2025-03-22 07:30:31,798 - transformer_training - INFO - Main loop iteration: 3983
2025-03-22 07:30:32,076 - transformer_training - INFO - Main loop iteration: 3984
2025-03-22 07:30:32,328 - transformer_training - INFO - Main loop iteration: 3985
2025-03-22 07:30:32,589 - transformer_training - INFO - Main loop iteration: 3986
2025-03-22 07:30:32,853 - transformer_training - INFO - Main loop iteration: 3987
2025-03-22 07:30:33,132 - transformer_training - INFO - Main loop iteration: 3988
2025-03-22 07:30:33,383 - transformer_training - INFO - Main loop iteration: 3989
2025-03-22 07:30:33,644 - transformer_training - INFO - Main loop iteration: 3990
Iter 3990: loss 4.3746, lr 0.000485, 126568.03 tokens/sec
2025-03-22 07:30:33,904 - transformer_training - INFO - Main loop iteration: 3991
2025-03-22 07:30:34,182 - transformer_training - INFO - Main loop iteration: 3992
2025-03-22 07:30:34,433 - transformer_training - INFO - Main loop iteration: 3993
2025-03-22 07:30:34,695 - transformer_training - INFO - Main loop iteration: 3994
2025-03-22 07:30:34,956 - transformer_training - INFO - Main loop iteration: 3995
2025-03-22 07:30:35,234 - transformer_training - INFO - Main loop iteration: 3996
2025-03-22 07:30:35,485 - transformer_training - INFO - Main loop iteration: 3997
2025-03-22 07:30:35,746 - transformer_training - INFO - Main loop iteration: 3998
2025-03-22 07:30:36,007 - transformer_training - INFO - Main loop iteration: 3999
2025-03-22 07:30:36,285 - transformer_training - INFO - Main loop iteration: 4000
Iter 4000: loss 4.3901, lr 0.000485, 130716.34 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
terminate called without an active exception
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 4000: train loss 4.2383, val loss 4.2061
New best model saved with val loss: 4.2061
2025-03-22 07:30:58,485 - transformer_training - INFO - Main loop iteration: 4001
2025-03-22 07:30:58,738 - transformer_training - INFO - Main loop iteration: 4002
2025-03-22 07:30:58,999 - transformer_training - INFO - Main loop iteration: 4003
2025-03-22 07:30:59,276 - transformer_training - INFO - Main loop iteration: 4004
2025-03-22 07:30:59,527 - transformer_training - INFO - Main loop iteration: 4005
2025-03-22 07:30:59,789 - transformer_training - INFO - Main loop iteration: 4006
2025-03-22 07:31:00,049 - transformer_training - INFO - Main loop iteration: 4007
2025-03-22 07:31:00,332 - transformer_training - INFO - Main loop iteration: 4008
2025-03-22 07:31:00,583 - transformer_training - INFO - Main loop iteration: 4009
2025-03-22 07:31:00,845 - transformer_training - INFO - Main loop iteration: 4010
Iter 4010: loss 4.3257, lr 0.000485, 126114.39 tokens/sec
2025-03-22 07:31:01,106 - transformer_training - INFO - Main loop iteration: 4011
2025-03-22 07:31:01,384 - transformer_training - INFO - Main loop iteration: 4012
2025-03-22 07:31:01,636 - transformer_training - INFO - Main loop iteration: 4013
2025-03-22 07:31:01,897 - transformer_training - INFO - Main loop iteration: 4014
2025-03-22 07:31:02,158 - transformer_training - INFO - Main loop iteration: 4015
2025-03-22 07:31:02,436 - transformer_training - INFO - Main loop iteration: 4016
2025-03-22 07:31:02,687 - transformer_training - INFO - Main loop iteration: 4017
2025-03-22 07:31:02,949 - transformer_training - INFO - Main loop iteration: 4018
2025-03-22 07:31:03,209 - transformer_training - INFO - Main loop iteration: 4019
2025-03-22 07:31:03,488 - transformer_training - INFO - Main loop iteration: 4020
Iter 4020: loss 4.3846, lr 0.000485, 126210.05 tokens/sec
2025-03-22 07:31:03,748 - transformer_training - INFO - Main loop iteration: 4021
2025-03-22 07:31:04,016 - transformer_training - INFO - Main loop iteration: 4022
2025-03-22 07:31:04,290 - transformer_training - INFO - Main loop iteration: 4023
2025-03-22 07:31:04,576 - transformer_training - INFO - Main loop iteration: 4024
2025-03-22 07:31:04,835 - transformer_training - INFO - Main loop iteration: 4025
2025-03-22 07:31:05,105 - transformer_training - INFO - Main loop iteration: 4026
2025-03-22 07:31:05,375 - transformer_training - INFO - Main loop iteration: 4027
2025-03-22 07:31:05,666 - transformer_training - INFO - Main loop iteration: 4028
2025-03-22 07:31:05,924 - transformer_training - INFO - Main loop iteration: 4029
2025-03-22 07:31:06,192 - transformer_training - INFO - Main loop iteration: 4030
Iter 4030: loss 4.3046, lr 0.000485, 125982.83 tokens/sec
2025-03-22 07:31:06,454 - transformer_training - INFO - Main loop iteration: 4031
2025-03-22 07:31:06,732 - transformer_training - INFO - Main loop iteration: 4032
2025-03-22 07:31:06,983 - transformer_training - INFO - Main loop iteration: 4033
2025-03-22 07:31:07,245 - transformer_training - INFO - Main loop iteration: 4034
2025-03-22 07:31:07,505 - transformer_training - INFO - Main loop iteration: 4035
2025-03-22 07:31:07,783 - transformer_training - INFO - Main loop iteration: 4036
2025-03-22 07:31:08,034 - transformer_training - INFO - Main loop iteration: 4037
2025-03-22 07:31:08,296 - transformer_training - INFO - Main loop iteration: 4038
2025-03-22 07:31:08,556 - transformer_training - INFO - Main loop iteration: 4039
2025-03-22 07:31:08,834 - transformer_training - INFO - Main loop iteration: 4040
Iter 4040: loss 4.4870, lr 0.000484, 130823.22 tokens/sec
2025-03-22 07:31:09,085 - transformer_training - INFO - Main loop iteration: 4041
2025-03-22 07:31:09,347 - transformer_training - INFO - Main loop iteration: 4042
2025-03-22 07:31:09,607 - transformer_training - INFO - Main loop iteration: 4043
2025-03-22 07:31:09,884 - transformer_training - INFO - Main loop iteration: 4044
2025-03-22 07:31:10,136 - transformer_training - INFO - Main loop iteration: 4045
2025-03-22 07:31:10,398 - transformer_training - INFO - Main loop iteration: 4046
2025-03-22 07:31:10,658 - transformer_training - INFO - Main loop iteration: 4047
2025-03-22 07:31:10,936 - transformer_training - INFO - Main loop iteration: 4048
2025-03-22 07:31:11,188 - transformer_training - INFO - Main loop iteration: 4049
2025-03-22 07:31:11,450 - transformer_training - INFO - Main loop iteration: 4050
Iter 4050: loss 4.2974, lr 0.000484, 126318.16 tokens/sec
2025-03-22 07:31:11,710 - transformer_training - INFO - Main loop iteration: 4051
2025-03-22 07:31:11,988 - transformer_training - INFO - Main loop iteration: 4052
2025-03-22 07:31:12,239 - transformer_training - INFO - Main loop iteration: 4053
2025-03-22 07:31:12,501 - transformer_training - INFO - Main loop iteration: 4054
2025-03-22 07:31:12,761 - transformer_training - INFO - Main loop iteration: 4055
2025-03-22 07:31:13,039 - transformer_training - INFO - Main loop iteration: 4056
2025-03-22 07:31:13,291 - transformer_training - INFO - Main loop iteration: 4057
2025-03-22 07:31:13,552 - transformer_training - INFO - Main loop iteration: 4058
2025-03-22 07:31:13,824 - transformer_training - INFO - Main loop iteration: 4059
2025-03-22 07:31:14,103 - transformer_training - INFO - Main loop iteration: 4060
Iter 4060: loss 4.3860, lr 0.000484, 130453.43 tokens/sec
2025-03-22 07:31:14,355 - transformer_training - INFO - Main loop iteration: 4061
2025-03-22 07:31:14,621 - transformer_training - INFO - Main loop iteration: 4062
2025-03-22 07:31:14,886 - transformer_training - INFO - Main loop iteration: 4063
2025-03-22 07:31:15,164 - transformer_training - INFO - Main loop iteration: 4064
2025-03-22 07:31:15,415 - transformer_training - INFO - Main loop iteration: 4065
2025-03-22 07:31:15,677 - transformer_training - INFO - Main loop iteration: 4066
2025-03-22 07:31:15,940 - transformer_training - INFO - Main loop iteration: 4067
2025-03-22 07:31:16,218 - transformer_training - INFO - Main loop iteration: 4068
2025-03-22 07:31:16,470 - transformer_training - INFO - Main loop iteration: 4069
2025-03-22 07:31:16,731 - transformer_training - INFO - Main loop iteration: 4070
Iter 4070: loss 4.5294, lr 0.000484, 126105.59 tokens/sec
2025-03-22 07:31:16,992 - transformer_training - INFO - Main loop iteration: 4071
2025-03-22 07:31:17,270 - transformer_training - INFO - Main loop iteration: 4072
2025-03-22 07:31:17,632 - transformer_training - INFO - Main loop iteration: 4073
2025-03-22 07:31:17,894 - transformer_training - INFO - Main loop iteration: 4074
2025-03-22 07:31:18,155 - transformer_training - INFO - Main loop iteration: 4075
2025-03-22 07:31:18,433 - transformer_training - INFO - Main loop iteration: 4076
2025-03-22 07:31:18,684 - transformer_training - INFO - Main loop iteration: 4077
2025-03-22 07:31:18,946 - transformer_training - INFO - Main loop iteration: 4078
2025-03-22 07:31:19,206 - transformer_training - INFO - Main loop iteration: 4079
2025-03-22 07:31:19,485 - transformer_training - INFO - Main loop iteration: 4080
Iter 4080: loss 4.3794, lr 0.000484, 130572.04 tokens/sec
2025-03-22 07:31:19,736 - transformer_training - INFO - Main loop iteration: 4081
2025-03-22 07:31:19,998 - transformer_training - INFO - Main loop iteration: 4082
2025-03-22 07:31:20,258 - transformer_training - INFO - Main loop iteration: 4083
2025-03-22 07:31:20,536 - transformer_training - INFO - Main loop iteration: 4084
2025-03-22 07:31:20,788 - transformer_training - INFO - Main loop iteration: 4085
2025-03-22 07:31:21,051 - transformer_training - INFO - Main loop iteration: 4086
2025-03-22 07:31:21,315 - transformer_training - INFO - Main loop iteration: 4087
2025-03-22 07:31:21,593 - transformer_training - INFO - Main loop iteration: 4088
2025-03-22 07:31:21,844 - transformer_training - INFO - Main loop iteration: 4089
2025-03-22 07:31:22,106 - transformer_training - INFO - Main loop iteration: 4090
Iter 4090: loss 4.4216, lr 0.000484, 126318.39 tokens/sec
2025-03-22 07:31:22,366 - transformer_training - INFO - Main loop iteration: 4091
2025-03-22 07:31:22,643 - transformer_training - INFO - Main loop iteration: 4092
2025-03-22 07:31:22,895 - transformer_training - INFO - Main loop iteration: 4093
2025-03-22 07:31:23,156 - transformer_training - INFO - Main loop iteration: 4094
2025-03-22 07:31:23,421 - transformer_training - INFO - Main loop iteration: 4095
2025-03-22 07:31:23,719 - transformer_training - INFO - Main loop iteration: 4096
2025-03-22 07:31:23,970 - transformer_training - INFO - Main loop iteration: 4097
2025-03-22 07:31:24,231 - transformer_training - INFO - Main loop iteration: 4098
2025-03-22 07:31:24,492 - transformer_training - INFO - Main loop iteration: 4099
2025-03-22 07:31:24,769 - transformer_training - INFO - Main loop iteration: 4100
Iter 4100: loss 4.3436, lr 0.000483, 130256.73 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 4100: train loss 4.2155, val loss 4.1760
New best model saved with val loss: 4.1760
2025-03-22 07:31:44,897 - transformer_training - INFO - Main loop iteration: 4101
2025-03-22 07:31:45,148 - transformer_training - INFO - Main loop iteration: 4102
2025-03-22 07:31:45,409 - transformer_training - INFO - Main loop iteration: 4103
2025-03-22 07:31:45,684 - transformer_training - INFO - Main loop iteration: 4104
2025-03-22 07:31:45,933 - transformer_training - INFO - Main loop iteration: 4105
2025-03-22 07:31:46,193 - transformer_training - INFO - Main loop iteration: 4106
2025-03-22 07:31:46,452 - transformer_training - INFO - Main loop iteration: 4107
2025-03-22 07:31:46,730 - transformer_training - INFO - Main loop iteration: 4108
2025-03-22 07:31:46,982 - transformer_training - INFO - Main loop iteration: 4109
2025-03-22 07:31:47,243 - transformer_training - INFO - Main loop iteration: 4110
Iter 4110: loss 4.3414, lr 0.000483, 126741.23 tokens/sec
2025-03-22 07:31:47,502 - transformer_training - INFO - Main loop iteration: 4111
2025-03-22 07:31:47,779 - transformer_training - INFO - Main loop iteration: 4112
2025-03-22 07:31:48,029 - transformer_training - INFO - Main loop iteration: 4113
2025-03-22 07:31:48,289 - transformer_training - INFO - Main loop iteration: 4114
2025-03-22 07:31:48,548 - transformer_training - INFO - Main loop iteration: 4115
2025-03-22 07:31:48,825 - transformer_training - INFO - Main loop iteration: 4116
2025-03-22 07:31:49,075 - transformer_training - INFO - Main loop iteration: 4117
2025-03-22 07:31:49,336 - transformer_training - INFO - Main loop iteration: 4118
2025-03-22 07:31:49,594 - transformer_training - INFO - Main loop iteration: 4119
2025-03-22 07:31:49,872 - transformer_training - INFO - Main loop iteration: 4120
Iter 4120: loss 4.3566, lr 0.000483, 131664.42 tokens/sec
2025-03-22 07:31:50,121 - transformer_training - INFO - Main loop iteration: 4121
2025-03-22 07:31:50,382 - transformer_training - INFO - Main loop iteration: 4122
2025-03-22 07:31:50,641 - transformer_training - INFO - Main loop iteration: 4123
2025-03-22 07:31:50,918 - transformer_training - INFO - Main loop iteration: 4124
2025-03-22 07:31:51,167 - transformer_training - INFO - Main loop iteration: 4125
2025-03-22 07:31:51,427 - transformer_training - INFO - Main loop iteration: 4126
2025-03-22 07:31:51,686 - transformer_training - INFO - Main loop iteration: 4127
2025-03-22 07:31:51,963 - transformer_training - INFO - Main loop iteration: 4128
2025-03-22 07:31:52,213 - transformer_training - INFO - Main loop iteration: 4129
2025-03-22 07:31:52,474 - transformer_training - INFO - Main loop iteration: 4130
Iter 4130: loss 4.3979, lr 0.000483, 126942.46 tokens/sec
2025-03-22 07:31:52,733 - transformer_training - INFO - Main loop iteration: 4131
2025-03-22 07:31:53,011 - transformer_training - INFO - Main loop iteration: 4132
2025-03-22 07:31:53,260 - transformer_training - INFO - Main loop iteration: 4133
2025-03-22 07:31:53,521 - transformer_training - INFO - Main loop iteration: 4134
2025-03-22 07:31:53,780 - transformer_training - INFO - Main loop iteration: 4135
2025-03-22 07:31:54,057 - transformer_training - INFO - Main loop iteration: 4136
2025-03-22 07:31:54,311 - transformer_training - INFO - Main loop iteration: 4137
2025-03-22 07:31:54,572 - transformer_training - INFO - Main loop iteration: 4138
2025-03-22 07:31:54,830 - transformer_training - INFO - Main loop iteration: 4139
2025-03-22 07:31:55,108 - transformer_training - INFO - Main loop iteration: 4140
Iter 4140: loss 4.3040, lr 0.000483, 131189.61 tokens/sec
2025-03-22 07:31:55,358 - transformer_training - INFO - Main loop iteration: 4141
2025-03-22 07:31:55,618 - transformer_training - INFO - Main loop iteration: 4142
2025-03-22 07:31:55,883 - transformer_training - INFO - Main loop iteration: 4143
2025-03-22 07:31:56,160 - transformer_training - INFO - Main loop iteration: 4144
2025-03-22 07:31:56,410 - transformer_training - INFO - Main loop iteration: 4145
2025-03-22 07:31:56,670 - transformer_training - INFO - Main loop iteration: 4146
2025-03-22 07:31:56,929 - transformer_training - INFO - Main loop iteration: 4147
2025-03-22 07:31:57,207 - transformer_training - INFO - Main loop iteration: 4148
2025-03-22 07:31:57,456 - transformer_training - INFO - Main loop iteration: 4149
2025-03-22 07:31:57,717 - transformer_training - INFO - Main loop iteration: 4150
Iter 4150: loss 4.3457, lr 0.000483, 126856.23 tokens/sec
2025-03-22 07:31:57,976 - transformer_training - INFO - Main loop iteration: 4151
2025-03-22 07:31:58,253 - transformer_training - INFO - Main loop iteration: 4152
2025-03-22 07:31:58,502 - transformer_training - INFO - Main loop iteration: 4153
2025-03-22 07:31:58,763 - transformer_training - INFO - Main loop iteration: 4154
2025-03-22 07:31:59,021 - transformer_training - INFO - Main loop iteration: 4155
2025-03-22 07:31:59,299 - transformer_training - INFO - Main loop iteration: 4156
2025-03-22 07:31:59,552 - transformer_training - INFO - Main loop iteration: 4157
2025-03-22 07:31:59,813 - transformer_training - INFO - Main loop iteration: 4158
2025-03-22 07:32:00,072 - transformer_training - INFO - Main loop iteration: 4159
2025-03-22 07:32:00,349 - transformer_training - INFO - Main loop iteration: 4160
Iter 4160: loss 4.1676, lr 0.000483, 129564.66 tokens/sec
2025-03-22 07:32:00,603 - transformer_training - INFO - Main loop iteration: 4161
2025-03-22 07:32:00,863 - transformer_training - INFO - Main loop iteration: 4162
2025-03-22 07:32:01,123 - transformer_training - INFO - Main loop iteration: 4163
2025-03-22 07:32:01,400 - transformer_training - INFO - Main loop iteration: 4164
2025-03-22 07:32:01,649 - transformer_training - INFO - Main loop iteration: 4165
2025-03-22 07:32:01,909 - transformer_training - INFO - Main loop iteration: 4166
2025-03-22 07:32:02,169 - transformer_training - INFO - Main loop iteration: 4167
2025-03-22 07:32:02,446 - transformer_training - INFO - Main loop iteration: 4168
2025-03-22 07:32:02,699 - transformer_training - INFO - Main loop iteration: 4169
2025-03-22 07:32:02,959 - transformer_training - INFO - Main loop iteration: 4170
Iter 4170: loss 4.3893, lr 0.000482, 126863.14 tokens/sec
2025-03-22 07:32:03,218 - transformer_training - INFO - Main loop iteration: 4171
2025-03-22 07:32:03,500 - transformer_training - INFO - Main loop iteration: 4172
2025-03-22 07:32:03,750 - transformer_training - INFO - Main loop iteration: 4173
2025-03-22 07:32:04,010 - transformer_training - INFO - Main loop iteration: 4174
2025-03-22 07:32:04,269 - transformer_training - INFO - Main loop iteration: 4175
2025-03-22 07:32:04,546 - transformer_training - INFO - Main loop iteration: 4176
2025-03-22 07:32:04,796 - transformer_training - INFO - Main loop iteration: 4177
2025-03-22 07:32:05,057 - transformer_training - INFO - Main loop iteration: 4178
2025-03-22 07:32:05,316 - transformer_training - INFO - Main loop iteration: 4179
2025-03-22 07:32:05,601 - transformer_training - INFO - Main loop iteration: 4180
Iter 4180: loss 4.3922, lr 0.000482, 131498.13 tokens/sec
2025-03-22 07:32:05,851 - transformer_training - INFO - Main loop iteration: 4181
2025-03-22 07:32:06,112 - transformer_training - INFO - Main loop iteration: 4182
2025-03-22 07:32:06,371 - transformer_training - INFO - Main loop iteration: 4183
2025-03-22 07:32:06,655 - transformer_training - INFO - Main loop iteration: 4184
2025-03-22 07:32:06,904 - transformer_training - INFO - Main loop iteration: 4185
2025-03-22 07:32:07,165 - transformer_training - INFO - Main loop iteration: 4186
2025-03-22 07:32:07,423 - transformer_training - INFO - Main loop iteration: 4187
2025-03-22 07:32:07,700 - transformer_training - INFO - Main loop iteration: 4188
2025-03-22 07:32:07,950 - transformer_training - INFO - Main loop iteration: 4189
2025-03-22 07:32:08,210 - transformer_training - INFO - Main loop iteration: 4190
Iter 4190: loss 4.4050, lr 0.000482, 126755.38 tokens/sec
2025-03-22 07:32:08,470 - transformer_training - INFO - Main loop iteration: 4191
2025-03-22 07:32:08,746 - transformer_training - INFO - Main loop iteration: 4192
2025-03-22 07:32:08,997 - transformer_training - INFO - Main loop iteration: 4193
2025-03-22 07:32:09,257 - transformer_training - INFO - Main loop iteration: 4194
2025-03-22 07:32:09,516 - transformer_training - INFO - Main loop iteration: 4195
2025-03-22 07:32:09,793 - transformer_training - INFO - Main loop iteration: 4196
2025-03-22 07:32:10,042 - transformer_training - INFO - Main loop iteration: 4197
2025-03-22 07:32:10,303 - transformer_training - INFO - Main loop iteration: 4198
2025-03-22 07:32:10,562 - transformer_training - INFO - Main loop iteration: 4199
2025-03-22 07:32:10,839 - transformer_training - INFO - Main loop iteration: 4200
Iter 4200: loss 4.2968, lr 0.000482, 131476.24 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 4200: train loss 4.2091, val loss 4.1597
New best model saved with val loss: 4.1597
2025-03-22 07:32:31,177 - transformer_training - INFO - Main loop iteration: 4201
2025-03-22 07:32:31,452 - transformer_training - INFO - Main loop iteration: 4202
2025-03-22 07:32:31,712 - transformer_training - INFO - Main loop iteration: 4203
2025-03-22 07:32:31,988 - transformer_training - INFO - Main loop iteration: 4204
2025-03-22 07:32:32,238 - transformer_training - INFO - Main loop iteration: 4205
2025-03-22 07:32:32,498 - transformer_training - INFO - Main loop iteration: 4206
2025-03-22 07:32:32,757 - transformer_training - INFO - Main loop iteration: 4207
2025-03-22 07:32:33,034 - transformer_training - INFO - Main loop iteration: 4208
2025-03-22 07:32:33,284 - transformer_training - INFO - Main loop iteration: 4209
2025-03-22 07:32:33,544 - transformer_training - INFO - Main loop iteration: 4210
Iter 4210: loss 4.3689, lr 0.000482, 126986.68 tokens/sec
2025-03-22 07:32:33,803 - transformer_training - INFO - Main loop iteration: 4211
2025-03-22 07:32:34,080 - transformer_training - INFO - Main loop iteration: 4212
2025-03-22 07:32:34,331 - transformer_training - INFO - Main loop iteration: 4213
2025-03-22 07:32:34,591 - transformer_training - INFO - Main loop iteration: 4214
2025-03-22 07:32:34,850 - transformer_training - INFO - Main loop iteration: 4215
2025-03-22 07:32:35,127 - transformer_training - INFO - Main loop iteration: 4216
2025-03-22 07:32:35,382 - transformer_training - INFO - Main loop iteration: 4217
2025-03-22 07:32:35,642 - transformer_training - INFO - Main loop iteration: 4218
2025-03-22 07:32:35,900 - transformer_training - INFO - Main loop iteration: 4219
2025-03-22 07:32:36,178 - transformer_training - INFO - Main loop iteration: 4220
Iter 4220: loss 4.2954, lr 0.000482, 131583.49 tokens/sec
2025-03-22 07:32:36,427 - transformer_training - INFO - Main loop iteration: 4221
2025-03-22 07:32:36,687 - transformer_training - INFO - Main loop iteration: 4222
2025-03-22 07:32:36,946 - transformer_training - INFO - Main loop iteration: 4223
2025-03-22 07:32:37,223 - transformer_training - INFO - Main loop iteration: 4224
2025-03-22 07:32:37,473 - transformer_training - INFO - Main loop iteration: 4225
2025-03-22 07:32:37,733 - transformer_training - INFO - Main loop iteration: 4226
2025-03-22 07:32:37,992 - transformer_training - INFO - Main loop iteration: 4227
2025-03-22 07:32:38,269 - transformer_training - INFO - Main loop iteration: 4228
2025-03-22 07:32:38,519 - transformer_training - INFO - Main loop iteration: 4229
2025-03-22 07:32:38,779 - transformer_training - INFO - Main loop iteration: 4230
Iter 4230: loss 4.4038, lr 0.000481, 126771.04 tokens/sec
2025-03-22 07:32:39,038 - transformer_training - INFO - Main loop iteration: 4231
2025-03-22 07:32:39,315 - transformer_training - INFO - Main loop iteration: 4232
2025-03-22 07:32:39,565 - transformer_training - INFO - Main loop iteration: 4233
2025-03-22 07:32:39,825 - transformer_training - INFO - Main loop iteration: 4234
2025-03-22 07:32:40,085 - transformer_training - INFO - Main loop iteration: 4235
2025-03-22 07:32:40,362 - transformer_training - INFO - Main loop iteration: 4236
2025-03-22 07:32:40,611 - transformer_training - INFO - Main loop iteration: 4237
2025-03-22 07:32:40,872 - transformer_training - INFO - Main loop iteration: 4238
2025-03-22 07:32:41,131 - transformer_training - INFO - Main loop iteration: 4239
2025-03-22 07:32:41,408 - transformer_training - INFO - Main loop iteration: 4240
Iter 4240: loss 4.3364, lr 0.000481, 131504.55 tokens/sec
2025-03-22 07:32:41,658 - transformer_training - INFO - Main loop iteration: 4241
2025-03-22 07:32:41,918 - transformer_training - INFO - Main loop iteration: 4242
2025-03-22 07:32:42,177 - transformer_training - INFO - Main loop iteration: 4243
2025-03-22 07:32:42,455 - transformer_training - INFO - Main loop iteration: 4244
2025-03-22 07:32:42,704 - transformer_training - INFO - Main loop iteration: 4245
2025-03-22 07:32:43,076 - transformer_training - INFO - Main loop iteration: 4246
2025-03-22 07:32:43,334 - transformer_training - INFO - Main loop iteration: 4247
2025-03-22 07:32:43,611 - transformer_training - INFO - Main loop iteration: 4248
2025-03-22 07:32:43,861 - transformer_training - INFO - Main loop iteration: 4249
2025-03-22 07:32:44,122 - transformer_training - INFO - Main loop iteration: 4250
Iter 4250: loss 4.2871, lr 0.000481, 126936.25 tokens/sec
2025-03-22 07:32:44,381 - transformer_training - INFO - Main loop iteration: 4251
2025-03-22 07:32:44,658 - transformer_training - INFO - Main loop iteration: 4252
2025-03-22 07:32:44,907 - transformer_training - INFO - Main loop iteration: 4253
2025-03-22 07:32:45,172 - transformer_training - INFO - Main loop iteration: 4254
2025-03-22 07:32:45,431 - transformer_training - INFO - Main loop iteration: 4255
2025-03-22 07:32:45,709 - transformer_training - INFO - Main loop iteration: 4256
2025-03-22 07:32:45,961 - transformer_training - INFO - Main loop iteration: 4257
2025-03-22 07:32:46,222 - transformer_training - INFO - Main loop iteration: 4258
2025-03-22 07:32:46,482 - transformer_training - INFO - Main loop iteration: 4259
2025-03-22 07:32:46,760 - transformer_training - INFO - Main loop iteration: 4260
Iter 4260: loss 4.2954, lr 0.000481, 130898.73 tokens/sec
2025-03-22 07:32:47,011 - transformer_training - INFO - Main loop iteration: 4261
2025-03-22 07:32:47,277 - transformer_training - INFO - Main loop iteration: 4262
2025-03-22 07:32:47,537 - transformer_training - INFO - Main loop iteration: 4263
2025-03-22 07:32:47,815 - transformer_training - INFO - Main loop iteration: 4264
2025-03-22 07:32:48,067 - transformer_training - INFO - Main loop iteration: 4265
2025-03-22 07:32:48,328 - transformer_training - INFO - Main loop iteration: 4266
2025-03-22 07:32:48,588 - transformer_training - INFO - Main loop iteration: 4267
2025-03-22 07:32:48,866 - transformer_training - INFO - Main loop iteration: 4268
2025-03-22 07:32:49,118 - transformer_training - INFO - Main loop iteration: 4269
2025-03-22 07:32:49,380 - transformer_training - INFO - Main loop iteration: 4270
Iter 4270: loss 4.2805, lr 0.000481, 126476.48 tokens/sec
2025-03-22 07:32:49,640 - transformer_training - INFO - Main loop iteration: 4271
2025-03-22 07:32:49,918 - transformer_training - INFO - Main loop iteration: 4272
2025-03-22 07:32:50,169 - transformer_training - INFO - Main loop iteration: 4273
2025-03-22 07:32:50,430 - transformer_training - INFO - Main loop iteration: 4274
2025-03-22 07:32:50,690 - transformer_training - INFO - Main loop iteration: 4275
2025-03-22 07:32:50,968 - transformer_training - INFO - Main loop iteration: 4276
2025-03-22 07:32:51,219 - transformer_training - INFO - Main loop iteration: 4277
2025-03-22 07:32:51,481 - transformer_training - INFO - Main loop iteration: 4278
2025-03-22 07:32:51,740 - transformer_training - INFO - Main loop iteration: 4279
2025-03-22 07:32:52,019 - transformer_training - INFO - Main loop iteration: 4280
Iter 4280: loss 4.2611, lr 0.000481, 130645.14 tokens/sec
2025-03-22 07:32:52,271 - transformer_training - INFO - Main loop iteration: 4281
2025-03-22 07:32:52,532 - transformer_training - INFO - Main loop iteration: 4282
2025-03-22 07:32:52,792 - transformer_training - INFO - Main loop iteration: 4283
2025-03-22 07:32:53,070 - transformer_training - INFO - Main loop iteration: 4284
2025-03-22 07:32:53,321 - transformer_training - INFO - Main loop iteration: 4285
2025-03-22 07:32:53,582 - transformer_training - INFO - Main loop iteration: 4286
2025-03-22 07:32:53,842 - transformer_training - INFO - Main loop iteration: 4287
2025-03-22 07:32:54,120 - transformer_training - INFO - Main loop iteration: 4288
2025-03-22 07:32:54,371 - transformer_training - INFO - Main loop iteration: 4289
2025-03-22 07:32:54,632 - transformer_training - INFO - Main loop iteration: 4290
Iter 4290: loss 4.3915, lr 0.000480, 126410.75 tokens/sec
2025-03-22 07:32:54,893 - transformer_training - INFO - Main loop iteration: 4291
2025-03-22 07:32:55,175 - transformer_training - INFO - Main loop iteration: 4292
2025-03-22 07:32:55,426 - transformer_training - INFO - Main loop iteration: 4293
2025-03-22 07:32:55,688 - transformer_training - INFO - Main loop iteration: 4294
2025-03-22 07:32:55,948 - transformer_training - INFO - Main loop iteration: 4295
2025-03-22 07:32:56,226 - transformer_training - INFO - Main loop iteration: 4296
2025-03-22 07:32:56,477 - transformer_training - INFO - Main loop iteration: 4297
2025-03-22 07:32:56,738 - transformer_training - INFO - Main loop iteration: 4298
2025-03-22 07:32:56,998 - transformer_training - INFO - Main loop iteration: 4299
2025-03-22 07:32:57,277 - transformer_training - INFO - Main loop iteration: 4300
Iter 4300: loss 4.2830, lr 0.000480, 130722.43 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 4300: train loss 4.1860, val loss 4.1268
New best model saved with val loss: 4.1268
2025-03-22 07:33:16,612 - transformer_training - INFO - Main loop iteration: 4301
2025-03-22 07:33:16,866 - transformer_training - INFO - Main loop iteration: 4302
2025-03-22 07:33:17,126 - transformer_training - INFO - Main loop iteration: 4303
2025-03-22 07:33:17,403 - transformer_training - INFO - Main loop iteration: 4304
2025-03-22 07:33:17,658 - transformer_training - INFO - Main loop iteration: 4305
2025-03-22 07:33:17,920 - transformer_training - INFO - Main loop iteration: 4306
2025-03-22 07:33:18,185 - transformer_training - INFO - Main loop iteration: 4307
2025-03-22 07:33:18,464 - transformer_training - INFO - Main loop iteration: 4308
2025-03-22 07:33:18,715 - transformer_training - INFO - Main loop iteration: 4309
2025-03-22 07:33:18,976 - transformer_training - INFO - Main loop iteration: 4310
Iter 4310: loss 4.2673, lr 0.000480, 126339.64 tokens/sec
2025-03-22 07:33:19,237 - transformer_training - INFO - Main loop iteration: 4311
2025-03-22 07:33:19,515 - transformer_training - INFO - Main loop iteration: 4312
2025-03-22 07:33:19,766 - transformer_training - INFO - Main loop iteration: 4313
2025-03-22 07:33:20,028 - transformer_training - INFO - Main loop iteration: 4314
2025-03-22 07:33:20,289 - transformer_training - INFO - Main loop iteration: 4315
2025-03-22 07:33:20,573 - transformer_training - INFO - Main loop iteration: 4316
2025-03-22 07:33:20,828 - transformer_training - INFO - Main loop iteration: 4317
2025-03-22 07:33:21,091 - transformer_training - INFO - Main loop iteration: 4318
2025-03-22 07:33:21,349 - transformer_training - INFO - Main loop iteration: 4319
2025-03-22 07:33:21,627 - transformer_training - INFO - Main loop iteration: 4320
Iter 4320: loss 4.3529, lr 0.000480, 129180.45 tokens/sec
2025-03-22 07:33:21,881 - transformer_training - INFO - Main loop iteration: 4321
2025-03-22 07:33:22,142 - transformer_training - INFO - Main loop iteration: 4322
2025-03-22 07:33:22,400 - transformer_training - INFO - Main loop iteration: 4323
2025-03-22 07:33:22,678 - transformer_training - INFO - Main loop iteration: 4324
2025-03-22 07:33:22,928 - transformer_training - INFO - Main loop iteration: 4325
2025-03-22 07:33:23,188 - transformer_training - INFO - Main loop iteration: 4326
2025-03-22 07:33:23,448 - transformer_training - INFO - Main loop iteration: 4327
2025-03-22 07:33:23,726 - transformer_training - INFO - Main loop iteration: 4328
2025-03-22 07:33:23,976 - transformer_training - INFO - Main loop iteration: 4329
2025-03-22 07:33:24,236 - transformer_training - INFO - Main loop iteration: 4330
Iter 4330: loss 4.3636, lr 0.000480, 127045.96 tokens/sec
2025-03-22 07:33:24,495 - transformer_training - INFO - Main loop iteration: 4331
2025-03-22 07:33:24,773 - transformer_training - INFO - Main loop iteration: 4332
2025-03-22 07:33:25,022 - transformer_training - INFO - Main loop iteration: 4333
2025-03-22 07:33:25,283 - transformer_training - INFO - Main loop iteration: 4334
2025-03-22 07:33:25,542 - transformer_training - INFO - Main loop iteration: 4335
2025-03-22 07:33:25,819 - transformer_training - INFO - Main loop iteration: 4336
2025-03-22 07:33:26,071 - transformer_training - INFO - Main loop iteration: 4337
2025-03-22 07:33:26,332 - transformer_training - INFO - Main loop iteration: 4338
2025-03-22 07:33:26,591 - transformer_training - INFO - Main loop iteration: 4339
2025-03-22 07:33:26,868 - transformer_training - INFO - Main loop iteration: 4340
Iter 4340: loss 4.3037, lr 0.000480, 131197.50 tokens/sec
2025-03-22 07:33:27,118 - transformer_training - INFO - Main loop iteration: 4341
2025-03-22 07:33:27,379 - transformer_training - INFO - Main loop iteration: 4342
2025-03-22 07:33:27,637 - transformer_training - INFO - Main loop iteration: 4343
2025-03-22 07:33:27,916 - transformer_training - INFO - Main loop iteration: 4344
2025-03-22 07:33:28,166 - transformer_training - INFO - Main loop iteration: 4345
2025-03-22 07:33:28,426 - transformer_training - INFO - Main loop iteration: 4346
2025-03-22 07:33:28,685 - transformer_training - INFO - Main loop iteration: 4347
2025-03-22 07:33:28,963 - transformer_training - INFO - Main loop iteration: 4348
2025-03-22 07:33:29,212 - transformer_training - INFO - Main loop iteration: 4349
2025-03-22 07:33:29,473 - transformer_training - INFO - Main loop iteration: 4350
Iter 4350: loss 4.2891, lr 0.000479, 123920.69 tokens/sec
2025-03-22 07:33:29,738 - transformer_training - INFO - Main loop iteration: 4351
2025-03-22 07:33:30,015 - transformer_training - INFO - Main loop iteration: 4352
2025-03-22 07:33:30,264 - transformer_training - INFO - Main loop iteration: 4353
2025-03-22 07:33:30,525 - transformer_training - INFO - Main loop iteration: 4354
2025-03-22 07:33:30,783 - transformer_training - INFO - Main loop iteration: 4355
2025-03-22 07:33:31,060 - transformer_training - INFO - Main loop iteration: 4356
2025-03-22 07:33:31,310 - transformer_training - INFO - Main loop iteration: 4357
2025-03-22 07:33:31,571 - transformer_training - INFO - Main loop iteration: 4358
2025-03-22 07:33:31,829 - transformer_training - INFO - Main loop iteration: 4359
2025-03-22 07:33:32,107 - transformer_training - INFO - Main loop iteration: 4360
Iter 4360: loss 4.1874, lr 0.000479, 131750.87 tokens/sec
2025-03-22 07:33:32,356 - transformer_training - INFO - Main loop iteration: 4361
2025-03-22 07:33:32,617 - transformer_training - INFO - Main loop iteration: 4362
2025-03-22 07:33:32,875 - transformer_training - INFO - Main loop iteration: 4363
2025-03-22 07:33:33,153 - transformer_training - INFO - Main loop iteration: 4364
2025-03-22 07:33:33,402 - transformer_training - INFO - Main loop iteration: 4365
2025-03-22 07:33:33,662 - transformer_training - INFO - Main loop iteration: 4366
2025-03-22 07:33:33,921 - transformer_training - INFO - Main loop iteration: 4367
2025-03-22 07:33:34,201 - transformer_training - INFO - Main loop iteration: 4368
2025-03-22 07:33:34,451 - transformer_training - INFO - Main loop iteration: 4369
2025-03-22 07:33:34,711 - transformer_training - INFO - Main loop iteration: 4370
Iter 4370: loss 4.4543, lr 0.000479, 126961.34 tokens/sec
2025-03-22 07:33:34,970 - transformer_training - INFO - Main loop iteration: 4371
2025-03-22 07:33:35,247 - transformer_training - INFO - Main loop iteration: 4372
2025-03-22 07:33:35,497 - transformer_training - INFO - Main loop iteration: 4373
2025-03-22 07:33:35,758 - transformer_training - INFO - Main loop iteration: 4374
2025-03-22 07:33:36,016 - transformer_training - INFO - Main loop iteration: 4375
2025-03-22 07:33:36,293 - transformer_training - INFO - Main loop iteration: 4376
2025-03-22 07:33:36,543 - transformer_training - INFO - Main loop iteration: 4377
2025-03-22 07:33:36,803 - transformer_training - INFO - Main loop iteration: 4378
2025-03-22 07:33:37,061 - transformer_training - INFO - Main loop iteration: 4379
2025-03-22 07:33:37,338 - transformer_training - INFO - Main loop iteration: 4380
Iter 4380: loss 4.1990, lr 0.000479, 131464.04 tokens/sec
2025-03-22 07:33:37,588 - transformer_training - INFO - Main loop iteration: 4381
2025-03-22 07:33:37,848 - transformer_training - INFO - Main loop iteration: 4382
2025-03-22 07:33:38,107 - transformer_training - INFO - Main loop iteration: 4383
2025-03-22 07:33:38,384 - transformer_training - INFO - Main loop iteration: 4384
2025-03-22 07:33:38,633 - transformer_training - INFO - Main loop iteration: 4385
2025-03-22 07:33:38,893 - transformer_training - INFO - Main loop iteration: 4386
2025-03-22 07:33:39,152 - transformer_training - INFO - Main loop iteration: 4387
2025-03-22 07:33:39,429 - transformer_training - INFO - Main loop iteration: 4388
2025-03-22 07:33:39,678 - transformer_training - INFO - Main loop iteration: 4389
2025-03-22 07:33:39,938 - transformer_training - INFO - Main loop iteration: 4390
Iter 4390: loss 4.2499, lr 0.000479, 126889.84 tokens/sec
2025-03-22 07:33:40,197 - transformer_training - INFO - Main loop iteration: 4391
2025-03-22 07:33:40,474 - transformer_training - INFO - Main loop iteration: 4392
2025-03-22 07:33:40,724 - transformer_training - INFO - Main loop iteration: 4393
2025-03-22 07:33:40,985 - transformer_training - INFO - Main loop iteration: 4394
2025-03-22 07:33:41,243 - transformer_training - INFO - Main loop iteration: 4395
2025-03-22 07:33:41,521 - transformer_training - INFO - Main loop iteration: 4396
2025-03-22 07:33:41,770 - transformer_training - INFO - Main loop iteration: 4397
2025-03-22 07:33:42,030 - transformer_training - INFO - Main loop iteration: 4398
2025-03-22 07:33:42,288 - transformer_training - INFO - Main loop iteration: 4399
2025-03-22 07:33:42,565 - transformer_training - INFO - Main loop iteration: 4400
Iter 4400: loss 4.1375, lr 0.000478, 131550.74 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 4400: train loss 4.1650, val loss 4.0942
New best model saved with val loss: 4.0942
2025-03-22 07:34:02,227 - transformer_training - INFO - Main loop iteration: 4401
2025-03-22 07:34:02,480 - transformer_training - INFO - Main loop iteration: 4402
2025-03-22 07:34:02,741 - transformer_training - INFO - Main loop iteration: 4403
2025-03-22 07:34:03,019 - transformer_training - INFO - Main loop iteration: 4404
2025-03-22 07:34:03,270 - transformer_training - INFO - Main loop iteration: 4405
2025-03-22 07:34:03,531 - transformer_training - INFO - Main loop iteration: 4406
2025-03-22 07:34:03,791 - transformer_training - INFO - Main loop iteration: 4407
2025-03-22 07:34:04,069 - transformer_training - INFO - Main loop iteration: 4408
2025-03-22 07:34:04,434 - transformer_training - INFO - Main loop iteration: 4409
2025-03-22 07:34:04,696 - transformer_training - INFO - Main loop iteration: 4410
Iter 4410: loss 4.3411, lr 0.000478, 126296.91 tokens/sec
2025-03-22 07:34:04,957 - transformer_training - INFO - Main loop iteration: 4411
2025-03-22 07:34:05,242 - transformer_training - INFO - Main loop iteration: 4412
2025-03-22 07:34:05,493 - transformer_training - INFO - Main loop iteration: 4413
2025-03-22 07:34:05,754 - transformer_training - INFO - Main loop iteration: 4414
2025-03-22 07:34:06,015 - transformer_training - INFO - Main loop iteration: 4415
2025-03-22 07:34:06,297 - transformer_training - INFO - Main loop iteration: 4416
2025-03-22 07:34:06,577 - transformer_training - INFO - Main loop iteration: 4417
2025-03-22 07:34:06,839 - transformer_training - INFO - Main loop iteration: 4418
2025-03-22 07:34:07,100 - transformer_training - INFO - Main loop iteration: 4419
2025-03-22 07:34:07,379 - transformer_training - INFO - Main loop iteration: 4420
Iter 4420: loss 4.2447, lr 0.000478, 130871.43 tokens/sec
2025-03-22 07:34:07,630 - transformer_training - INFO - Main loop iteration: 4421
2025-03-22 07:34:07,898 - transformer_training - INFO - Main loop iteration: 4422
2025-03-22 07:34:08,158 - transformer_training - INFO - Main loop iteration: 4423
2025-03-22 07:34:08,436 - transformer_training - INFO - Main loop iteration: 4424
2025-03-22 07:34:08,687 - transformer_training - INFO - Main loop iteration: 4425
2025-03-22 07:34:08,949 - transformer_training - INFO - Main loop iteration: 4426
2025-03-22 07:34:09,209 - transformer_training - INFO - Main loop iteration: 4427
2025-03-22 07:34:09,488 - transformer_training - INFO - Main loop iteration: 4428
2025-03-22 07:34:09,738 - transformer_training - INFO - Main loop iteration: 4429
2025-03-22 07:34:10,000 - transformer_training - INFO - Main loop iteration: 4430
Iter 4430: loss 4.2172, lr 0.000478, 126283.57 tokens/sec
2025-03-22 07:34:10,261 - transformer_training - INFO - Main loop iteration: 4431
2025-03-22 07:34:10,538 - transformer_training - INFO - Main loop iteration: 4432
2025-03-22 07:34:10,790 - transformer_training - INFO - Main loop iteration: 4433
2025-03-22 07:34:11,051 - transformer_training - INFO - Main loop iteration: 4434
2025-03-22 07:34:11,311 - transformer_training - INFO - Main loop iteration: 4435
2025-03-22 07:34:11,589 - transformer_training - INFO - Main loop iteration: 4436
2025-03-22 07:34:11,840 - transformer_training - INFO - Main loop iteration: 4437
2025-03-22 07:34:12,102 - transformer_training - INFO - Main loop iteration: 4438
2025-03-22 07:34:12,361 - transformer_training - INFO - Main loop iteration: 4439
2025-03-22 07:34:12,639 - transformer_training - INFO - Main loop iteration: 4440
Iter 4440: loss 4.2810, lr 0.000478, 131089.50 tokens/sec
2025-03-22 07:34:12,890 - transformer_training - INFO - Main loop iteration: 4441
2025-03-22 07:34:13,151 - transformer_training - INFO - Main loop iteration: 4442
2025-03-22 07:34:13,411 - transformer_training - INFO - Main loop iteration: 4443
2025-03-22 07:34:13,693 - transformer_training - INFO - Main loop iteration: 4444
2025-03-22 07:34:13,944 - transformer_training - INFO - Main loop iteration: 4445
2025-03-22 07:34:14,206 - transformer_training - INFO - Main loop iteration: 4446
2025-03-22 07:34:14,466 - transformer_training - INFO - Main loop iteration: 4447
2025-03-22 07:34:14,744 - transformer_training - INFO - Main loop iteration: 4448
2025-03-22 07:34:14,995 - transformer_training - INFO - Main loop iteration: 4449
2025-03-22 07:34:15,256 - transformer_training - INFO - Main loop iteration: 4450
Iter 4450: loss 4.2886, lr 0.000478, 126338.71 tokens/sec
2025-03-22 07:34:15,516 - transformer_training - INFO - Main loop iteration: 4451
2025-03-22 07:34:15,799 - transformer_training - INFO - Main loop iteration: 4452
2025-03-22 07:34:16,050 - transformer_training - INFO - Main loop iteration: 4453
2025-03-22 07:34:16,311 - transformer_training - INFO - Main loop iteration: 4454
2025-03-22 07:34:16,576 - transformer_training - INFO - Main loop iteration: 4455
2025-03-22 07:34:16,858 - transformer_training - INFO - Main loop iteration: 4456
2025-03-22 07:34:17,109 - transformer_training - INFO - Main loop iteration: 4457
2025-03-22 07:34:17,370 - transformer_training - INFO - Main loop iteration: 4458
2025-03-22 07:34:17,630 - transformer_training - INFO - Main loop iteration: 4459
2025-03-22 07:34:17,909 - transformer_training - INFO - Main loop iteration: 4460
Iter 4460: loss 4.2203, lr 0.000477, 130966.96 tokens/sec
2025-03-22 07:34:18,160 - transformer_training - INFO - Main loop iteration: 4461
2025-03-22 07:34:18,421 - transformer_training - INFO - Main loop iteration: 4462
2025-03-22 07:34:18,681 - transformer_training - INFO - Main loop iteration: 4463
2025-03-22 07:34:18,959 - transformer_training - INFO - Main loop iteration: 4464
2025-03-22 07:34:19,210 - transformer_training - INFO - Main loop iteration: 4465
2025-03-22 07:34:19,471 - transformer_training - INFO - Main loop iteration: 4466
2025-03-22 07:34:19,731 - transformer_training - INFO - Main loop iteration: 4467
2025-03-22 07:34:20,009 - transformer_training - INFO - Main loop iteration: 4468
2025-03-22 07:34:20,260 - transformer_training - INFO - Main loop iteration: 4469
2025-03-22 07:34:20,521 - transformer_training - INFO - Main loop iteration: 4470
Iter 4470: loss 4.3144, lr 0.000477, 126296.91 tokens/sec
2025-03-22 07:34:20,782 - transformer_training - INFO - Main loop iteration: 4471
2025-03-22 07:34:21,060 - transformer_training - INFO - Main loop iteration: 4472
2025-03-22 07:34:21,311 - transformer_training - INFO - Main loop iteration: 4473
2025-03-22 07:34:21,573 - transformer_training - INFO - Main loop iteration: 4474
2025-03-22 07:34:21,833 - transformer_training - INFO - Main loop iteration: 4475
2025-03-22 07:34:22,111 - transformer_training - INFO - Main loop iteration: 4476
2025-03-22 07:34:22,361 - transformer_training - INFO - Main loop iteration: 4477
2025-03-22 07:34:22,623 - transformer_training - INFO - Main loop iteration: 4478
2025-03-22 07:34:22,883 - transformer_training - INFO - Main loop iteration: 4479
2025-03-22 07:34:23,160 - transformer_training - INFO - Main loop iteration: 4480
Iter 4480: loss 4.2516, lr 0.000477, 130942.25 tokens/sec
2025-03-22 07:34:23,411 - transformer_training - INFO - Main loop iteration: 4481
2025-03-22 07:34:23,672 - transformer_training - INFO - Main loop iteration: 4482
2025-03-22 07:34:23,932 - transformer_training - INFO - Main loop iteration: 4483
2025-03-22 07:34:24,210 - transformer_training - INFO - Main loop iteration: 4484
2025-03-22 07:34:24,461 - transformer_training - INFO - Main loop iteration: 4485
2025-03-22 07:34:24,722 - transformer_training - INFO - Main loop iteration: 4486
2025-03-22 07:34:24,982 - transformer_training - INFO - Main loop iteration: 4487
2025-03-22 07:34:25,261 - transformer_training - INFO - Main loop iteration: 4488
2025-03-22 07:34:25,512 - transformer_training - INFO - Main loop iteration: 4489
2025-03-22 07:34:25,773 - transformer_training - INFO - Main loop iteration: 4490
Iter 4490: loss 4.2837, lr 0.000477, 125865.96 tokens/sec
2025-03-22 07:34:26,035 - transformer_training - INFO - Main loop iteration: 4491
2025-03-22 07:34:26,317 - transformer_training - INFO - Main loop iteration: 4492
2025-03-22 07:34:26,569 - transformer_training - INFO - Main loop iteration: 4493
2025-03-22 07:34:26,830 - transformer_training - INFO - Main loop iteration: 4494
2025-03-22 07:34:27,090 - transformer_training - INFO - Main loop iteration: 4495
2025-03-22 07:34:27,369 - transformer_training - INFO - Main loop iteration: 4496
2025-03-22 07:34:27,620 - transformer_training - INFO - Main loop iteration: 4497
2025-03-22 07:34:27,882 - transformer_training - INFO - Main loop iteration: 4498
2025-03-22 07:34:28,143 - transformer_training - INFO - Main loop iteration: 4499
2025-03-22 07:34:28,426 - transformer_training - INFO - Main loop iteration: 4500
Iter 4500: loss 4.2477, lr 0.000477, 130558.27 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPTYour base folder is: /workspace/GPT

Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 4500: train loss 4.1534, val loss 4.0734
New best model saved with val loss: 4.0734
2025-03-22 07:34:51,164 - transformer_training - INFO - Main loop iteration: 4501
2025-03-22 07:34:51,442 - transformer_training - INFO - Main loop iteration: 4502
2025-03-22 07:34:51,703 - transformer_training - INFO - Main loop iteration: 4503
2025-03-22 07:34:51,980 - transformer_training - INFO - Main loop iteration: 4504
2025-03-22 07:34:52,234 - transformer_training - INFO - Main loop iteration: 4505
2025-03-22 07:34:52,496 - transformer_training - INFO - Main loop iteration: 4506
2025-03-22 07:34:52,756 - transformer_training - INFO - Main loop iteration: 4507
2025-03-22 07:34:53,033 - transformer_training - INFO - Main loop iteration: 4508
2025-03-22 07:34:53,287 - transformer_training - INFO - Main loop iteration: 4509
2025-03-22 07:34:53,548 - transformer_training - INFO - Main loop iteration: 4510
Iter 4510: loss 4.2549, lr 0.000477, 126426.33 tokens/sec
2025-03-22 07:34:53,809 - transformer_training - INFO - Main loop iteration: 4511
2025-03-22 07:34:54,087 - transformer_training - INFO - Main loop iteration: 4512
2025-03-22 07:34:54,337 - transformer_training - INFO - Main loop iteration: 4513
2025-03-22 07:34:54,599 - transformer_training - INFO - Main loop iteration: 4514
2025-03-22 07:34:54,859 - transformer_training - INFO - Main loop iteration: 4515
2025-03-22 07:34:55,137 - transformer_training - INFO - Main loop iteration: 4516
2025-03-22 07:34:55,388 - transformer_training - INFO - Main loop iteration: 4517
2025-03-22 07:34:55,650 - transformer_training - INFO - Main loop iteration: 4518
2025-03-22 07:34:55,910 - transformer_training - INFO - Main loop iteration: 4519
2025-03-22 07:34:56,188 - transformer_training - INFO - Main loop iteration: 4520
Iter 4520: loss 4.2719, lr 0.000476, 130821.73 tokens/sec
2025-03-22 07:34:56,440 - transformer_training - INFO - Main loop iteration: 4521
2025-03-22 07:34:56,701 - transformer_training - INFO - Main loop iteration: 4522
2025-03-22 07:34:56,961 - transformer_training - INFO - Main loop iteration: 4523
2025-03-22 07:34:57,238 - transformer_training - INFO - Main loop iteration: 4524
2025-03-22 07:34:57,490 - transformer_training - INFO - Main loop iteration: 4525
2025-03-22 07:34:57,751 - transformer_training - INFO - Main loop iteration: 4526
2025-03-22 07:34:58,011 - transformer_training - INFO - Main loop iteration: 4527
2025-03-22 07:34:58,290 - transformer_training - INFO - Main loop iteration: 4528
2025-03-22 07:34:58,541 - transformer_training - INFO - Main loop iteration: 4529
2025-03-22 07:34:58,802 - transformer_training - INFO - Main loop iteration: 4530
Iter 4530: loss 4.2392, lr 0.000476, 126482.53 tokens/sec
2025-03-22 07:34:59,062 - transformer_training - INFO - Main loop iteration: 4531
2025-03-22 07:34:59,339 - transformer_training - INFO - Main loop iteration: 4532
2025-03-22 07:34:59,590 - transformer_training - INFO - Main loop iteration: 4533
2025-03-22 07:34:59,852 - transformer_training - INFO - Main loop iteration: 4534
2025-03-22 07:35:00,112 - transformer_training - INFO - Main loop iteration: 4535
2025-03-22 07:35:00,391 - transformer_training - INFO - Main loop iteration: 4536
2025-03-22 07:35:00,642 - transformer_training - INFO - Main loop iteration: 4537
2025-03-22 07:35:00,904 - transformer_training - INFO - Main loop iteration: 4538
2025-03-22 07:35:01,164 - transformer_training - INFO - Main loop iteration: 4539
2025-03-22 07:35:01,446 - transformer_training - INFO - Main loop iteration: 4540
Iter 4540: loss 4.2301, lr 0.000476, 130652.35 tokens/sec
2025-03-22 07:35:01,698 - transformer_training - INFO - Main loop iteration: 4541
2025-03-22 07:35:01,960 - transformer_training - INFO - Main loop iteration: 4542
2025-03-22 07:35:02,220 - transformer_training - INFO - Main loop iteration: 4543
2025-03-22 07:35:02,498 - transformer_training - INFO - Main loop iteration: 4544
2025-03-22 07:35:02,753 - transformer_training - INFO - Main loop iteration: 4545
2025-03-22 07:35:03,014 - transformer_training - INFO - Main loop iteration: 4546
2025-03-22 07:35:03,273 - transformer_training - INFO - Main loop iteration: 4547
2025-03-22 07:35:03,551 - transformer_training - INFO - Main loop iteration: 4548
2025-03-22 07:35:03,802 - transformer_training - INFO - Main loop iteration: 4549
2025-03-22 07:35:04,063 - transformer_training - INFO - Main loop iteration: 4550
Iter 4550: loss 4.2030, lr 0.000476, 126315.02 tokens/sec
2025-03-22 07:35:04,324 - transformer_training - INFO - Main loop iteration: 4551
2025-03-22 07:35:04,601 - transformer_training - INFO - Main loop iteration: 4552
2025-03-22 07:35:04,852 - transformer_training - INFO - Main loop iteration: 4553
2025-03-22 07:35:05,114 - transformer_training - INFO - Main loop iteration: 4554
2025-03-22 07:35:05,375 - transformer_training - INFO - Main loop iteration: 4555
2025-03-22 07:35:05,653 - transformer_training - INFO - Main loop iteration: 4556
2025-03-22 07:35:05,904 - transformer_training - INFO - Main loop iteration: 4557
2025-03-22 07:35:06,166 - transformer_training - INFO - Main loop iteration: 4558
2025-03-22 07:35:06,426 - transformer_training - INFO - Main loop iteration: 4559
2025-03-22 07:35:06,709 - transformer_training - INFO - Main loop iteration: 4560
Iter 4560: loss 4.1981, lr 0.000476, 130727.66 tokens/sec
2025-03-22 07:35:06,960 - transformer_training - INFO - Main loop iteration: 4561
2025-03-22 07:35:07,221 - transformer_training - INFO - Main loop iteration: 4562
2025-03-22 07:35:07,482 - transformer_training - INFO - Main loop iteration: 4563
2025-03-22 07:35:07,760 - transformer_training - INFO - Main loop iteration: 4564
2025-03-22 07:35:08,011 - transformer_training - INFO - Main loop iteration: 4565
2025-03-22 07:35:08,272 - transformer_training - INFO - Main loop iteration: 4566
2025-03-22 07:35:08,644 - transformer_training - INFO - Main loop iteration: 4567
2025-03-22 07:35:08,922 - transformer_training - INFO - Main loop iteration: 4568
2025-03-22 07:35:09,173 - transformer_training - INFO - Main loop iteration: 4569
2025-03-22 07:35:09,435 - transformer_training - INFO - Main loop iteration: 4570
Iter 4570: loss 4.2208, lr 0.000475, 126409.13 tokens/sec
2025-03-22 07:35:09,695 - transformer_training - INFO - Main loop iteration: 4571
2025-03-22 07:35:09,973 - transformer_training - INFO - Main loop iteration: 4572
2025-03-22 07:35:10,224 - transformer_training - INFO - Main loop iteration: 4573
2025-03-22 07:35:10,486 - transformer_training - INFO - Main loop iteration: 4574
2025-03-22 07:35:10,745 - transformer_training - INFO - Main loop iteration: 4575
2025-03-22 07:35:11,023 - transformer_training - INFO - Main loop iteration: 4576
2025-03-22 07:35:11,274 - transformer_training - INFO - Main loop iteration: 4577
2025-03-22 07:35:11,557 - transformer_training - INFO - Main loop iteration: 4578
2025-03-22 07:35:11,816 - transformer_training - INFO - Main loop iteration: 4579
2025-03-22 07:35:12,094 - transformer_training - INFO - Main loop iteration: 4580
Iter 4580: loss 4.2600, lr 0.000475, 130969.83 tokens/sec
2025-03-22 07:35:12,345 - transformer_training - INFO - Main loop iteration: 4581
2025-03-22 07:35:12,606 - transformer_training - INFO - Main loop iteration: 4582
2025-03-22 07:35:12,866 - transformer_training - INFO - Main loop iteration: 4583
2025-03-22 07:35:13,143 - transformer_training - INFO - Main loop iteration: 4584
2025-03-22 07:35:13,394 - transformer_training - INFO - Main loop iteration: 4585
2025-03-22 07:35:13,656 - transformer_training - INFO - Main loop iteration: 4586
2025-03-22 07:35:13,915 - transformer_training - INFO - Main loop iteration: 4587
2025-03-22 07:35:14,193 - transformer_training - INFO - Main loop iteration: 4588
2025-03-22 07:35:14,444 - transformer_training - INFO - Main loop iteration: 4589
2025-03-22 07:35:14,706 - transformer_training - INFO - Main loop iteration: 4590
Iter 4590: loss 4.2144, lr 0.000475, 126445.53 tokens/sec
2025-03-22 07:35:14,966 - transformer_training - INFO - Main loop iteration: 4591
2025-03-22 07:35:15,243 - transformer_training - INFO - Main loop iteration: 4592
2025-03-22 07:35:15,494 - transformer_training - INFO - Main loop iteration: 4593
2025-03-22 07:35:15,756 - transformer_training - INFO - Main loop iteration: 4594
2025-03-22 07:35:16,015 - transformer_training - INFO - Main loop iteration: 4595
2025-03-22 07:35:16,293 - transformer_training - INFO - Main loop iteration: 4596
2025-03-22 07:35:16,543 - transformer_training - INFO - Main loop iteration: 4597
2025-03-22 07:35:16,805 - transformer_training - INFO - Main loop iteration: 4598
2025-03-22 07:35:17,065 - transformer_training - INFO - Main loop iteration: 4599
2025-03-22 07:35:17,343 - transformer_training - INFO - Main loop iteration: 4600
Iter 4600: loss 4.1912, lr 0.000475, 130866.82 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 4600: train loss 4.1352, val loss 4.0496
New best model saved with val loss: 4.0496
2025-03-22 07:35:37,910 - transformer_training - INFO - Main loop iteration: 4601
2025-03-22 07:35:38,187 - transformer_training - INFO - Main loop iteration: 4602
2025-03-22 07:35:38,448 - transformer_training - INFO - Main loop iteration: 4603
2025-03-22 07:35:38,725 - transformer_training - INFO - Main loop iteration: 4604
2025-03-22 07:35:38,976 - transformer_training - INFO - Main loop iteration: 4605
2025-03-22 07:35:39,238 - transformer_training - INFO - Main loop iteration: 4606
2025-03-22 07:35:39,504 - transformer_training - INFO - Main loop iteration: 4607
2025-03-22 07:35:39,788 - transformer_training - INFO - Main loop iteration: 4608
2025-03-22 07:35:40,039 - transformer_training - INFO - Main loop iteration: 4609
2025-03-22 07:35:40,300 - transformer_training - INFO - Main loop iteration: 4610
Iter 4610: loss 4.3340, lr 0.000475, 123495.90 tokens/sec
2025-03-22 07:35:40,567 - transformer_training - INFO - Main loop iteration: 4611
2025-03-22 07:35:40,845 - transformer_training - INFO - Main loop iteration: 4612
2025-03-22 07:35:41,096 - transformer_training - INFO - Main loop iteration: 4613
2025-03-22 07:35:41,357 - transformer_training - INFO - Main loop iteration: 4614
2025-03-22 07:35:41,623 - transformer_training - INFO - Main loop iteration: 4615
2025-03-22 07:35:41,901 - transformer_training - INFO - Main loop iteration: 4616
2025-03-22 07:35:42,152 - transformer_training - INFO - Main loop iteration: 4617
2025-03-22 07:35:42,413 - transformer_training - INFO - Main loop iteration: 4618
2025-03-22 07:35:42,678 - transformer_training - INFO - Main loop iteration: 4619
2025-03-22 07:35:42,955 - transformer_training - INFO - Main loop iteration: 4620
Iter 4620: loss 4.4253, lr 0.000474, 130576.88 tokens/sec
2025-03-22 07:35:43,207 - transformer_training - INFO - Main loop iteration: 4621
2025-03-22 07:35:43,469 - transformer_training - INFO - Main loop iteration: 4622
2025-03-22 07:35:43,732 - transformer_training - INFO - Main loop iteration: 4623
2025-03-22 07:35:44,010 - transformer_training - INFO - Main loop iteration: 4624
2025-03-22 07:35:44,261 - transformer_training - INFO - Main loop iteration: 4625
2025-03-22 07:35:44,522 - transformer_training - INFO - Main loop iteration: 4626
2025-03-22 07:35:44,782 - transformer_training - INFO - Main loop iteration: 4627
2025-03-22 07:35:45,060 - transformer_training - INFO - Main loop iteration: 4628
2025-03-22 07:35:45,311 - transformer_training - INFO - Main loop iteration: 4629
2025-03-22 07:35:45,572 - transformer_training - INFO - Main loop iteration: 4630
Iter 4630: loss 4.0460, lr 0.000474, 126367.98 tokens/sec
2025-03-22 07:35:45,832 - transformer_training - INFO - Main loop iteration: 4631
2025-03-22 07:35:46,110 - transformer_training - INFO - Main loop iteration: 4632
2025-03-22 07:35:46,362 - transformer_training - INFO - Main loop iteration: 4633
2025-03-22 07:35:46,623 - transformer_training - INFO - Main loop iteration: 4634
2025-03-22 07:35:46,887 - transformer_training - INFO - Main loop iteration: 4635
2025-03-22 07:35:47,166 - transformer_training - INFO - Main loop iteration: 4636
2025-03-22 07:35:47,417 - transformer_training - INFO - Main loop iteration: 4637
2025-03-22 07:35:47,678 - transformer_training - INFO - Main loop iteration: 4638
2025-03-22 07:35:47,938 - transformer_training - INFO - Main loop iteration: 4639
2025-03-22 07:35:48,216 - transformer_training - INFO - Main loop iteration: 4640
Iter 4640: loss 4.1862, lr 0.000474, 130699.81 tokens/sec
2025-03-22 07:35:48,469 - transformer_training - INFO - Main loop iteration: 4641
2025-03-22 07:35:48,729 - transformer_training - INFO - Main loop iteration: 4642
2025-03-22 07:35:48,990 - transformer_training - INFO - Main loop iteration: 4643
2025-03-22 07:35:49,268 - transformer_training - INFO - Main loop iteration: 4644
2025-03-22 07:35:49,519 - transformer_training - INFO - Main loop iteration: 4645
2025-03-22 07:35:49,780 - transformer_training - INFO - Main loop iteration: 4646
2025-03-22 07:35:50,040 - transformer_training - INFO - Main loop iteration: 4647
2025-03-22 07:35:50,318 - transformer_training - INFO - Main loop iteration: 4648
2025-03-22 07:35:50,569 - transformer_training - INFO - Main loop iteration: 4649
2025-03-22 07:35:50,831 - transformer_training - INFO - Main loop iteration: 4650
Iter 4650: loss 4.1648, lr 0.000474, 126345.79 tokens/sec
2025-03-22 07:35:51,091 - transformer_training - INFO - Main loop iteration: 4651
2025-03-22 07:35:51,368 - transformer_training - INFO - Main loop iteration: 4652
2025-03-22 07:35:51,619 - transformer_training - INFO - Main loop iteration: 4653
2025-03-22 07:35:51,881 - transformer_training - INFO - Main loop iteration: 4654
2025-03-22 07:35:52,141 - transformer_training - INFO - Main loop iteration: 4655
2025-03-22 07:35:52,419 - transformer_training - INFO - Main loop iteration: 4656
2025-03-22 07:35:52,670 - transformer_training - INFO - Main loop iteration: 4657
2025-03-22 07:35:52,931 - transformer_training - INFO - Main loop iteration: 4658
2025-03-22 07:35:53,192 - transformer_training - INFO - Main loop iteration: 4659
2025-03-22 07:35:53,470 - transformer_training - INFO - Main loop iteration: 4660
Iter 4660: loss 4.2286, lr 0.000474, 130984.56 tokens/sec
2025-03-22 07:35:53,721 - transformer_training - INFO - Main loop iteration: 4661
2025-03-22 07:35:53,982 - transformer_training - INFO - Main loop iteration: 4662
2025-03-22 07:35:54,247 - transformer_training - INFO - Main loop iteration: 4663
2025-03-22 07:35:54,525 - transformer_training - INFO - Main loop iteration: 4664
2025-03-22 07:35:54,776 - transformer_training - INFO - Main loop iteration: 4665
2025-03-22 07:35:55,037 - transformer_training - INFO - Main loop iteration: 4666
2025-03-22 07:35:55,298 - transformer_training - INFO - Main loop iteration: 4667
2025-03-22 07:35:55,576 - transformer_training - INFO - Main loop iteration: 4668
2025-03-22 07:35:55,827 - transformer_training - INFO - Main loop iteration: 4669
2025-03-22 07:35:56,088 - transformer_training - INFO - Main loop iteration: 4670
Iter 4670: loss 4.2930, lr 0.000473, 126227.55 tokens/sec
2025-03-22 07:35:56,349 - transformer_training - INFO - Main loop iteration: 4671
2025-03-22 07:35:56,633 - transformer_training - INFO - Main loop iteration: 4672
2025-03-22 07:35:56,884 - transformer_training - INFO - Main loop iteration: 4673
2025-03-22 07:35:57,145 - transformer_training - INFO - Main loop iteration: 4674
2025-03-22 07:35:57,405 - transformer_training - INFO - Main loop iteration: 4675
2025-03-22 07:35:57,683 - transformer_training - INFO - Main loop iteration: 4676
2025-03-22 07:35:57,934 - transformer_training - INFO - Main loop iteration: 4677
2025-03-22 07:35:58,195 - transformer_training - INFO - Main loop iteration: 4678
2025-03-22 07:35:58,455 - transformer_training - INFO - Main loop iteration: 4679
2025-03-22 07:35:58,733 - transformer_training - INFO - Main loop iteration: 4680
Iter 4680: loss 4.1747, lr 0.000473, 130871.68 tokens/sec
2025-03-22 07:35:58,984 - transformer_training - INFO - Main loop iteration: 4681
2025-03-22 07:35:59,245 - transformer_training - INFO - Main loop iteration: 4682
2025-03-22 07:35:59,505 - transformer_training - INFO - Main loop iteration: 4683
2025-03-22 07:35:59,783 - transformer_training - INFO - Main loop iteration: 4684
2025-03-22 07:36:00,033 - transformer_training - INFO - Main loop iteration: 4685
2025-03-22 07:36:00,295 - transformer_training - INFO - Main loop iteration: 4686
2025-03-22 07:36:00,555 - transformer_training - INFO - Main loop iteration: 4687
2025-03-22 07:36:00,833 - transformer_training - INFO - Main loop iteration: 4688
2025-03-22 07:36:01,083 - transformer_training - INFO - Main loop iteration: 4689
2025-03-22 07:36:01,345 - transformer_training - INFO - Main loop iteration: 4690
Iter 4690: loss 4.2942, lr 0.000473, 126395.87 tokens/sec
2025-03-22 07:36:01,606 - transformer_training - INFO - Main loop iteration: 4691
2025-03-22 07:36:01,883 - transformer_training - INFO - Main loop iteration: 4692
2025-03-22 07:36:02,134 - transformer_training - INFO - Main loop iteration: 4693
2025-03-22 07:36:02,396 - transformer_training - INFO - Main loop iteration: 4694
2025-03-22 07:36:02,656 - transformer_training - INFO - Main loop iteration: 4695
2025-03-22 07:36:02,934 - transformer_training - INFO - Main loop iteration: 4696
2025-03-22 07:36:03,184 - transformer_training - INFO - Main loop iteration: 4697
2025-03-22 07:36:03,446 - transformer_training - INFO - Main loop iteration: 4698
2025-03-22 07:36:03,706 - transformer_training - INFO - Main loop iteration: 4699
2025-03-22 07:36:03,984 - transformer_training - INFO - Main loop iteration: 4700
Iter 4700: loss 4.0701, lr 0.000473, 130885.64 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 4700: train loss 4.1190, val loss 4.0285
New best model saved with val loss: 4.0285
2025-03-22 07:36:24,721 - transformer_training - INFO - Main loop iteration: 4701
2025-03-22 07:36:24,999 - transformer_training - INFO - Main loop iteration: 4702
2025-03-22 07:36:25,260 - transformer_training - INFO - Main loop iteration: 4703
2025-03-22 07:36:25,537 - transformer_training - INFO - Main loop iteration: 4704
2025-03-22 07:36:25,788 - transformer_training - INFO - Main loop iteration: 4705
2025-03-22 07:36:26,050 - transformer_training - INFO - Main loop iteration: 4706
2025-03-22 07:36:26,316 - transformer_training - INFO - Main loop iteration: 4707
2025-03-22 07:36:26,601 - transformer_training - INFO - Main loop iteration: 4708
2025-03-22 07:36:26,852 - transformer_training - INFO - Main loop iteration: 4709
2025-03-22 07:36:27,113 - transformer_training - INFO - Main loop iteration: 4710
Iter 4710: loss 4.1661, lr 0.000473, 124151.18 tokens/sec
2025-03-22 07:36:27,378 - transformer_training - INFO - Main loop iteration: 4711
2025-03-22 07:36:27,656 - transformer_training - INFO - Main loop iteration: 4712
2025-03-22 07:36:27,907 - transformer_training - INFO - Main loop iteration: 4713
2025-03-22 07:36:28,168 - transformer_training - INFO - Main loop iteration: 4714
2025-03-22 07:36:28,432 - transformer_training - INFO - Main loop iteration: 4715
2025-03-22 07:36:28,709 - transformer_training - INFO - Main loop iteration: 4716
2025-03-22 07:36:28,962 - transformer_training - INFO - Main loop iteration: 4717
2025-03-22 07:36:29,223 - transformer_training - INFO - Main loop iteration: 4718
2025-03-22 07:36:29,482 - transformer_training - INFO - Main loop iteration: 4719
2025-03-22 07:36:29,759 - transformer_training - INFO - Main loop iteration: 4720
Iter 4720: loss 4.2293, lr 0.000472, 131725.49 tokens/sec
2025-03-22 07:36:30,008 - transformer_training - INFO - Main loop iteration: 4721
2025-03-22 07:36:30,269 - transformer_training - INFO - Main loop iteration: 4722
2025-03-22 07:36:30,528 - transformer_training - INFO - Main loop iteration: 4723
2025-03-22 07:36:30,805 - transformer_training - INFO - Main loop iteration: 4724
2025-03-22 07:36:31,054 - transformer_training - INFO - Main loop iteration: 4725
2025-03-22 07:36:31,314 - transformer_training - INFO - Main loop iteration: 4726
2025-03-22 07:36:31,573 - transformer_training - INFO - Main loop iteration: 4727
2025-03-22 07:36:31,851 - transformer_training - INFO - Main loop iteration: 4728
2025-03-22 07:36:32,100 - transformer_training - INFO - Main loop iteration: 4729
2025-03-22 07:36:32,361 - transformer_training - INFO - Main loop iteration: 4730
Iter 4730: loss 4.1059, lr 0.000472, 126802.74 tokens/sec
2025-03-22 07:36:32,620 - transformer_training - INFO - Main loop iteration: 4731
2025-03-22 07:36:32,898 - transformer_training - INFO - Main loop iteration: 4732
2025-03-22 07:36:33,148 - transformer_training - INFO - Main loop iteration: 4733
2025-03-22 07:36:33,408 - transformer_training - INFO - Main loop iteration: 4734
2025-03-22 07:36:33,667 - transformer_training - INFO - Main loop iteration: 4735
2025-03-22 07:36:33,944 - transformer_training - INFO - Main loop iteration: 4736
2025-03-22 07:36:34,312 - transformer_training - INFO - Main loop iteration: 4737
2025-03-22 07:36:34,572 - transformer_training - INFO - Main loop iteration: 4738
2025-03-22 07:36:34,831 - transformer_training - INFO - Main loop iteration: 4739
2025-03-22 07:36:35,108 - transformer_training - INFO - Main loop iteration: 4740
Iter 4740: loss 4.2893, lr 0.000472, 129684.96 tokens/sec
2025-03-22 07:36:35,361 - transformer_training - INFO - Main loop iteration: 4741
2025-03-22 07:36:35,622 - transformer_training - INFO - Main loop iteration: 4742
2025-03-22 07:36:35,880 - transformer_training - INFO - Main loop iteration: 4743
2025-03-22 07:36:36,158 - transformer_training - INFO - Main loop iteration: 4744
2025-03-22 07:36:36,408 - transformer_training - INFO - Main loop iteration: 4745
2025-03-22 07:36:36,668 - transformer_training - INFO - Main loop iteration: 4746
2025-03-22 07:36:36,927 - transformer_training - INFO - Main loop iteration: 4747
2025-03-22 07:36:37,204 - transformer_training - INFO - Main loop iteration: 4748
2025-03-22 07:36:37,454 - transformer_training - INFO - Main loop iteration: 4749
2025-03-22 07:36:37,714 - transformer_training - INFO - Main loop iteration: 4750
Iter 4750: loss 4.1266, lr 0.000472, 127150.68 tokens/sec
2025-03-22 07:36:37,973 - transformer_training - INFO - Main loop iteration: 4751
2025-03-22 07:36:38,250 - transformer_training - INFO - Main loop iteration: 4752
2025-03-22 07:36:38,499 - transformer_training - INFO - Main loop iteration: 4753
2025-03-22 07:36:38,760 - transformer_training - INFO - Main loop iteration: 4754
2025-03-22 07:36:39,019 - transformer_training - INFO - Main loop iteration: 4755
2025-03-22 07:36:39,296 - transformer_training - INFO - Main loop iteration: 4756
2025-03-22 07:36:39,545 - transformer_training - INFO - Main loop iteration: 4757
2025-03-22 07:36:39,805 - transformer_training - INFO - Main loop iteration: 4758
2025-03-22 07:36:40,064 - transformer_training - INFO - Main loop iteration: 4759
2025-03-22 07:36:40,341 - transformer_training - INFO - Main loop iteration: 4760
Iter 4760: loss 4.0771, lr 0.000472, 131646.38 tokens/sec
2025-03-22 07:36:40,590 - transformer_training - INFO - Main loop iteration: 4761
2025-03-22 07:36:40,851 - transformer_training - INFO - Main loop iteration: 4762
2025-03-22 07:36:41,110 - transformer_training - INFO - Main loop iteration: 4763
2025-03-22 07:36:41,387 - transformer_training - INFO - Main loop iteration: 4764
2025-03-22 07:36:41,636 - transformer_training - INFO - Main loop iteration: 4765
2025-03-22 07:36:41,897 - transformer_training - INFO - Main loop iteration: 4766
2025-03-22 07:36:42,156 - transformer_training - INFO - Main loop iteration: 4767
2025-03-22 07:36:42,432 - transformer_training - INFO - Main loop iteration: 4768
2025-03-22 07:36:42,682 - transformer_training - INFO - Main loop iteration: 4769
2025-03-22 07:36:42,942 - transformer_training - INFO - Main loop iteration: 4770
Iter 4770: loss 4.2321, lr 0.000471, 127019.89 tokens/sec
2025-03-22 07:36:43,201 - transformer_training - INFO - Main loop iteration: 4771
2025-03-22 07:36:43,478 - transformer_training - INFO - Main loop iteration: 4772
2025-03-22 07:36:43,728 - transformer_training - INFO - Main loop iteration: 4773
2025-03-22 07:36:43,988 - transformer_training - INFO - Main loop iteration: 4774
2025-03-22 07:36:44,247 - transformer_training - INFO - Main loop iteration: 4775
2025-03-22 07:36:44,524 - transformer_training - INFO - Main loop iteration: 4776
2025-03-22 07:36:44,774 - transformer_training - INFO - Main loop iteration: 4777
2025-03-22 07:36:45,034 - transformer_training - INFO - Main loop iteration: 4778
2025-03-22 07:36:45,293 - transformer_training - INFO - Main loop iteration: 4779
2025-03-22 07:36:45,570 - transformer_training - INFO - Main loop iteration: 4780
Iter 4780: loss 4.3162, lr 0.000471, 131536.26 tokens/sec
2025-03-22 07:36:45,820 - transformer_training - INFO - Main loop iteration: 4781
2025-03-22 07:36:46,080 - transformer_training - INFO - Main loop iteration: 4782
2025-03-22 07:36:46,339 - transformer_training - INFO - Main loop iteration: 4783
2025-03-22 07:36:46,616 - transformer_training - INFO - Main loop iteration: 4784
2025-03-22 07:36:46,866 - transformer_training - INFO - Main loop iteration: 4785
2025-03-22 07:36:47,126 - transformer_training - INFO - Main loop iteration: 4786
2025-03-22 07:36:47,385 - transformer_training - INFO - Main loop iteration: 4787
2025-03-22 07:36:47,662 - transformer_training - INFO - Main loop iteration: 4788
2025-03-22 07:36:47,912 - transformer_training - INFO - Main loop iteration: 4789
2025-03-22 07:36:48,173 - transformer_training - INFO - Main loop iteration: 4790
Iter 4790: loss 4.1570, lr 0.000471, 127026.70 tokens/sec
2025-03-22 07:36:48,432 - transformer_training - INFO - Main loop iteration: 4791
2025-03-22 07:36:48,708 - transformer_training - INFO - Main loop iteration: 4792
2025-03-22 07:36:48,958 - transformer_training - INFO - Main loop iteration: 4793
2025-03-22 07:36:49,218 - transformer_training - INFO - Main loop iteration: 4794
2025-03-22 07:36:49,477 - transformer_training - INFO - Main loop iteration: 4795
2025-03-22 07:36:49,754 - transformer_training - INFO - Main loop iteration: 4796
2025-03-22 07:36:50,004 - transformer_training - INFO - Main loop iteration: 4797
2025-03-22 07:36:50,264 - transformer_training - INFO - Main loop iteration: 4798
2025-03-22 07:36:50,523 - transformer_training - INFO - Main loop iteration: 4799
2025-03-22 07:36:50,800 - transformer_training - INFO - Main loop iteration: 4800
Iter 4800: loss 4.1547, lr 0.000471, 129563.93 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 4800: train loss 4.0987, val loss 3.9951
New best model saved with val loss: 3.9951
2025-03-22 07:37:11,279 - transformer_training - INFO - Main loop iteration: 4801
2025-03-22 07:37:11,531 - transformer_training - INFO - Main loop iteration: 4802
2025-03-22 07:37:11,790 - transformer_training - INFO - Main loop iteration: 4803
2025-03-22 07:37:12,066 - transformer_training - INFO - Main loop iteration: 4804
2025-03-22 07:37:12,316 - transformer_training - INFO - Main loop iteration: 4805
2025-03-22 07:37:12,576 - transformer_training - INFO - Main loop iteration: 4806
2025-03-22 07:37:12,835 - transformer_training - INFO - Main loop iteration: 4807
2025-03-22 07:37:13,112 - transformer_training - INFO - Main loop iteration: 4808
2025-03-22 07:37:13,362 - transformer_training - INFO - Main loop iteration: 4809
2025-03-22 07:37:13,622 - transformer_training - INFO - Main loop iteration: 4810
Iter 4810: loss 4.1637, lr 0.000471, 126905.54 tokens/sec
2025-03-22 07:37:13,881 - transformer_training - INFO - Main loop iteration: 4811
2025-03-22 07:37:14,158 - transformer_training - INFO - Main loop iteration: 4812
2025-03-22 07:37:14,407 - transformer_training - INFO - Main loop iteration: 4813
2025-03-22 07:37:14,668 - transformer_training - INFO - Main loop iteration: 4814
2025-03-22 07:37:14,926 - transformer_training - INFO - Main loop iteration: 4815
2025-03-22 07:37:15,203 - transformer_training - INFO - Main loop iteration: 4816
2025-03-22 07:37:15,452 - transformer_training - INFO - Main loop iteration: 4817
2025-03-22 07:37:15,712 - transformer_training - INFO - Main loop iteration: 4818
2025-03-22 07:37:15,972 - transformer_training - INFO - Main loop iteration: 4819
2025-03-22 07:37:16,249 - transformer_training - INFO - Main loop iteration: 4820
Iter 4820: loss 4.2760, lr 0.000470, 131481.90 tokens/sec
2025-03-22 07:37:16,499 - transformer_training - INFO - Main loop iteration: 4821
2025-03-22 07:37:16,759 - transformer_training - INFO - Main loop iteration: 4822
2025-03-22 07:37:17,018 - transformer_training - INFO - Main loop iteration: 4823
2025-03-22 07:37:17,295 - transformer_training - INFO - Main loop iteration: 4824
2025-03-22 07:37:17,544 - transformer_training - INFO - Main loop iteration: 4825
2025-03-22 07:37:17,805 - transformer_training - INFO - Main loop iteration: 4826
2025-03-22 07:37:18,064 - transformer_training - INFO - Main loop iteration: 4827
2025-03-22 07:37:18,341 - transformer_training - INFO - Main loop iteration: 4828
2025-03-22 07:37:18,591 - transformer_training - INFO - Main loop iteration: 4829
2025-03-22 07:37:18,851 - transformer_training - INFO - Main loop iteration: 4830
Iter 4830: loss 4.1415, lr 0.000470, 126927.69 tokens/sec
2025-03-22 07:37:19,110 - transformer_training - INFO - Main loop iteration: 4831
2025-03-22 07:37:19,387 - transformer_training - INFO - Main loop iteration: 4832
2025-03-22 07:37:19,636 - transformer_training - INFO - Main loop iteration: 4833
2025-03-22 07:37:19,897 - transformer_training - INFO - Main loop iteration: 4834
2025-03-22 07:37:20,155 - transformer_training - INFO - Main loop iteration: 4835
2025-03-22 07:37:20,432 - transformer_training - INFO - Main loop iteration: 4836
2025-03-22 07:37:20,682 - transformer_training - INFO - Main loop iteration: 4837
2025-03-22 07:37:20,942 - transformer_training - INFO - Main loop iteration: 4838
2025-03-22 07:37:21,201 - transformer_training - INFO - Main loop iteration: 4839
2025-03-22 07:37:21,478 - transformer_training - INFO - Main loop iteration: 4840
Iter 4840: loss 4.1685, lr 0.000470, 131558.80 tokens/sec
2025-03-22 07:37:21,728 - transformer_training - INFO - Main loop iteration: 4841
2025-03-22 07:37:21,988 - transformer_training - INFO - Main loop iteration: 4842
2025-03-22 07:37:22,247 - transformer_training - INFO - Main loop iteration: 4843
2025-03-22 07:37:22,524 - transformer_training - INFO - Main loop iteration: 4844
2025-03-22 07:37:22,773 - transformer_training - INFO - Main loop iteration: 4845
2025-03-22 07:37:23,034 - transformer_training - INFO - Main loop iteration: 4846
2025-03-22 07:37:23,293 - transformer_training - INFO - Main loop iteration: 4847
2025-03-22 07:37:23,570 - transformer_training - INFO - Main loop iteration: 4848
2025-03-22 07:37:23,819 - transformer_training - INFO - Main loop iteration: 4849
2025-03-22 07:37:24,079 - transformer_training - INFO - Main loop iteration: 4850
Iter 4850: loss 4.1776, lr 0.000470, 127077.68 tokens/sec
2025-03-22 07:37:24,338 - transformer_training - INFO - Main loop iteration: 4851
2025-03-22 07:37:24,614 - transformer_training - INFO - Main loop iteration: 4852
2025-03-22 07:37:24,864 - transformer_training - INFO - Main loop iteration: 4853
2025-03-22 07:37:25,125 - transformer_training - INFO - Main loop iteration: 4854
2025-03-22 07:37:25,383 - transformer_training - INFO - Main loop iteration: 4855
2025-03-22 07:37:25,660 - transformer_training - INFO - Main loop iteration: 4856
2025-03-22 07:37:25,910 - transformer_training - INFO - Main loop iteration: 4857
2025-03-22 07:37:26,170 - transformer_training - INFO - Main loop iteration: 4858
2025-03-22 07:37:26,429 - transformer_training - INFO - Main loop iteration: 4859
2025-03-22 07:37:26,706 - transformer_training - INFO - Main loop iteration: 4860
Iter 4860: loss 4.1085, lr 0.000470, 131694.32 tokens/sec
2025-03-22 07:37:26,955 - transformer_training - INFO - Main loop iteration: 4861
2025-03-22 07:37:27,216 - transformer_training - INFO - Main loop iteration: 4862
2025-03-22 07:37:27,475 - transformer_training - INFO - Main loop iteration: 4863
2025-03-22 07:37:27,752 - transformer_training - INFO - Main loop iteration: 4864
2025-03-22 07:37:28,001 - transformer_training - INFO - Main loop iteration: 4865
2025-03-22 07:37:28,261 - transformer_training - INFO - Main loop iteration: 4866
2025-03-22 07:37:28,520 - transformer_training - INFO - Main loop iteration: 4867
2025-03-22 07:37:28,797 - transformer_training - INFO - Main loop iteration: 4868
2025-03-22 07:37:29,047 - transformer_training - INFO - Main loop iteration: 4869
2025-03-22 07:37:29,307 - transformer_training - INFO - Main loop iteration: 4870
Iter 4870: loss 4.3105, lr 0.000469, 126887.03 tokens/sec
2025-03-22 07:37:29,566 - transformer_training - INFO - Main loop iteration: 4871
2025-03-22 07:37:29,843 - transformer_training - INFO - Main loop iteration: 4872
2025-03-22 07:37:30,092 - transformer_training - INFO - Main loop iteration: 4873
2025-03-22 07:37:30,353 - transformer_training - INFO - Main loop iteration: 4874
2025-03-22 07:37:30,611 - transformer_training - INFO - Main loop iteration: 4875
2025-03-22 07:37:30,888 - transformer_training - INFO - Main loop iteration: 4876
2025-03-22 07:37:31,137 - transformer_training - INFO - Main loop iteration: 4877
2025-03-22 07:37:31,397 - transformer_training - INFO - Main loop iteration: 4878
2025-03-22 07:37:31,656 - transformer_training - INFO - Main loop iteration: 4879
2025-03-22 07:37:31,933 - transformer_training - INFO - Main loop iteration: 4880
Iter 4880: loss 4.1499, lr 0.000469, 131613.98 tokens/sec
2025-03-22 07:37:32,183 - transformer_training - INFO - Main loop iteration: 4881
2025-03-22 07:37:32,443 - transformer_training - INFO - Main loop iteration: 4882
2025-03-22 07:37:32,702 - transformer_training - INFO - Main loop iteration: 4883
2025-03-22 07:37:32,979 - transformer_training - INFO - Main loop iteration: 4884
2025-03-22 07:37:33,227 - transformer_training - INFO - Main loop iteration: 4885
2025-03-22 07:37:33,488 - transformer_training - INFO - Main loop iteration: 4886
2025-03-22 07:37:33,747 - transformer_training - INFO - Main loop iteration: 4887
2025-03-22 07:37:34,024 - transformer_training - INFO - Main loop iteration: 4888
2025-03-22 07:37:34,273 - transformer_training - INFO - Main loop iteration: 4889
2025-03-22 07:37:34,535 - transformer_training - INFO - Main loop iteration: 4890
Iter 4890: loss 4.1466, lr 0.000469, 126920.66 tokens/sec
2025-03-22 07:37:34,794 - transformer_training - INFO - Main loop iteration: 4891
2025-03-22 07:37:35,070 - transformer_training - INFO - Main loop iteration: 4892
2025-03-22 07:37:35,319 - transformer_training - INFO - Main loop iteration: 4893
2025-03-22 07:37:35,580 - transformer_training - INFO - Main loop iteration: 4894
2025-03-22 07:37:35,839 - transformer_training - INFO - Main loop iteration: 4895
2025-03-22 07:37:36,116 - transformer_training - INFO - Main loop iteration: 4896
2025-03-22 07:37:36,365 - transformer_training - INFO - Main loop iteration: 4897
2025-03-22 07:37:36,625 - transformer_training - INFO - Main loop iteration: 4898
2025-03-22 07:37:36,884 - transformer_training - INFO - Main loop iteration: 4899
2025-03-22 07:37:37,161 - transformer_training - INFO - Main loop iteration: 4900
Iter 4900: loss 4.2204, lr 0.000469, 131568.12 tokens/sec
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 4900: train loss 4.0764, val loss 3.9707
New best model saved with val loss: 3.9707
2025-03-22 07:37:57,826 - transformer_training - INFO - Main loop iteration: 4901
2025-03-22 07:37:58,079 - transformer_training - INFO - Main loop iteration: 4902
2025-03-22 07:37:58,337 - transformer_training - INFO - Main loop iteration: 4903
2025-03-22 07:37:58,615 - transformer_training - INFO - Main loop iteration: 4904
2025-03-22 07:37:58,863 - transformer_training - INFO - Main loop iteration: 4905
2025-03-22 07:37:59,124 - transformer_training - INFO - Main loop iteration: 4906
2025-03-22 07:37:59,382 - transformer_training - INFO - Main loop iteration: 4907
2025-03-22 07:37:59,660 - transformer_training - INFO - Main loop iteration: 4908
2025-03-22 07:37:59,909 - transformer_training - INFO - Main loop iteration: 4909
2025-03-22 07:38:00,284 - transformer_training - INFO - Main loop iteration: 4910
Iter 4910: loss 4.1643, lr 0.000469, 127319.47 tokens/sec
2025-03-22 07:38:00,542 - transformer_training - INFO - Main loop iteration: 4911
2025-03-22 07:38:00,819 - transformer_training - INFO - Main loop iteration: 4912
2025-03-22 07:38:01,068 - transformer_training - INFO - Main loop iteration: 4913
2025-03-22 07:38:01,328 - transformer_training - INFO - Main loop iteration: 4914
2025-03-22 07:38:01,587 - transformer_training - INFO - Main loop iteration: 4915
2025-03-22 07:38:01,864 - transformer_training - INFO - Main loop iteration: 4916
2025-03-22 07:38:02,113 - transformer_training - INFO - Main loop iteration: 4917
2025-03-22 07:38:02,374 - transformer_training - INFO - Main loop iteration: 4918
2025-03-22 07:38:02,632 - transformer_training - INFO - Main loop iteration: 4919
2025-03-22 07:38:02,909 - transformer_training - INFO - Main loop iteration: 4920
Iter 4920: loss 4.2504, lr 0.000468, 131820.50 tokens/sec
2025-03-22 07:38:03,158 - transformer_training - INFO - Main loop iteration: 4921
2025-03-22 07:38:03,418 - transformer_training - INFO - Main loop iteration: 4922
2025-03-22 07:38:03,677 - transformer_training - INFO - Main loop iteration: 4923
2025-03-22 07:38:03,955 - transformer_training - INFO - Main loop iteration: 4924
2025-03-22 07:38:04,204 - transformer_training - INFO - Main loop iteration: 4925
2025-03-22 07:38:04,465 - transformer_training - INFO - Main loop iteration: 4926
2025-03-22 07:38:04,723 - transformer_training - INFO - Main loop iteration: 4927
2025-03-22 07:38:05,000 - transformer_training - INFO - Main loop iteration: 4928
2025-03-22 07:38:05,249 - transformer_training - INFO - Main loop iteration: 4929
2025-03-22 07:38:05,510 - transformer_training - INFO - Main loop iteration: 4930
Iter 4930: loss 4.2095, lr 0.000468, 126583.76 tokens/sec
2025-03-22 07:38:05,770 - transformer_training - INFO - Main loop iteration: 4931
2025-03-22 07:38:06,054 - transformer_training - INFO - Main loop iteration: 4932
2025-03-22 07:38:06,307 - transformer_training - INFO - Main loop iteration: 4933
2025-03-22 07:38:06,567 - transformer_training - INFO - Main loop iteration: 4934
2025-03-22 07:38:06,827 - transformer_training - INFO - Main loop iteration: 4935
2025-03-22 07:38:07,104 - transformer_training - INFO - Main loop iteration: 4936
2025-03-22 07:38:07,353 - transformer_training - INFO - Main loop iteration: 4937
2025-03-22 07:38:07,614 - transformer_training - INFO - Main loop iteration: 4938
2025-03-22 07:38:07,872 - transformer_training - INFO - Main loop iteration: 4939
2025-03-22 07:38:08,149 - transformer_training - INFO - Main loop iteration: 4940
Iter 4940: loss 4.1415, lr 0.000468, 131716.53 tokens/sec
2025-03-22 07:38:08,399 - transformer_training - INFO - Main loop iteration: 4941
2025-03-22 07:38:08,659 - transformer_training - INFO - Main loop iteration: 4942
2025-03-22 07:38:08,918 - transformer_training - INFO - Main loop iteration: 4943
2025-03-22 07:38:09,195 - transformer_training - INFO - Main loop iteration: 4944
2025-03-22 07:38:09,445 - transformer_training - INFO - Main loop iteration: 4945
2025-03-22 07:38:09,705 - transformer_training - INFO - Main loop iteration: 4946
2025-03-22 07:38:09,963 - transformer_training - INFO - Main loop iteration: 4947
2025-03-22 07:38:10,240 - transformer_training - INFO - Main loop iteration: 4948
2025-03-22 07:38:10,490 - transformer_training - INFO - Main loop iteration: 4949
2025-03-22 07:38:10,751 - transformer_training - INFO - Main loop iteration: 4950
Iter 4950: loss 4.1636, lr 0.000468, 126995.13 tokens/sec
2025-03-22 07:38:11,010 - transformer_training - INFO - Main loop iteration: 4951
2025-03-22 07:38:11,294 - transformer_training - INFO - Main loop iteration: 4952
2025-03-22 07:38:11,543 - transformer_training - INFO - Main loop iteration: 4953
2025-03-22 07:38:11,804 - transformer_training - INFO - Main loop iteration: 4954
2025-03-22 07:38:12,062 - transformer_training - INFO - Main loop iteration: 4955
2025-03-22 07:38:12,343 - transformer_training - INFO - Main loop iteration: 4956
2025-03-22 07:38:12,597 - transformer_training - INFO - Main loop iteration: 4957
2025-03-22 07:38:12,865 - transformer_training - INFO - Main loop iteration: 4958
2025-03-22 07:38:13,129 - transformer_training - INFO - Main loop iteration: 4959
2025-03-22 07:38:13,407 - transformer_training - INFO - Main loop iteration: 4960
Iter 4960: loss 4.0522, lr 0.000467, 130763.35 tokens/sec
2025-03-22 07:38:13,658 - transformer_training - INFO - Main loop iteration: 4961
2025-03-22 07:38:13,925 - transformer_training - INFO - Main loop iteration: 4962
2025-03-22 07:38:14,185 - transformer_training - INFO - Main loop iteration: 4963
2025-03-22 07:38:14,463 - transformer_training - INFO - Main loop iteration: 4964
2025-03-22 07:38:14,714 - transformer_training - INFO - Main loop iteration: 4965
2025-03-22 07:38:14,976 - transformer_training - INFO - Main loop iteration: 4966
2025-03-22 07:38:15,235 - transformer_training - INFO - Main loop iteration: 4967
2025-03-22 07:38:15,513 - transformer_training - INFO - Main loop iteration: 4968
2025-03-22 07:38:15,764 - transformer_training - INFO - Main loop iteration: 4969
2025-03-22 07:38:16,025 - transformer_training - INFO - Main loop iteration: 4970
Iter 4970: loss 4.1492, lr 0.000467, 126504.88 tokens/sec
2025-03-22 07:38:16,286 - transformer_training - INFO - Main loop iteration: 4971
2025-03-22 07:38:16,563 - transformer_training - INFO - Main loop iteration: 4972
2025-03-22 07:38:16,814 - transformer_training - INFO - Main loop iteration: 4973
2025-03-22 07:38:17,076 - transformer_training - INFO - Main loop iteration: 4974
2025-03-22 07:38:17,336 - transformer_training - INFO - Main loop iteration: 4975
2025-03-22 07:38:17,621 - transformer_training - INFO - Main loop iteration: 4976
2025-03-22 07:38:17,871 - transformer_training - INFO - Main loop iteration: 4977
2025-03-22 07:38:18,133 - transformer_training - INFO - Main loop iteration: 4978
2025-03-22 07:38:18,393 - transformer_training - INFO - Main loop iteration: 4979
2025-03-22 07:38:18,671 - transformer_training - INFO - Main loop iteration: 4980
Iter 4980: loss 4.2077, lr 0.000467, 130931.90 tokens/sec
2025-03-22 07:38:18,922 - transformer_training - INFO - Main loop iteration: 4981
2025-03-22 07:38:19,183 - transformer_training - INFO - Main loop iteration: 4982
2025-03-22 07:38:19,443 - transformer_training - INFO - Main loop iteration: 4983
2025-03-22 07:38:19,720 - transformer_training - INFO - Main loop iteration: 4984
2025-03-22 07:38:19,971 - transformer_training - INFO - Main loop iteration: 4985
2025-03-22 07:38:20,233 - transformer_training - INFO - Main loop iteration: 4986
2025-03-22 07:38:20,492 - transformer_training - INFO - Main loop iteration: 4987
2025-03-22 07:38:20,770 - transformer_training - INFO - Main loop iteration: 4988
2025-03-22 07:38:21,021 - transformer_training - INFO - Main loop iteration: 4989
2025-03-22 07:38:21,282 - transformer_training - INFO - Main loop iteration: 4990
Iter 4990: loss 4.2733, lr 0.000467, 126465.77 tokens/sec
2025-03-22 07:38:21,542 - transformer_training - INFO - Main loop iteration: 4991
2025-03-22 07:38:21,819 - transformer_training - INFO - Main loop iteration: 4992
2025-03-22 07:38:22,071 - transformer_training - INFO - Main loop iteration: 4993
2025-03-22 07:38:22,332 - transformer_training - INFO - Main loop iteration: 4994
2025-03-22 07:38:22,592 - transformer_training - INFO - Main loop iteration: 4995
2025-03-22 07:38:22,870 - transformer_training - INFO - Main loop iteration: 4996
2025-03-22 07:38:23,120 - transformer_training - INFO - Main loop iteration: 4997
2025-03-22 07:38:23,383 - transformer_training - INFO - Main loop iteration: 4998
2025-03-22 07:38:23,643 - transformer_training - INFO - Main loop iteration: 4999
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Your base folder is: /workspace/GPT
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Flash Attention is available!
Iter 4999: train loss 4.0603, val loss 3.9556
New best model saved with val loss: 3.9556
Training completed in 2378.15 seconds
[rank0]:[W322 07:38:44.681379441 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/workspace/GPT/models/gpt_a100.py", line 792, in <module>
    main()
  File "/workspace/GPT/models/gpt_a100.py", line 783, in main
    mp.spawn(
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 340, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 296, in start_processes
    while not context.join():
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 215, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 90, in _wrap
    fn(i, *args)
  File "/workspace/GPT/models/gpt_a100.py", line 680, in train
    generated_sequence = model.module.generate(context, max_new_tokens=200, max_seq_len=config.block_size)
  File "/workspace/GPT/models/gpt_a100.py", line 343, in generate
    logits, _ = self(idx_cond)
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/GPT/models/gpt_a100.py", line 320, in forward
    x = block(x)
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/GPT/models/gpt_a100.py", line 238, in forward
    attn_output = self.self_attention(normed_input1)
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/GPT/models/gpt_a100.py", line 147, in forward
    head_outputs = [head(input_tensor) for head in self.heads]
  File "/workspace/GPT/models/gpt_a100.py", line 147, in <listcomp>
    head_outputs = [head(input_tensor) for head in self.heads]
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/GPT/models/gpt_a100.py", line 116, in forward
    output = flash_attn_func(q, k, v, causal=True)
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py", line 1201, in flash_attn_func
    return FlashAttnFunc.apply(
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py", line 839, in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_forward(
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/_ops.py", line 1123, in __call__
    return self._op(*args, **(kwargs or {}))
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/_library/autograd.py", line 113, in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/_library/autograd.py", line 40, in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/_ops.py", line 728, in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/_library/custom_ops.py", line 305, in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/_compile.py", line 32, in inner
    return disable_fn(*args, **kwargs)
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
    return fn(*args, **kwargs)
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/torch/_library/custom_ops.py", line 337, in wrapped_fn
    return fn(*args, **kwargs)
  File "/workspace/GPT/.venv/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py", line 96, in _flash_attn_forward
    out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(
RuntimeError: FlashAttention only support fp16 and bf16 data type

